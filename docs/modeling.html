<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Modeling in the Social Sciences | Methods and Techniques for Social and Economic Research: Syllabus</title>
  <meta name="description" content="<p>This syllabus contains lecture notes for the course
Methods and Techniques for Social and Economic Research for the program
Earth, Economics and Sustainability</p>" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Modeling in the Social Sciences | Methods and Techniques for Social and Economic Research: Syllabus" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This syllabus contains lecture notes for the course
Methods and Techniques for Social and Economic Research for the program
Earth, Economics and Sustainability</p>" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Modeling in the Social Sciences | Methods and Techniques for Social and Economic Research: Syllabus" />
  
  <meta name="twitter:description" content="<p>This syllabus contains lecture notes for the course
Methods and Techniques for Social and Economic Research for the program
Earth, Economics and Sustainability</p>" />
  

<meta name="author" content="Paul Koster &amp; Thomas de Graaff" />


<meta name="date" content="2022-10-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="univariateregression.html"/>
<link rel="next" href="specification.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what"><i class="fa fa-check"></i>What</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why"><i class="fa fa-check"></i>Why</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#for-whom"><i class="fa fa-check"></i>For Whom</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#theory-models-and-hypotheses"><i class="fa fa-check"></i><b>1.1</b> Theory, Models and Hypotheses</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#doing-research-in-the-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Doing Research (in the Social Sciences)</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#work-tidy"><i class="fa fa-check"></i><b>1.2.1</b> Work tidy</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#know-where-your-stuff-is"><i class="fa fa-check"></i><b>1.2.2</b> Know where your stuff is</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#make-notes"><i class="fa fa-check"></i><b>1.2.3</b> Make notes</a></li>
<li class="chapter" data-level="1.2.4" data-path="introduction.html"><a href="introduction.html#use-a-reference-manager"><i class="fa fa-check"></i><b>1.2.4</b> Use a reference manager!</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#statistical-software"><i class="fa fa-check"></i><b>1.3</b> Statistical software</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#reading-guide"><i class="fa fa-check"></i><b>1.4</b> Reading Guide</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="surplus.html"><a href="surplus.html"><i class="fa fa-check"></i><b>2</b> Introduction to Economic Surplus</a>
<ul>
<li class="chapter" data-level="2.1" data-path="surplus.html"><a href="surplus.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="surplus.html"><a href="surplus.html#background"><i class="fa fa-check"></i><b>2.1.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="surplus.html"><a href="surplus.html#consumer-choices-and-consumer-value"><i class="fa fa-check"></i><b>2.2</b> Consumer choices and consumer value</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="surplus.html"><a href="surplus.html#utility-functions-inverse-demand-and-demand"><i class="fa fa-check"></i><b>2.2.1</b> Utility functions, inverse demand and demand</a></li>
<li class="chapter" data-level="2.2.2" data-path="surplus.html"><a href="surplus.html#examples-of-demand-and-inverse-demand-functions"><i class="fa fa-check"></i><b>2.2.2</b> Examples of demand and inverse demand functions</a></li>
<li class="chapter" data-level="2.2.3" data-path="surplus.html"><a href="surplus.html#consumer-benefits-and-surplus-in-markets"><i class="fa fa-check"></i><b>2.2.3</b> Consumer benefits and surplus in markets</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="surplus.html"><a href="surplus.html#poducer-behaviour-and-surplus"><i class="fa fa-check"></i><b>2.3</b> Poducer behaviour and surplus</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="surplus.html"><a href="surplus.html#producer-cost-functions-and-cost-minimisation"><i class="fa fa-check"></i><b>2.3.1</b> Producer cost functions and cost minimisation</a></li>
<li class="chapter" data-level="2.3.2" data-path="surplus.html"><a href="surplus.html#specifying-and-interpreting-the-production-function"><i class="fa fa-check"></i><b>2.3.2</b> Specifying and interpreting the production function</a></li>
<li class="chapter" data-level="2.3.3" data-path="surplus.html"><a href="surplus.html#the-cost-function"><i class="fa fa-check"></i><b>2.3.3</b> The cost function</a></li>
<li class="chapter" data-level="2.3.4" data-path="surplus.html"><a href="surplus.html#special-case-constant-average-and-marginal-costs"><i class="fa fa-check"></i><b>2.3.4</b> Special case: constant average and marginal costs</a></li>
<li class="chapter" data-level="2.3.5" data-path="surplus.html"><a href="surplus.html#empirical-examples"><i class="fa fa-check"></i><b>2.3.5</b> Empirical examples</a></li>
<li class="chapter" data-level="2.3.6" data-path="surplus.html"><a href="surplus.html#from-firm-costs-to-market-inverse-supply-and-supply"><i class="fa fa-check"></i><b>2.3.6</b> From firm costs to market inverse supply and supply</a></li>
<li class="chapter" data-level="2.3.7" data-path="surplus.html"><a href="surplus.html#producer-surplus"><i class="fa fa-check"></i><b>2.3.7</b> Producer surplus</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="surplus.html"><a href="surplus.html#analysis-of-economic-surplus"><i class="fa fa-check"></i><b>2.4</b> Analysis of economic surplus</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="surplus.html"><a href="surplus.html#introduction-2"><i class="fa fa-check"></i><b>2.4.1</b> Introduction</a></li>
<li class="chapter" data-level="2.4.2" data-path="surplus.html"><a href="surplus.html#equilibrium"><i class="fa fa-check"></i><b>2.4.2</b> Equilibrium</a></li>
<li class="chapter" data-level="2.4.3" data-path="surplus.html"><a href="surplus.html#calibration-of-equilibrium"><i class="fa fa-check"></i><b>2.4.3</b> Calibration of equilibrium</a></li>
<li class="chapter" data-level="2.4.4" data-path="surplus.html"><a href="surplus.html#analysis-of-economic-surplus-1"><i class="fa fa-check"></i><b>2.4.4</b> Analysis of economic surplus</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="surplus.html"><a href="surplus.html#external-costs-and-economic-surplus"><i class="fa fa-check"></i><b>2.5</b> External costs and economic surplus</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="surplus.html"><a href="surplus.html#introduction-3"><i class="fa fa-check"></i><b>2.5.1</b> Introduction</a></li>
<li class="chapter" data-level="2.5.2" data-path="surplus.html"><a href="surplus.html#general-analysis-of-the-consumer-externality-tax"><i class="fa fa-check"></i><b>2.5.2</b> General analysis of the consumer externality tax</a></li>
<li class="chapter" data-level="2.5.3" data-path="surplus.html"><a href="surplus.html#consumer-tax-for-specific-inverse-demand-and-supply-functions"><i class="fa fa-check"></i><b>2.5.3</b> Consumer tax for specific inverse demand and supply functions</a></li>
<li class="chapter" data-level="2.5.4" data-path="surplus.html"><a href="surplus.html#stylised-solutions-for-the-case-when-marginal-external-costs-are-proportional-to-the-equilibrium-price."><i class="fa fa-check"></i><b>2.5.4</b> Stylised solutions for the case when marginal external costs are proportional to the equilibrium price.</a></li>
<li class="chapter" data-level="2.5.5" data-path="surplus.html"><a href="surplus.html#analysis-of-changes-in-economic-surplus-due-to-externality-taxation"><i class="fa fa-check"></i><b>2.5.5</b> Analysis of changes in economic surplus due to externality taxation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="surplus.html"><a href="surplus.html#conclusion"><i class="fa fa-check"></i><b>2.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html"><i class="fa fa-check"></i><b>3</b> Behavioural Error and Economic Surplus</a>
<ul>
<li class="chapter" data-level="3.1" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#introduction-4"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#arguments"><i class="fa fa-check"></i><b>3.2</b> Arguments against neo-classical valuation and responses</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#introduction-5"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#discussion-of-premise-a"><i class="fa fa-check"></i><b>3.2.2</b> Discussion of Premise A</a></li>
<li class="chapter" data-level="3.2.3" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#discussion-of-premise-b"><i class="fa fa-check"></i><b>3.2.3</b> Discussion of premise (B)</a></li>
<li class="chapter" data-level="3.2.4" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#discussion-of-premise-c"><i class="fa fa-check"></i><b>3.2.4</b> Discussion of premise C</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#choicemodelserrors"><i class="fa fa-check"></i><b>3.3</b> Behavioural choice models with errors</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#introduction-8"><i class="fa fa-check"></i><b>3.3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.3.2" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#approach1"><i class="fa fa-check"></i><b>3.3.2</b> Approach 1: separated decision and experienced utility</a></li>
<li class="chapter" data-level="3.3.3" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#approach2"><i class="fa fa-check"></i><b>3.3.3</b> Approach 2: satisficing and the inverse demand curve</a></li>
<li class="chapter" data-level="3.3.4" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#approach3"><i class="fa fa-check"></i><b>3.3.4</b> Approach 3: System I and system II thinking</a></li>
<li class="chapter" data-level="3.3.5" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#approach4"><i class="fa fa-check"></i><b>3.3.5</b> Approach 4: direct utility weights.</a></li>
<li class="chapter" data-level="3.3.6" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#conclusion-1"><i class="fa fa-check"></i><b>3.3.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#beherrorsurplus"><i class="fa fa-check"></i><b>3.4</b> Behavioural errors and economic social surplus</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#implications-of-behavioural-errors-for-consumer-surplus"><i class="fa fa-check"></i><b>3.4.1</b> Implications of behavioural errors for consumer surplus</a></li>
<li class="chapter" data-level="3.4.2" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#the-impact-of-behaviour-error-on-supply-decisions-and-producer-surplus"><i class="fa fa-check"></i><b>3.4.2</b> The impact of behaviour error on supply decisions and producer surplus</a></li>
<li class="chapter" data-level="3.4.3" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#totalsurplus"><i class="fa fa-check"></i><b>3.4.3</b> Behavioural error and total economic surplus</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#behavioural-error-and-policy-recommendations"><i class="fa fa-check"></i><b>3.5</b> Behavioural error and policy recommendations</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#behavioural-errors-and-information-provision"><i class="fa fa-check"></i><b>3.5.1</b> Behavioural errors and information provision</a></li>
<li class="chapter" data-level="3.5.2" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#pricing-of-internalities"><i class="fa fa-check"></i><b>3.5.2</b> Pricing of internalities</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#behavioral-errors-and-pricing-of-consumption-externalities"><i class="fa fa-check"></i><b>3.6</b> Behavioral errors and pricing of consumption externalities</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#introduction-9"><i class="fa fa-check"></i><b>3.6.1</b> Introduction</a></li>
<li class="chapter" data-level="3.6.2" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#a-combined-externality-internality-tax"><i class="fa fa-check"></i><b>3.6.2</b> A combined externality-internality tax</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#discussion-and-conclusion"><i class="fa fa-check"></i><b>3.7</b> Discussion and conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="moral.html"><a href="moral.html"><i class="fa fa-check"></i><b>4</b> Moral Considerations and Economic Surplus </a>
<ul>
<li class="chapter" data-level="4.1" data-path="moral.html"><a href="moral.html#introduction-10"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="moral.html"><a href="moral.html#dealing-with-moral-considerations-at-the-valuation-stage"><i class="fa fa-check"></i><b>4.2</b> Dealing with moral considerations at the valuation stage</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="moral.html"><a href="moral.html#introduction-11"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="moral.html"><a href="moral.html#ethical-checkbox-approach"><i class="fa fa-check"></i><b>4.2.2</b> Ethical checkbox approach</a></li>
<li class="chapter" data-level="4.2.3" data-path="moral.html"><a href="moral.html#economizing-ethics-approach"><i class="fa fa-check"></i><b>4.2.3</b> Economizing ethics approach</a></li>
<li class="chapter" data-level="4.2.4" data-path="moral.html"><a href="moral.html#ethicizing-economics-approach"><i class="fa fa-check"></i><b>4.2.4</b> Ethicizing economics approach</a></li>
<li class="chapter" data-level="4.2.5" data-path="moral.html"><a href="moral.html#qualitative-valuation-approach"><i class="fa fa-check"></i><b>4.2.5</b> Qualitative valuation approach</a></li>
<li class="chapter" data-level="4.2.6" data-path="moral.html"><a href="moral.html#contributions-of-the-remainder-of-this-chapter"><i class="fa fa-check"></i><b>4.2.6</b> Contributions of the remainder of this chapter</a></li>
<li class="chapter" data-level="4.2.7" data-path="moral.html"><a href="moral.html#scope"><i class="fa fa-check"></i><b>4.2.7</b> Scope</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="moral.html"><a href="moral.html#redefining-the-economic-pie"><i class="fa fa-check"></i><b>4.3</b> Redefining the economic pie</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="moral.html"><a href="moral.html#the-advisory-committee"><i class="fa fa-check"></i><b>4.3.1</b> The advisory committee</a></li>
<li class="chapter" data-level="4.3.2" data-path="moral.html"><a href="moral.html#adjusted-consumer-surplus"><i class="fa fa-check"></i><b>4.3.2</b> Adjusted consumer surplus</a></li>
<li class="chapter" data-level="4.3.3" data-path="moral.html"><a href="moral.html#adjusted-producer-surplus"><i class="fa fa-check"></i><b>4.3.3</b> Adjusted producer surplus</a></li>
<li class="chapter" data-level="4.3.4" data-path="moral.html"><a href="moral.html#adjusted-economic-surplus"><i class="fa fa-check"></i><b>4.3.4</b> Adjusted economic surplus</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="moral.html"><a href="moral.html#taxation-and-moral-consideration"><i class="fa fa-check"></i><b>4.4</b> Taxation and moral consideration</a></li>
<li class="chapter" data-level="4.5" data-path="moral.html"><a href="moral.html#externality-taxation-and-moral-considerations"><i class="fa fa-check"></i><b>4.5</b> Externality taxation and moral considerations</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="moral.html"><a href="moral.html#moral-considerations-related-to-the-kind-of-externality"><i class="fa fa-check"></i><b>4.5.1</b> Moral considerations related to the kind of externality</a></li>
<li class="chapter" data-level="4.5.2" data-path="moral.html"><a href="moral.html#application-pricing-consumer-externalities-that-are-external-to-the-market"><i class="fa fa-check"></i><b>4.5.2</b> Application: pricing consumer externalities that are external to the market</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="moral.html"><a href="moral.html#conclusion-and-discussion"><i class="fa fa-check"></i><b>4.6</b> Conclusion and discussion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="univariateregression.html"><a href="univariateregression.html"><i class="fa fa-check"></i><b>5</b> Regression analysis in the social sciences</a>
<ul>
<li class="chapter" data-level="5.1" data-path="univariateregression.html"><a href="univariateregression.html#introduction-12"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="univariateregression.html"><a href="univariateregression.html#secproblem"><i class="fa fa-check"></i><b>5.2</b> So, what is the problem?</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="univariateregression.html"><a href="univariateregression.html#a-first-encounter-with-stata"><i class="fa fa-check"></i><b>5.2.1</b> A first encounter with <code>STATA</code></a></li>
<li class="chapter" data-level="5.2.2" data-path="univariateregression.html"><a href="univariateregression.html#sec:numevidence"><i class="fa fa-check"></i><b>5.2.2</b> Numerical evidence</a></li>
<li class="chapter" data-level="5.2.3" data-path="univariateregression.html"><a href="univariateregression.html#sec:smart"><i class="fa fa-check"></i><b>5.2.3</b> Always be smart (and a bit lazy)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="univariateregression.html"><a href="univariateregression.html#sec:uniregress"><i class="fa fa-check"></i><b>5.3</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="univariateregression.html"><a href="univariateregression.html#sec:genesis"><i class="fa fa-check"></i><b>5.3.1</b> Genesis: <em>regression towards the mean</em></a></li>
<li class="chapter" data-level="5.3.2" data-path="univariateregression.html"><a href="univariateregression.html#regression-with-one-regressor"><i class="fa fa-check"></i><b>5.3.2</b> Regression with one regressor</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="univariateregression.html"><a href="univariateregression.html#least-squares-assumptions-for-causal-inference"><i class="fa fa-check"></i><b>5.4</b> Least squares assumptions for causal inference</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="univariateregression.html"><a href="univariateregression.html#least-squares-assumption-1-conditional-mean-independence"><i class="fa fa-check"></i><b>5.4.1</b> Least squares assumption 1: conditional mean independence</a></li>
<li class="chapter" data-level="5.4.2" data-path="univariateregression.html"><a href="univariateregression.html#least-squares-assumption-2-independenty-and-identically-distributed"><i class="fa fa-check"></i><b>5.4.2</b> Least squares assumption 2: independenty and identically distributed</a></li>
<li class="chapter" data-level="5.4.3" data-path="univariateregression.html"><a href="univariateregression.html#least-squares-assumption-3-large-outliers-are-rare"><i class="fa fa-check"></i><b>5.4.3</b> Least squares assumption 3: Large outliers are rare</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="univariateregression.html"><a href="univariateregression.html#other-least-squares-assumptions"><i class="fa fa-check"></i><b>5.5</b> Other least squares assumptions</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="univariateregression.html"><a href="univariateregression.html#homoskedasticity"><i class="fa fa-check"></i><b>5.5.1</b> Homoskedasticity</a></li>
<li class="chapter" data-level="5.5.2" data-path="univariateregression.html"><a href="univariateregression.html#normal-distributed-regression-term"><i class="fa fa-check"></i><b>5.5.2</b> Normal distributed regression term</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="univariateregression.html"><a href="univariateregression.html#measures-of-fit"><i class="fa fa-check"></i><b>5.6</b> Measures of fit</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="univariateregression.html"><a href="univariateregression.html#the-regression-r2"><i class="fa fa-check"></i><b>5.6.1</b> The regression <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="5.6.2" data-path="univariateregression.html"><a href="univariateregression.html#the-standard-error-of-the-regression"><i class="fa fa-check"></i><b>5.6.2</b> The Standard Error of the Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="univariateregression.html"><a href="univariateregression.html#conclusion-and-discussion-1"><i class="fa fa-check"></i><b>5.7</b> Conclusion and discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>6</b> Modeling in the Social Sciences</a>
<ul>
<li class="chapter" data-level="6.1" data-path="modeling.html"><a href="modeling.html#sec:morevar"><i class="fa fa-check"></i><b>6.1</b> Why more independent variables?</a></li>
<li class="chapter" data-level="6.2" data-path="modeling.html"><a href="modeling.html#sec:multivariate"><i class="fa fa-check"></i><b>6.2</b> Multivariate regression analysis</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="modeling.html"><a href="modeling.html#measures-of-fit-for-multiple-regression"><i class="fa fa-check"></i><b>6.2.1</b> Measures of fit for multiple regression</a></li>
<li class="chapter" data-level="6.2.2" data-path="modeling.html"><a href="modeling.html#the-least-squares-assumptions-for-multivariate-regression"><i class="fa fa-check"></i><b>6.2.2</b> The least squares assumptions for multivariate regression</a></li>
<li class="chapter" data-level="6.2.3" data-path="modeling.html"><a href="modeling.html#testing-with-multivariate-regression-models"><i class="fa fa-check"></i><b>6.2.3</b> Testing with multivariate regression models</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="modeling.html"><a href="modeling.html#sec:nonlinear"><i class="fa fa-check"></i><b>6.3</b> Non-linear specifications</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="modeling.html"><a href="modeling.html#polynomials"><i class="fa fa-check"></i><b>6.3.1</b> Polynomials</a></li>
<li class="chapter" data-level="6.3.2" data-path="modeling.html"><a href="modeling.html#interaction-variables"><i class="fa fa-check"></i><b>6.3.2</b> Interaction variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="modeling.html"><a href="modeling.html#logarithmic-transformation"><i class="fa fa-check"></i><b>6.3.3</b> Logarithmic transformation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="modeling.html"><a href="modeling.html#sec:fixedeffects"><i class="fa fa-check"></i><b>6.4</b> Using fixed effects in panel data</a></li>
<li class="chapter" data-level="6.5" data-path="modeling.html"><a href="modeling.html#conclusion-and-discussion-2"><i class="fa fa-check"></i><b>6.5</b> Conclusion and discussion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="specification.html"><a href="specification.html"><i class="fa fa-check"></i><b>7</b> Specification and Assessment Issues</a>
<ul>
<li class="chapter" data-level="7.1" data-path="specification.html"><a href="specification.html#specification-of-your-model"><i class="fa fa-check"></i><b>7.1</b> Specification of your model</a></li>
<li class="chapter" data-level="7.2" data-path="specification.html"><a href="specification.html#presentation-of-results"><i class="fa fa-check"></i><b>7.2</b> Presentation of results</a></li>
<li class="chapter" data-level="7.3" data-path="specification.html"><a href="specification.html#potential-sources-of-bias"><i class="fa fa-check"></i><b>7.3</b> Potential sources of bias</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="in-conclusion.html"><a href="in-conclusion.html"><i class="fa fa-check"></i><b>8</b> In conclusion</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="apperror.html"><a href="apperror.html"><i class="fa fa-check"></i><b>A</b> Taxation in the presence of behavioural error</a></li>
<li class="chapter" data-level="B" data-path="background-calculations-for-chapter-refmoral.html"><a href="background-calculations-for-chapter-refmoral.html"><i class="fa fa-check"></i><b>B</b> Background calculations for Chapter @ref(moral)</a>
<ul>
<li class="chapter" data-level="B.1" data-path="background-calculations-for-chapter-refmoral.html"><a href="background-calculations-for-chapter-refmoral.html#apptax"><i class="fa fa-check"></i><b>B.1</b> Deriving the tax that optimizes adjusted social surplus</a></li>
<li class="chapter" data-level="B.2" data-path="background-calculations-for-chapter-refmoral.html"><a href="background-calculations-for-chapter-refmoral.html#apppricing"><i class="fa fa-check"></i><b>B.2</b> Pricing of a consumption externality</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appreviewstat.html"><a href="appreviewstat.html"><i class="fa fa-check"></i><b>C</b> Reviewing probability and statistics</a>
<ul>
<li class="chapter" data-level="C.1" data-path="appreviewstat.html"><a href="appreviewstat.html#reviewing-probability"><i class="fa fa-check"></i><b>C.1</b> Reviewing probability</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="appreviewstat.html"><a href="appreviewstat.html#probability"><i class="fa fa-check"></i><b>C.1.1</b> Probability</a></li>
<li class="chapter" data-level="C.1.2" data-path="appreviewstat.html"><a href="appreviewstat.html#population-random-variables"><i class="fa fa-check"></i><b>C.1.2</b> Population &amp; random variables</a></li>
<li class="chapter" data-level="C.1.3" data-path="appreviewstat.html"><a href="appreviewstat.html#distribution-functions"><i class="fa fa-check"></i><b>C.1.3</b> Distribution functions</a></li>
<li class="chapter" data-level="C.1.4" data-path="appreviewstat.html"><a href="appreviewstat.html#conditional-distributions-and-conditional-means"><i class="fa fa-check"></i><b>C.1.4</b> Conditional distributions and conditional means</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="appreviewstat.html"><a href="appreviewstat.html#secssampling"><i class="fa fa-check"></i><b>C.2</b> Sampling in frequentist statistics</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="appreviewstat.html"><a href="appreviewstat.html#the-sampling-distribution-of-bary"><i class="fa fa-check"></i><b>C.2.1</b> The sampling distribution of <span class="math inline">\(\bar{Y}\)</span></a></li>
<li class="chapter" data-level="C.2.2" data-path="appreviewstat.html"><a href="appreviewstat.html#example-simple-binomial-random-variables"><i class="fa fa-check"></i><b>C.2.2</b> Example: simple binomial random variables</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Methods and Techniques for Social and Economic Research: Syllabus</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modeling" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Modeling in the Social Sciences<a href="modeling.html#modeling" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In Chapter <a href="univariateregression.html#univariateregression">5</a> we discussed the origins of, working of and assumptions behind univariate regression. That is, a regression model with only one independent variable <span class="math inline">\(X\)</span> on the right hand side.<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a> However, and especially in the social science, you almost always see regressions many independent variables. Depending on the field, these variables can be called control variables, confounding factors or moderator variables. But why are these variables included? Is it only to improve model performance or are there other reasons? Section <a href="modeling.html#sec:morevar">6.1</a> deals with this question where after Section <a href="modeling.html#sec:multivariate">6.2</a> shows how you can include additional variables in a <em>multivariate regression model</em> and especially how you should interpret them. Section <a href="modeling.html#sec:nonlinear">6.3</a> extends the multivariate regression model and shows how you can actually use this model to estimate a broad range of non-linear economic models. Section <a href="modeling.html#sec:fixedeffects">6.4</a> discusses the use of multiple dummy variables (see again Subsection <a href="univariateregression.html#sec:dummy">5.3.2.4</a>) in a way that economists refer to as <em>fixed effects</em>. The last section concludes and provides a futher discussion of the benefits and limitations of multivariate regression models.</p>
<div id="sec:morevar" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Why more independent variables?<a href="modeling.html#sec:morevar" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So, why do we include more variables? One possible answer is because it makes a better predictive model. That is, a model that is able to explain the variation in the dependent variable <span class="math inline">\(Y\)</span> better.<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a> So, the R<span class="math inline">\(^2\)</span> increases. But, as argued in Chapter <a href="univariateregression.html#univariateregression">5</a> we are not so much interested in prediction, but more in establishing a <strong>causal</strong> relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. So, if you change <span class="math inline">\(X\)</span> (and only <span class="math inline">\(X\)</span>) does <span class="math inline">\(Y\)</span> changes and then with how much?</p>
<p>Although economists often claim that they are the only (social-)science which is focused on causality and provides a statistical framework for that, there are other approaches to causality as well. One that is often used in other sciences is the approach of the mathematican Judea Pearl <span class="citation">(Pearl 2009)</span>. This approach focuses on the use of Directed Acyclical Graphs (DAGs), which is graphical visualisation of causality chains (or, what impacts what). We borrow this approach for the most simple setting as explained in Figure <a href="modeling.html#fig:unknown">6.1</a>. Here, we go back to our Californian school district dataset again, where we still are interested in the effect of class size on school performance. So, we suppose that that there is an effect from student teacher ratio on test scores as displayed with an directed arrow in Figure <a href="modeling.html#fig:unknown">6.1</a>. We also know that the R<span class="math inline">\(^2\)</span> of that regression model was rather low (5%), so by default there must be other but yet unknown factors, let us name them for now <span class="math inline">\(U\)</span>, that influence test scores as well (so a directed arrow going from <span class="math inline">\(U\)</span> to test scores).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unknown"></span>
<img src="figures/unknown.png" alt="Unrelated omitted variables" width="600px" />
<p class="caption">
Figure 6.1: Unrelated omitted variables
</p>
</div>
<p>Now we are fine with this is as long as <span class="math inline">\(U\)</span> does <strong>not impact</strong> the student teacher ratio. Then, there is still an isolated effect of student teacher ratio on class size and that is exactly what we want to measure. However, if there is directed arrow going from <span class="math inline">\(U\)</span> into <span class="math inline">\(STR\)</span> as depicted by Figure <a href="modeling.html#fig:unobshet">6.2</a>, then the effect of student teacher ratio is not isolated anymore. Essentially, the effect of student teacher ratio on class size is composed out of two parts:</p>
<ol style="list-style-type: decimal">
<li>The <strong>causal</strong> effect on student teacher ratio on class size captured by the chain <span class="math inline">\(\text{STR} \longrightarrow \text{testscore}\)</span>. The one we are after.</li>
<li>The impact of the unknown variables on test scores. As we have not modeled them in our regression model, the effect is captured by the chain <span class="math inline">\(U \longrightarrow \text{STR} \longrightarrow \text{testscore}\)</span></li>
</ol>
<p>Economists refer to this phenomenon as <strong>omitted variable bias</strong>, whilst in the statistical world, this is as often called confounding variables or the <strong>the confounding fork</strong> <span class="citation">(McElreath 2020)</span> and it, unfortunately, occurs very often.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unobshet"></span>
<img src="figures/Unobshet.png" alt="Related omitted variables" width="600px" />
<p class="caption">
Figure 6.2: Related omitted variables
</p>
</div>
<p>So, when <strong>U</strong> is a <em>common</em> cause for both student teacher ratio and test scores there is omitted variable bias. If we go back to our population regression model as follows:
<span class="math display">\[\begin{equation}
Y_i = \beta_0 + \beta_1 X_i + u_i,
\end{equation}\]</span>
then we know that the error <span class="math inline">\(u\)</span> arises because of factors that influence <span class="math inline">\(Y\)</span> but are not included in the regression function; so, there are <em>always</em> omitted variables. But do not always lead to bias. For omitted variable bias to occur, the omitted factor, let’s call it <span class="math inline">\(Z\)</span><a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a>, must be:</p>
<ol style="list-style-type: decimal">
<li>A <strong>determinant</strong> of <span class="math inline">\(Y\)</span> (i.e. <span class="math inline">\(Z\)</span> is part of <span class="math inline">\(u\)</span>)</li>
<li>A <strong>determinant</strong> of the regressor <span class="math inline">\(X\)</span> (<em>at least</em>, there should hold that <span class="math inline">\(corr(Z,X) \neq 0\)</span>)<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a></li>
</ol>
<p>Thus, both conditions must hold for the omission of <span class="math inline">\(Z\)</span> to result in omitted variable bias.</p>
<p>Now, in our Californian district school dataset we have many more variables. One of them is variable that measures the english language ability (whether the student has English as a second language). Note that in California there are many migrants, especially from Latin-America. Now, you can readily argue that not having English as first language plausibly affects standardized test scores: so, <span class="math inline">\(Z\)</span> is a <strong>determinant</strong> of <span class="math inline">\(Y\)</span>. Moreover, immigrant communities tend to be less affluent and thus have smaller school budgets—and, therefore, higher <span class="math inline">\(STR\)</span>: <span class="math inline">\(Z\)</span> is most likely as well a <strong>determinant</strong> of <span class="math inline">\(X\)</span>.</p>
<p>So, most likely, our original estimation from Chapter <a href="univariateregression.html#univariateregression">5</a>, <span class="math inline">\(\hat{\beta}_1\)</span>, is biased (so not the true causal effect). But can we say something about the direction that bias? Yes, but the argument tend to become very quickly rather complex. In this case, note that districts with more migrant communities tend to have (<em>i</em>) higher class sizes and (<em>ii</em>) lower test scores. So, to the original estimation they added a <em>negative</em> effect. Thus, following this reasoning, the “true” effect must be less negative. Now, especially with negative signs this becomes rather complex, so if common sense fails you, there is the following formula:</p>
<p><span class="math display">\[\begin{equation}
\hat{\beta}_1 \overset{p}{\to} \beta_1 + \frac{\sigma_u}{\sigma_X}\rho_{Xu},
\end{equation}\]</span>
where you should focus on the sign of the correlation between <span class="math inline">\(X\)</span> and the regression residual <span class="math inline">\(u\)</span> (all standard errors, <span class="math inline">\(\sigma\)</span>, are always positive by default). Now, the first least squares assumption states that <span class="math inline">\(\rho_{Xu} = 0\)</span>—no correlation between the regressor and the regression residual. But now there is because of omitted variable bias. And because the negative relation between immigrants communities and school performance <span class="math inline">\(\rho_{Xu}\)</span> should be negative. And because the original estimation from Chapter <a href="univariateregression.html#univariateregression">5</a> was already negative to begin with the “true” <span class="math inline">\(\beta_1\)</span> should be less negative. In conclusion, districts with more English learning students (<em>i</em>) do worse on standardized tests and (<em>ii</em>) have bigger classes (smaller budgets), so ignoring the English learning factor results in overstating the class size effect.</p>
<p>You might wonder whether this is actually going on in the Californian district school data. To see this, Figure <a href="modeling.html#fig:omitca">6.3</a> offers a cross tabulation of test scores by class size and percentage English learners.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:omitca"></span>
<img src="figures/Sheet7.png" alt="Cross tabulation of test scores by class size and percentage English learners" width="800px" />
<p class="caption">
Figure 6.3: Cross tabulation of test scores by class size and percentage English learners
</p>
</div>
<p>Now, the table depicted in Figure <a href="modeling.html#fig:omitca">6.3</a> is complex in its various dimensions. We have our two categories of class size (small and large), together with the difference in test scores, but we now stratify this by four categories of percentage English learners. There are several important observations to make here:</p>
<ol style="list-style-type: decimal">
<li>districts with <em>fewer</em> English Learners (so less migrants) have on average <em>higher</em> test scores (what we assumed above);</li>
<li>districts with <em>fewer</em> English Learners (so less migrants) have <em>smaller</em> classes (what we assumed above);</li>
<li>the effect of class size with comparable percentages English learners is still (mostly negative), but not as much as we compares for all districts together (the Difference-column). This confirms our reasoning that our original estimate was too negative.</li>
</ol>
<p>No, as already mentioned above omitted variable bias occurs very often. So, how to correct for this such that the bias disappaers. In general, there are strategies:</p>
<ol style="list-style-type: decimal">
<li>we can run a randomized controlled experiment in which treatment (<span class="math inline">\(STR\)</span>) is randomly assigned: then percentage English learners (<span class="math inline">\(PctEL\)</span>) is still a determinant of test score, but by construction <span class="math inline">\(PctEL\)</span> should be uncorrelated with <span class="math inline">\(STR\)</span>. Unfortunately, is very difficult to randomize class size and often this strategy is just not attainable as being too costly or unethical (this accounts for all sciences);</li>
<li>we can adopt the cross tabulation approach of above, with finer gradations of <span class="math inline">\(STR\)</span> and <span class="math inline">\(PctEL\)</span>. Then by construction, within each group all classes have the same <span class="math inline">\(PctEL\)</span> so we control for <span class="math inline">\(PctEL\)</span>. A disadvantages is that one needs many observations, especially when one wants to stratify upon other variables as well;</li>
<li>finally, and perhaps the easiest approach, we can use a population regression model in which the omitted variable (<span class="math inline">\(PctEL\)</span>) is no longer omitted. We just include <span class="math inline">\(PctEL\)</span> as an additional regressor in a multiple regression model. This is what the next section deals with. Obviously, a disadvantage of this approach is that you need observations for the omitted variable.</li>
</ol>
</div>
<div id="sec:multivariate" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Multivariate regression analysis<a href="modeling.html#sec:multivariate" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So, if we have information about an important omitted variable, as in the case of the size of migrant communities in the example above, then we can use that information in a multivariate population regression model. In the case of two regressors, that would look like”
<span class="math display">\[\begin{equation}
Y_i =\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i, i=1,\ldots,n
\end{equation}\]</span>
where:</p>
<ul>
<li><span class="math inline">\(Y\)</span> is the dependent variable</li>
<li><span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> are the two independent variables (regressors)</li>
<li><span class="math inline">\((Y_i, X_{1i}, X_{2i})\)</span> denote the i<span class="math inline">\(^{\mathrm{th}}\)</span> observation on <span class="math inline">\(Y\)</span>, <span class="math inline">\(X_1\)</span>, and <span class="math inline">\(X_2\)</span>.</li>
<li><span class="math inline">\(\beta_0\)</span> is the unknown population intercept</li>
<li><span class="math inline">\(\beta_1\)</span> is the effect on <span class="math inline">\(Y\)</span> of a change in <span class="math inline">\(X_1\)</span>, <strong>holding</strong> <span class="math inline">\(X_2\)</span> constant</li>
<li><span class="math inline">\(\beta_2\)</span> is the effect on <span class="math inline">\(Y\)</span> of a change in <span class="math inline">\(X_2\)</span>, <strong>holding</strong> <span class="math inline">\(X_1\)</span> constant</li>
<li><span class="math inline">\(u_i\)</span> is the the regression error (omitted factors)</li>
</ul>
<p>Now the only element that changes is the interpretation of a parameter, say <span class="math inline">\(\beta_1\)</span>. In this case, it can still be seen as a ‘slope’ parameter, although now in 3-dimensional space, but it now states specifically that the other parameter(s) should be held constant. This does facilitate the interpretation of <span class="math inline">\(\beta_1\)</span>. For example, consider changing <span class="math inline">\(X_1\)</span> by <span class="math inline">\(\Delta X_1\)</span> while holding <span class="math inline">\(X_2\)</span> constant. That means that the population regression line before the change looks like:
<span class="math display">\[\begin{equation}
Y = \beta_0 + \beta_1 X_{1} + \beta_2 X_{2},
\end{equation}\]</span>
whilst the population regression line, after the change, looks like:
<span class="math display">\[\begin{equation}
Y + \Delta Y = \beta_0 + \beta_1 (X_{1} + \Delta X_1) + \beta_2 X_{2}
\end{equation}\]</span>
And if we take the difference, then the interpretation of <span class="math inline">\(\beta_1\)</span> boils down again to the marginal effect:<span class="math inline">\(\Delta Y = \beta_1 \Delta X_1\)</span>. Or, <span class="math inline">\(\beta_1 = \frac{\Delta Y}{\Delta X_1}\)</span> when holding <span class="math inline">\(X_2\)</span> constant and, likewise, <span class="math inline">\(\beta_2 = \frac{\Delta Y}{\Delta X_2}\)</span> when holding <span class="math inline">\(X_1\)</span> constant. <span class="math inline">\(\beta_0\)</span> is now the predicted value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X_1 = X_2 = 0\)</span></p>
<p>If we do this for the the Californian school district data, then the original population regression line was estimated as:
<span class="math display">\[\begin{equation}
\widehat{TestScore} = 698.9- 2.28 STR
\end{equation}\]</span>
But if we now include include percent English Learners in the district (<span class="math inline">\(PctEL\)</span>) to the model then the population regression ‘line’ becomes:
<span class="math display">\[\begin{equation}
\widehat{TestScore} = 686.0- 1.10 STR - 0.65  PctEL
\end{equation}\]</span></p>
<p>Clearly, the effect of student teacher ratio becomes smaller (that is, less negative). And this is what should happen as reasoned above. The <code>STATA</code> syntax for a multivariate regression model is now rather straighforward. You basically add another to the regression equation, as below:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb22-1"><a href="modeling.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr str el_pct, <span class="kw">robust</span></span></code></pre></div>
<pre><code>Linear regression                               Number of obs     =        420
                                                F(2, 417)         =     223.82
                                                Prob &gt; F          =     0.0000
                                                R-squared         =     0.4264
                                                Root MSE          =     14.464

------------------------------------------------------------------------------
             |               Robust
     testscr | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -1.101296   .4328472    -2.54   0.011     -1.95213   -.2504616
      el_pct |  -.6497768   .0310318   -20.94   0.000     -.710775   -.5887786
       _cons |   686.0322   8.728224    78.60   0.000     668.8754     703.189
------------------------------------------------------------------------------</code></pre>
<p>Obviously, the effect of student teacher ration reduces with 50%! The interpretation of the rest of the statistical output, such as measures of fit and test statistics, follows in the subsections below.</p>
<div id="measures-of-fit-for-multiple-regression" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Measures of fit for multiple regression<a href="modeling.html#measures-of-fit-for-multiple-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In multivariate regression models, there are four commonly used measures of fit.</p>
<ol style="list-style-type: decimal">
<li>The standard error of regression or the <span class="math inline">\(SER\)</span> denotes the standard deviation of <span class="math inline">\(\hat{u}_i\)</span> and includes a degrees of freedom correction (degrees of freedom in this case denotes how many variables your have used and typically is denoted with <span class="math inline">\(k\)</span>. The <span class="math inline">\(SER\)</span> is defined as:
<span class="math display">\[\begin{equation}
SER = s_{\hat{u}} = \sqrt{\frac{1}{n-k-1} \sum_{i=1}^n \hat{u}^2_i},
\end{equation}\]</span>
where <span class="math inline">\(k\)</span> is the number of variables (including the constant) use in the regression model. Note that in the univariate regression model <span class="math inline">\(k=2\)</span>—the slope coefficient and the constant.</li>
<li>The root mean square error (RMSE) which denotes as well the stdandard deviation of <span class="math inline">\(\hat{u}_i\)</span> but now without degrees of freedom. We have seen this before in Eq. <a href="univariateregression.html#eq:rmse">(5.14)</a> and does not change.</li>
<li>The <span class="math inline">\(R^2\)</span> which measures the fraction of variance of <span class="math inline">\(Y\)</span> explained by the independent variables. Again, we have seen this one before</li>
<li>The adjusted “adjusted <span class="math inline">\(R^2\)</span>” (or <span class="math inline">\(\bar{R}^2\)</span>) which is equal to the <span class="math inline">\(R^2\)</span> with a degrees-of-freedom correction that adjusts for estimation uncertainty. It can be formulated as:
<span class="math display">\[\begin{equation}
\bar{R}^2 = 1 - \frac{n-1}{n-k-1}\frac{SSR}{TSS}.
\end{equation}\]</span>
Note that using this formulation, in a multivariate setting, it always should hold that <span class="math inline">\(\bar{R}^2 &lt;R^2\)</span>. Why do we care so much for the amount of variables that we use (<span class="math inline">\(k\)</span>). That is because with each additional variable the <span class="math inline">\(R^2\)</span> always increases. And it is essential to notice that <span class="math inline">\(k=n\)</span>, the <span class="math inline">\(R^2 = 1\)</span>, so there is no variation left anymore. But that feels like cheating. You just have a parameter for each observation that you have, but such a model must be meaningless. Therefore, you always want to correct for the number of variables that you use.</li>
</ol>
<p>In our Californian school district example that would amount to the following two outcomes. First for the univariate model:
<span class="math display">\[\begin{eqnarray}
TestScore &amp;= &amp;698.9- 2.28  STR \\
&amp;&amp;R^2 = .05, SER = 18.6
\end{eqnarray}\]</span></p>
<p>And then for the multivariate model.</p>
<p><span class="math display">\[\begin{eqnarray}
TestScore &amp;=&amp; 686.0 - 1.10  STR - 0.65 PctEL \\
&amp;&amp;R^2=.426, \bar{R}^2=0.424, SER = 14.5
\end{eqnarray}\]</span></p>
<p>Note that all measures of fit increases. The <span class="math inline">\(\bar{R}^2\)</span> now indicates that 42% of all variation in test scores are explained. That is a <em>huge</em> improvement compared to the 5% explanatory power of the univariate case. That indicates that the <span class="math inline">\(PctEL\)</span> strongly correlates with testscores. But again, we are not so much interested prediction, but want to find the causal impact of class size instead. Another thing to notice here is that the <span class="math inline">\(R^2\)</span> and the <span class="math inline">\(\bar{R}^2\)</span> are very close. That is because the number of variables is much smaller than the number of observations <span class="math inline">\(k \ll n\)</span>, so that the impact of <span class="math inline">\(k\)</span> is not very big.</p>
<p>A list thing is a peculiarity of <code>STATA</code>. In the regression output of above <code>STATA</code> does not provide the <span class="math inline">\(\bar{R}^2\)</span>. That is because of the option <code>, robust</code>. Without that option, the regression output would look like, giving both measures of fit.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb24-1"><a href="modeling.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr str el_pct</span></code></pre></div>
<pre><code>      Source |       SS           df       MS      Number of obs   =       420
-------------+----------------------------------   F(2, 417)       =    155.01
       Model |  64864.3011         2  32432.1506   Prob &gt; F        =    0.0000
    Residual |  87245.2925       417  209.221325   R-squared       =    0.4264
-------------+----------------------------------   Adj R-squared   =    0.4237
       Total |  152109.594       419  363.030056   Root MSE        =    14.464

------------------------------------------------------------------------------
     testscr | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -1.101296   .3802783    -2.90   0.004    -1.848797   -.3537945
      el_pct |  -.6497768   .0393425   -16.52   0.000    -.7271112   -.5724423
       _cons |   686.0322   7.411312    92.57   0.000     671.4641    700.6004
------------------------------------------------------------------------------</code></pre>
<p>Another option is to specifically ask <code>STATA</code> to display the <span class="math inline">\(\bar{R}^2\)</span> by invoking the command <code>display</code>, then some text (text always goes between strings), and finally the thing you want to see (<code>e(r2_a)</code>). Something like:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb26-1"><a href="modeling.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">display</span> <span class="st">&quot;adjusted R2 = &quot;</span> <span class="fu">e</span>(r2_a)</span></code></pre></div>
<pre><code>adjusted R2 = .42368043</code></pre>
</div>
<div id="the-least-squares-assumptions-for-multivariate-regression" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> The least squares assumptions for multivariate regression<a href="modeling.html#the-least-squares-assumptions-for-multivariate-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, it is easy to add other variables, so that the multivariate regression model now looks like:
<span class="math display">\[\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i}+\ldots + \beta_k X_{ki}+u_i, i=1,\ldots,n
\end{equation}\]</span>
Suppose we are interested in <span class="math inline">\(\beta_1\)</span>. How do we then know whether our estimation <span class="math inline">\(\hat{\beta}_1\)</span> is unbiased? For that we again resort to our least squares assumption, some of them will change a bit and we have to add a fourth one:</p>
<ol style="list-style-type: decimal">
<li>The first least squares assumptions changes slightly. Now, we state that the conditional distribution of <span class="math inline">\(u\)</span> given all <span class="math inline">\(X_i\)</span>’s has mean zero, that is, <span class="math inline">\(E(u|X_1 = x_1,\ldots, X_k = x_k) = 0\)</span>. So, <span class="math inline">\(\beta_1\)</span> is biased even another variable <span class="math inline">\(X_k\)</span> is correlated with <span class="math inline">\(u\)</span>. So, only variables has to be correlated with <span class="math inline">\(u\)</span> and then all parameters are to a certain extent biased.</li>
<li>The second least squares assumption is more or less as before but now in a multivariate fashion, so the whole set of (X<span class="math inline">\(_{1i}\)</span>,,X<span class="math inline">\(_{ki}\)</span>,Y<span class="math inline">\(_i\)</span>), i =1,,n, should be independent and identical distributed (<span class="math inline">\(i.i.d\)</span>).</li>
<li>The third least squares assumptions states again that large outliers are rare for all variables included, so for all <span class="math inline">\(X_1,\ldots, X_k\)</span>, and <span class="math inline">\(Y\)</span>.</li>
<li>The fourth assumoption is news and states that there is no perfect multicollinearity. We discuss this further below.</li>
</ol>
<div id="multicollinearity" class="section level4 hasAnchor" number="6.2.2.1">
<h4><span class="header-section-number">6.2.2.1</span> Multicollinearity<a href="modeling.html#multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Multicollinearity comes in two flavours; perfect and imperfect. The former functions as a multivariate least squares assumptions, whilst the latter oftentimes gives the largest problems. We start the discussion with perfect multicollinearity and then continue with the case of imperfect multicollinearity.</p>
<div id="perfect-multicollinearity" class="section level5 hasAnchor" number="6.2.2.1.1">
<h5><span class="header-section-number">6.2.2.1.1</span> Perfect multicollinearity<a href="modeling.html#perfect-multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The official definition of perfect multicollinearity is that there is a <strong>perfect linear combination</strong> amongst your variables. That means that there is not one optimal solution, but instead many (actually, infinitely many) more. Let us illustrate by the following example. Suppose you include <span class="math inline">\(STR\)</span> twice in your regression. Now, <code>STATA</code> produces then the following output:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb28-1"><a href="modeling.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr str str el_pct, <span class="kw">robust</span></span></code></pre></div>
<pre><code>note: str omitted because of collinearity.

Linear regression                               Number of obs     =        420
                                                F(2, 417)         =     223.82
                                                Prob &gt; F          =     0.0000
                                                R-squared         =     0.4264
                                                Root MSE          =     14.464

------------------------------------------------------------------------------
             |               Robust
     testscr | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -1.101296   .4328472    -2.54   0.011     -1.95213   -.2504616
         str |          0  (omitted)
      el_pct |  -.6497768   .0310318   -20.94   0.000     -.710775   -.5887786
       _cons |   686.0322   8.728224    78.60   0.000     668.8754     703.189
------------------------------------------------------------------------------</code></pre>
<p>See that <code>STATA</code> drops one of the <span class="math inline">\(STR\)</span> variables. But why is that? See that the impact of twice this variable should be equivalent to:
<span class="math display">\[\begin{equation}
\beta_1 STR = w_1 \beta_1 STR + w_2 \beta_1 STR = (w_1 + w_2) \beta_1 STR ,
\end{equation}\]</span>
where <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> are weights chosen such that they satisfy the condition that <span class="math inline">\(w_1 + w_2 = 1\)</span>. But there is an infinite number of combinations that satisfy this condition! So, there is not an optimal solution and one of these variables should be dropped.</p>
<p>The violation of no perfect multicollearity often occurs when using dummies (see again Subsection <a href="univariateregression.html#sec:dummy">5.3.2.4</a>). Suppose that we regress <span class="math inline">\(TestScore\)</span> on a constant, <span class="math inline">\(D\)</span>, and <span class="math inline">\(B\)</span>, where:<span class="math inline">\(D_i =1\)</span> if <span class="math inline">\(STR \leq 20\)</span>, <span class="math inline">\(=0\)</span> otherwise ; <span class="math inline">\(B_i =1\)</span> if <span class="math inline">\(STR&gt;20\)</span>, <span class="math inline">\(= 0\)</span> otherwise. This example is slightly more complex as there is no perfect correlation between <span class="math inline">\(B\)</span> and <span class="math inline">\(D\)</span>. However, the model contains as well a constant and that create a perfect linear combination, namely <span class="math inline">\(B_i + D_i = 1\)</span> and that is the definition of a constant ($_1 ), so there is perfect multicollinearity in the model. Now, this example is a special case of the so-called dummy variable trap. Suppose you have a set of multiple binary (dummy) variables, which are mutually exclusive and exhaustive—that is, there are multiple categories and every observation falls in one and only one category (Freshmen, Sophomores, Juniors, Seniors, Other). If you include all these dummy variables and a constant, you will have perfect multicollinearity—the dummy variable trap.</p>
<p>There are possible solutions to the dummy variable trap:</p>
<ol style="list-style-type: decimal">
<li>Omit one of the groups (e.g., the Freshmen), or</li>
<li>Omit the intercept</li>
</ol>
<p>In most cases you omit one of the groups (typically the one with the lowest value). This give the constant then the interpretation of the average value of that left-out category, where the dummy variables are then the relative differences to that left-out category.</p>
<p>Now, perfect multicollinearity usually reflects a mistake in the definitions of the regressors, or an oddity in the data. And, usually this is not a problem, because if you have perfect multicollinearity, your statistical software will let you know—either by crashing or giving an error message or by “dropping” one of the variables arbitrarily and very often the solution to perfect multicollinearity is to modify your list of regressors such that you no longer have perfect multicollinearity.</p>
</div>
<div id="imperfect-multicollinearity" class="section level5 hasAnchor" number="6.2.2.1.2">
<h5><span class="header-section-number">6.2.2.1.2</span> Imperfect multicollinearity<a href="modeling.html#imperfect-multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Now imperfect and perfect multicollinearity are quite different despite the similarity of the names. Imperfect multicollinearity namely, occurs when two or more regressors are very highly correlated. And if two regressors are very highly correlated, then their scatterplot will pretty much look like a straight line—they are collinear—but unless the correlation is exactly <span class="math inline">\(\pm\)</span> 1, that collinearity is imperfect. What this implies is that one or more of the regression coefficients will be imprecisely estimated. Why is that? That is because of the definition of the coefficient in a multivariate regression model. Namely, the coefficient on <span class="math inline">\(X_1\)</span> is the effect of <span class="math inline">\(X_1\)</span> <strong>holding <span class="math inline">\(X_2\)</span> constant</strong>, but if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are highly correlated, then there is very little variation in <span class="math inline">\(X_1\)</span> once <span class="math inline">\(X_2\)</span> is held constant. That means that the data are pretty much uninformative about what happens when <span class="math inline">\(X_1\)</span> changes but <span class="math inline">\(X_2\)</span> doesn’t, so the variance of the OLS estimator of the coefficient on <span class="math inline">\(X_1\)</span> will be large. And this results in large standard errors for one or more of the OLS coefficients. But often this is very hard to detect. Are standard errors high because of imperfect multicollinearity, because the number of observations is very low, or because there is large variation in the data? The answer to this unfortunately boils down to reasoning, but before you start estimating your statistical models it always good to look at scatterplots and correlations between variables.</p>
<p>But what is a high correlation? With a reasonable amount of observations all correlations below <span class="math inline">\(0.9\)</span> can be considered fine. In practice, only correlations between variables higher than say <span class="math inline">\(0.95\)</span> start to impose problems.</p>
</div>
</div>
</div>
<div id="testing-with-multivariate-regression-models" class="section level3 hasAnchor" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> Testing with multivariate regression models<a href="modeling.html#testing-with-multivariate-regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="hypothesis-tests-and-confidence-intervals-for-a-single-coefficient-in-multiple-regression" class="section level4 hasAnchor" number="6.2.3.1">
<h4><span class="header-section-number">6.2.3.1</span> Hypothesis tests and confidence intervals for a single coefficient in multiple regression<a href="modeling.html#hypothesis-tests-and-confidence-intervals-for-a-single-coefficient-in-multiple-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recall from Subsection <a href="univariateregression.html#sec:unitesting">5.3.2.2</a> that for hypothesis testing in a classical statistical framework we make use of the fact that <span class="math inline">\(\frac{\hat{\beta}_1- E(\hat{\beta}_1)}{\sqrt{var(\hat{\beta}_1)}}\)</span> is approximately distributed as <span class="math inline">\(N(0,1)\)</span> according to the Central Limit theorem. Thus hypotheses on <span class="math inline">\(\beta_1\)</span> can be tested using the usual <span class="math inline">\(t\)</span>-statistic, and confidence intervals are constructed as <span class="math inline">\(\{\hat{\beta}_1 \pm 1.96 SE (\hat{\beta}_1)\}\)</span>. And this finding carries over to the multivariate setting where for <span class="math inline">\(\beta_2,\ldots, \beta_k\)</span> we make use of the same framework. One thing to keep in mind is that <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_2\)</span> are generally not independently distributed—so neither are their <span class="math inline">\(t\)</span>-statistics (more on this later).</p>
<p>Now, if we return to our Californian school district data set then we find that for the univariate case holds:</p>
<p><span class="math display">\[\begin{equation}
TestScore =\underbrace{698.9}_{10.4} - \underbrace{2.28}_{0.52}  STR,
\end{equation}\]</span></p>
<p>And the population regression “line” for the multivariate case is estimated as:
<span class="math display" id="eq:testmulti">\[\begin{equation}
TestScore = \underbrace{686.0}_{8.7} - \underbrace{1.10}_{0.43} STR - \underbrace{0.650}_{0.031} PctEL
    \tag{6.1}
\end{equation}\]</span></p>
<p>Remember, the coefficient on <span class="math inline">\(STR\)</span> in Eq. <a href="modeling.html#eq:testmulti">(6.1)</a> is the effect on <span class="math inline">\(TestScores\)</span> of a unit change in <span class="math inline">\(STR\)</span>, holding constant the percentage of English Learners in the district. The corresponding 95% confidence interval for coefficient on <span class="math inline">\(STR\)</span> in (2) is <span class="math inline">\(\{-1.10 \pm 1.96 \times 0.43\} = (-1.95,-0.26)\)</span>. And the <span class="math inline">\(t\)</span>-statistic testing <span class="math inline">\(\beta_{STR} = 0\)</span> is <span class="math inline">\(t = -1.10/0.43 = -2.54\)</span>, so we reject the null-hypothesis at the 5% significance level. More evidence for the strength of the <span class="math inline">\(PctEL\)</span> variable can be seen from the fact that, under the null-hypothesis of <span class="math inline">\(\beta_2 = 0\)</span>, the following must hold: <span class="math inline">\(t\text{-statistic} = \frac{\hat{\beta_1}}{\sigma_{\hat{\beta_1}}} = \frac{0.65}{0.03} = 21.7\)</span>, which is a very high number for a <span class="math inline">\(t\)</span>-statistic.</p>
</div>
<div id="tests-of-joint-hypotheses" class="section level4 hasAnchor" number="6.2.3.2">
<h4><span class="header-section-number">6.2.3.2</span> Tests of joint hypotheses<a href="modeling.html#tests-of-joint-hypotheses" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>So, testing of single coefficients is just as before. Now in the Californian school district dataset there is as well a variable called <span class="math inline">\(Expn\)</span> denoting the expenditures per pupil. Consider the following population
regression model:
<span class="math display">\[\begin{equation}
TestScore_i = \beta0 + \beta_1 STR_i + \beta_2 Expn_i + \beta_3PctEL_i + u_i
\end{equation}\]</span>
The null hypothesis that “school resources don’t matter” and the alternative that they do, corresponds to:</p>
<ul>
<li><span class="math inline">\(H_0:\beta_1 =0\)</span> and <span class="math inline">\(\beta_2 =0\)</span> vs</li>
<li><span class="math inline">\(H_1:\)</span> either <span class="math inline">\(\beta_1 \neq 0\)</span> or <span class="math inline">\(\beta_2 \neq 0\)</span> or both</li>
</ul>
<p>This is a joint hypothesis specifying a value for two or more coefficients, that is, it imposes a restriction on two or more coefficients. In general, a joint hypothesis will involve <span class="math inline">\(q\)</span> restrictions. In the example above, <span class="math inline">\(q = 2\)</span>, and the two restrictions are <span class="math inline">\(\beta_1 = 0\)</span> and <span class="math inline">\(\beta_2 = 0\)</span>. A “common sense” idea is to reject if either of the individual <span class="math inline">\(t\)</span>-statistics exceeds 1.96 in absolute value. But this “one at a time” test isn’t valid: the resulting test rejects too often under the null hypothesis (more than 5%)! That is because the <span class="math inline">\(t\)</span>-statistics themselved are often not independent. Instead, we need a <span class="math inline">\(F\)</span>-statistic, which tests all parts of a joint hypothesis at once. Unfortunately, these types of formulas can become quickly rather comples. Consider the <span class="math inline">\(F\)</span>-test for the special case of the joint hypothesis <span class="math inline">\(\beta_1 = \beta_{1,0}\)</span> and <span class="math inline">\(\beta_2 = \beta_{2,0}\)</span> in a regression with two regressors:</p>
<p><span class="math display">\[\begin{equation}
F = \frac{1}{2} \left(\frac{t_1^2 + t_2^2 - 2\hat{\rho}_{t_1,t_2}t_1 t_2}{1-\hat{\rho}^2_{t_1 t_2}}  \right)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat{\rho}_{t_1,t_2}\)</span> estimates the correlation between <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span>. Reject when <span class="math inline">\(F\)</span> is large (typically to be determined from large statistical tables). The F-statistic is large when <span class="math inline">\(t_1\)</span> and/or <span class="math inline">\(t_2\)</span> is large and the F-statistic corrects (in just the right way) for the correlation between <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span>. The formula for more than two <span class="math inline">\(\beta\)</span>’s is nasty unless you use matrix algebra. There is a nice large-sample approximate distribution, which is the tail probability of the <span class="math inline">\(\chi^2_q /q\)</span> distribution beyond the <span class="math inline">\(F\)</span>-statistic actually computed.</p>
<p>Now, <code>STATA</code> does this in a much easier way by invoking the <code>test</code> command <strong>right</strong> after the regression. So, for example, we want to test the joint hypothesis that the population coefficients on <span class="math inline">\(STR\)</span> and expenditures per pupil (<span class="math inline">\(expn\)</span>) are both zero, against the alternative that at least one of the population coefficients is nonzero.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb30-1"><a href="modeling.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr str expn_stu el_pct, <span class="fu">r</span> </span>
<span id="cb30-2"><a href="modeling.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="kw">test</span> str expn_stu</span></code></pre></div>
<pre><code>Linear regression                               Number of obs     =        420
                                                F(3, 416)         =     147.20
                                                Prob &gt; F          =     0.0000
                                                R-squared         =     0.4366
                                                Root MSE          =     14.353

------------------------------------------------------------------------------
             |               Robust
     testscr | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -.2863992   .4820728    -0.59   0.553    -1.234002     .661203
    expn_stu |   .0038679   .0015807     2.45   0.015     .0007607    .0069751
      el_pct |  -.6560227   .0317844   -20.64   0.000    -.7185008   -.5935446
       _cons |   649.5779   15.45834    42.02   0.000     619.1917    679.9641
------------------------------------------------------------------------------


 ( 1)  str = 0
 ( 2)  expn_stu = 0

       F(  2,   416) =    5.43
            Prob &gt; F =    0.0047</code></pre>
<p>The output shows an <span class="math inline">\(F\)</span>-statistic with <span class="math inline">\(q=2\)</span> restrictions with outcome 5.43. Do not directly interpret this number, but know that <span class="math inline">\(\text{Prob} &gt; F = 0.0047\)</span> gives the probability that under the null-hypothesis this outcome is produced. So the joint null-hypothesis that both types of expenditures are zero (at the same time), can be rejected at a 5% (and a 1%) significance level. Other types of joint tests can easily be constructed as well. For example, when you want to know whether both coefficient add up to 1, then you would state <code>test str + expn_stu = 1</code>. The final point to make is the <span class="math inline">\(F\)</span>-test in the regression output itself. Here, that is for example <code>F(3, 416) = 147.20</code>. This is joint test that all variables, except the constant, have no impact. So, <span class="math inline">\(\beta_i = 0\)</span> for all <span class="math inline">\(i\)</span> at the <strong>same time</strong>. It not often that you come across a general regression <span class="math inline">\(F\)</span>-test that does not reject the null-hypothesis. It namely implies that your independent variable do not contain information about the dependent variable.</p>
<p>And with the <span class="math inline">\(F\)</span>-test, we now have discussed all regression outcome components displayed by <code>STATA</code>. Most of this information you do not need for your report but we will come back later to this.</p>
</div>
</div>
</div>
<div id="sec:nonlinear" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Non-linear specifications<a href="modeling.html#sec:nonlinear" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <span class="math inline">\(TestScore\)</span>–<span class="math inline">\(STR\)</span> relation looks linear</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb32-1"><a href="modeling.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">graph</span> <span class="kw">twoway</span> (<span class="kw">lfit</span> testscr str) (<span class="kw">scatter</span> testscr str)</span></code></pre></div>
<p>And this provides the following <code>STATA</code> output.</p>
<div class="figure"><span style="display:block;" id="fig:scatterlfitcaschool"></span>
<img src="figures/scatterlfit.png" alt="A linear relation" width="660" />
<p class="caption">
Figure 6.4: A linear relation
</p>
</div>
<p>But the <span class="math inline">\(TestScore\)</span>–<span class="math inline">\(Income\)</span> relation looks</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb33-1"><a href="modeling.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">graph</span> <span class="kw">twoway</span> (<span class="kw">lfit</span> testscr avginc) (<span class="kw">scatter</span> testscr avginc)</span></code></pre></div>
<p>And this provides the following <code>STATA</code> output.</p>
<div class="figure"><span style="display:block;" id="fig:scatterincome"></span>
<img src="figures/scatterincome.png" alt="A non-linear relation" width="660" />
<p class="caption">
Figure 6.5: A non-linear relation
</p>
</div>
<p>Nonlinear regression population regression functions</p>
<p>If a relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is nonlinear:
- The effect on <span class="math inline">\(Y\)</span> of a change in <span class="math inline">\(X\)</span> depends on the value of <span class="math inline">\(X\)</span>—that is, the <em>marginal</em> effect of <span class="math inline">\(X\)</span> is not constant
- A linear regression is misspecified
- The estimator of the effect on <span class="math inline">\(Y\)</span> of <span class="math inline">\(X\)</span> is biased—it needn’t even be right on average.
- The solution to this is to estimate a regression function that is nonlinear in <span class="math inline">\(X\)</span></p>
<p>Nonlinear functions of a single independent variable</p>
<p>We look at two complementary approaches:
- Polynomials in <span class="math inline">\(X\)</span>
- The effect is approximated by a quadratic, cubic, or higher-degree polynomial
- Logarithmic transformations
- <span class="math inline">\(Y\)</span> and/or <span class="math inline">\(X\)</span> is transformed by taking its logarithm
- this gives a percentages interpretation which is often useful</p>
<div id="polynomials" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Polynomials<a href="modeling.html#polynomials" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Approximate the regression function by a polynomial:
<span class="math display">\[\begin{equation}
Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X^2_i + \ldots + \beta_r X_i^r + u_i
\end{equation}\]</span></p>
<p>This is just the linear regression model—except that the regressors are powers of <span class="math inline">\(X\)</span>! Estimation, hypothesis testing, etc. proceeds as in the multiple regression model using OLS</p>
<p>The coefficients are difficult to interpret, but the regression function itself is interpretable</p>
<p>Example: the <span class="math inline">\(TestScore\)</span>–<span class="math inline">\(Income\)</span> relation</p>
<p><span class="math inline">\(Income_i\)</span> = average district income in the <span class="math inline">\(i^{\mathrm{th}}\)</span> district (thousands of dollars per capita)
Quadratic specification:
<span class="math display">\[\begin{equation}
TestScore_i = \beta_0 + \beta_1 Income_i + \beta_2 (Income_i)^2 + u_i
\end{equation}\]</span>
Cubic specification:
<span class="math display">\[\begin{equation}
TestScore_i = \beta_0 + \beta_1 Income_i + \beta_2 (Income_i)^2 +
\beta_3 (Income_i)^3 + u_i
\end{equation}\]</span></p>
<p>Estimation of the quadratic</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb34-1"><a href="modeling.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr c.avginc##c.avginc, <span class="fu">r</span></span></code></pre></div>
<pre><code>Linear regression                               Number of obs     =        420
                                                F(2, 417)         =     428.52
                                                Prob &gt; F          =     0.0000
                                                R-squared         =     0.5562
                                                Root MSE          =     12.724

-----------------------------------------------------------------------------------
                  |               Robust
          testscr | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]
------------------+----------------------------------------------------------------
           avginc |   3.850995   .2680941    14.36   0.000      3.32401    4.377979
                  |
c.avginc#c.avginc |  -.0423085   .0047803    -8.85   0.000     -.051705   -.0329119
                  |
            _cons |   607.3017   2.901754   209.29   0.000     601.5978    613.0056
-----------------------------------------------------------------------------------</code></pre>
<p>Test the null hypothesis of linearity against the alternative that the regression function is a quadratic</p>
<p>Digression: nonlinearities and plots in stata</p>
<p>There are four factor-variable operators:</p>
<ul>
<li><code>i.</code> operator to specify indicators (dummies)</li>
<li><code>c.</code> operator to treat as continuous</li>
<li><code>#</code> binary operator to specify interactions</li>
<li><code>##</code> binary operator to specify factorial interactions</li>
</ul>
<div class="sourceCode" id="cb36"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb36-1"><a href="modeling.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">predict</span> hat1 </span>
<span id="cb36-2"><a href="modeling.html#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="kw">scatter</span> (testscr avginc) || (<span class="kw">line</span> hat1 avginc, <span class="kw">sort</span>)</span></code></pre></div>
<p>And this provides the following <code>STATA</code> output.</p>
<div class="figure"><span style="display:block;" id="fig:scatterqua"></span>
<img src="figures/scatterqua.png" alt="A non-linear relation" width="660" />
<p class="caption">
Figure 6.6: A non-linear relation
</p>
</div>
<p>Compute “effects” for different values of <span class="math inline">\(X\)</span>
<span class="math display">\[\begin{equation}
\widehat{TestScore_i} = 607.3 + 3.85 Income_i - 0.0423(Income_i)^2
\end{equation}\]</span>
Predicted change in TestScore for a change in income from $5,000 per capita to $6,000 per capita:
<span class="math display">\[\begin{eqnarray}
\Delta \widehat{TestScore} &amp;=&amp; 607.3 + 3.85 \times 6 -  0.0423 \times 6^2 \\
&amp;&amp; - (607.3 + 3.85\times 5 - 0.0423\times 5^2)\\
&amp;=&amp;3.4
\end{eqnarray}\]</span></p>
<p>Predicted “effects” for different values of <span class="math inline">\(X\)</span>:}</p>
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:effectqua">Table 6.1: </span>Effect of <span class="math inline">\(X\)</span>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Change in Income (1000 dollar per capita)
</th>
<th style="text-align:left;">
<span class="math inline">\(\Delta \widehat{TestScore}\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
from 5 to 6
</td>
<td style="text-align:left;">
3.4
</td>
</tr>
<tr>
<td style="text-align:left;">
from 25 to 26
</td>
<td style="text-align:left;">
1.7
</td>
</tr>
<tr>
<td style="text-align:left;">
from 45 to 46
</td>
<td style="text-align:left;">
0.0
</td>
</tr>
</tbody>
</table>
<p>The “effect” of a change in income is greater at low than high income levels (perhaps, a declining marginal benefit of an increase in school budgets?)</p>
<p>Caution! What is the effect of a change from 65 to 66? Don’t extrapolate outside the range of the data!</p>
<p>Estimation of a cubic</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb37-1"><a href="modeling.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr c.avginc##c.avginc##c.avginc, <span class="fu">r</span></span></code></pre></div>
<pre><code>Linear regression                               Number of obs     =        420
                                                F(3, 416)         =     270.18
                                                Prob &gt; F          =     0.0000
                                                R-squared         =     0.5584
                                                Root MSE          =     12.707

--------------------------------------------------------------------------------------------
                           |               Robust
                   testscr | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]
---------------------------+----------------------------------------------------------------
                    avginc |   5.018677   .7073504     7.10   0.000      3.62825    6.409103
                           |
         c.avginc#c.avginc |  -.0958052   .0289537    -3.31   0.001     -.152719   -.0388913
                           |
c.avginc#c.avginc#c.avginc |   .0006855   .0003471     1.98   0.049     3.26e-06    .0013677
                           |
                     _cons |    600.079   5.102062   117.61   0.000     590.0499     610.108
--------------------------------------------------------------------------------------------</code></pre>
<p>Testing the null hypothesis of linearity</p>
<p>Alternative hypothesis: the population regression is quadratic and/or cubic, that is, it is a polynomial of degree up to 3:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: Coefficients on <span class="math inline">\(Income^2\)</span> and <span class="math inline">\(Income^3 = 0\)</span></li>
<li><span class="math inline">\(H_1\)</span>: at least one of these coefficients is nonzero.</li>
</ul>
<p>The hypothesis that the population regression is linear is rejected at the 1% significance level against the alternative that it is a polynomial of degree up to 3.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb39-1"><a href="modeling.html#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">test</span> avginc#avginc avginc#avginc#avginc  </span></code></pre></div>
<pre><code> ( 1)  c.avginc#c.avginc = 0
 ( 2)  c.avginc#c.avginc#c.avginc = 0

       F(  2,   416) =   37.69
            Prob &gt; F =    0.0000</code></pre>
</div>
<div id="interaction-variables" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Interaction variables<a href="modeling.html#interaction-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Interactions between independent variables</p>
<p>Perhaps a class size reduction is more effective in some
circumstances than in others</p>
<p>Perhaps smaller classes help more if there are many English
learners, who need individual attention</p>
<p>That is, <span class="math inline">\(\frac{\partial TestScore}{\partial STR}\)</span> might on <span class="math inline">\(PctEL\)</span></p>
<p>More generally, <span class="math inline">\(\frac{\partial Y}{\partial X_1}\)</span> might on <span class="math inline">\(X_2\)</span></p>
<div id="interactions-between-two-binary-variables" class="section level4 hasAnchor" number="6.3.2.1">
<h4><span class="header-section-number">6.3.2.1</span> Interactions between two binary variables<a href="modeling.html#interactions-between-two-binary-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[\begin{equation}
Y_i =\beta_0 +\beta_1 D_{1i} + \beta_2 D_{2i} +u_i
\end{equation}\]</span></p>
<p><span class="math inline">\(D_{1i}\)</span> and $ D_{2i}$ are binary</p>
<p><span class="math inline">\(\beta_1\)</span> is the effect of changing <span class="math inline">\(D_1=0\)</span> to <span class="math inline">\(D_1=1\)</span>. In this specification, this effect doesn’t depend on the value of <span class="math inline">\(D_2\)</span>.</p>
<p>To allow the effect of changing <span class="math inline">\(D_1\)</span> to depend on <span class="math inline">\(D_2\)</span>, include the interaction term <span class="math inline">\(D_{1i} \times D_{2i}\)</span> as a regressor:\</p>
<p><span class="math display">\[\begin{equation}
Y_i =\beta_0 +\beta_1 D_{1i} + \beta_2 D_{2i} + \beta_3 (D_{1i} \times
D_{2i}) + u_i
\end{equation}\]</span></p>
<p>Interpreting the coefficients</p>
<p><span class="math display">\[\begin{equation}
Y_i =\beta_0 +\beta_1 D_{1i} + \beta_2 D_{2i} + \beta_3 (D_{1i} \times
D_{2i}) + u_i
\end{equation}\]</span>
General rule: compare the various cases:</p>
<p><span class="math display">\[\begin{eqnarray}
E(Y_i|D_{1i}=0, D_{2i}=d_2) &amp;=&amp; \beta_0 + \beta_2 d_2 \\
E(Y_i|D_{1i}=1, D_{2i}=d_2) &amp;=&amp; \beta_0 + \beta_1 + \beta_2 d_2 + \beta_3 d_2
\end{eqnarray}\]</span></p>
<p>subtract from each other:
<span class="math display">\[\begin{equation}
E(Y_i|D_{1i}=1, D_{2i}=d2) - E(Y_i|D_{1i}=0, D_{2i}=d_2) = \beta_1 +
\beta_3 d_2
\end{equation}\]</span></p>
<p>The effect of <span class="math inline">\(D_1\)</span> depends on <span class="math inline">\(d_2\)</span></p>
<p><span class="math inline">\(\beta_3\)</span> = increment to the effect of <span class="math inline">\(D_1\)</span>, when <span class="math inline">\(D_2 = 1\)</span>\</p>
<p>Example: <span class="math inline">\(TestScore\)</span>, <span class="math inline">\(STR\)</span>, English learners</p>
<p>Let:
<span class="math display">\[\begin{eqnarray}
HiSTR &amp;=&amp; 1 \text{ if } STR \geq 20 \text{ and } HiEL = 1 \text{ if }
PctEL \geq 10 \\
HiSTR &amp;=&amp; 0 \text{ if } STR &lt; 20 \text{ and } HiEL = 0 \text{ if }
PctEL &lt; 10 \\
\end{eqnarray}\]</span>
<span class="math display">\[\begin{equation}
\widehat{TestScore} = 664.1 - 18.2 HiEL - 1.9 HiSTR - 3.5(HiSTR \times
HiEL)
\end{equation}\]</span></p>
<p>“Effect” of <span class="math inline">\(HiSTR\)</span> when <span class="math inline">\(HiEL = 0\)</span> is <span class="math inline">\(-1.9\)</span></p>
<p>“Effect” of <span class="math inline">\(HiSTR\)</span> when <span class="math inline">\(HiEL = 1\)</span> is <span class="math inline">\(-1.9 - 3.5 = -5.4\)</span></p>
<p>Class size reduction is estimated to have a bigger effect when the percent of English learners is large</p>
<p>This interaction isn’t statistically significant: <span class="math inline">\(t = 3.5/3.1\)</span></p>
</div>
<div id="interactions-between-continuous-and-binary-variables" class="section level4 hasAnchor" number="6.3.2.2">
<h4><span class="header-section-number">6.3.2.2</span> Interactions between continuous and binary variables<a href="modeling.html#interactions-between-continuous-and-binary-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[\begin{equation}
Y_i =\beta_0 +\beta_1 D_i + \beta_2 X_i +u_i
\end{equation}\]</span></p>
<p><span class="math inline">\(D_i\)</span> is binary, <span class="math inline">\(X\)</span> is continuous</p>
<p>As specified above, the effect on <span class="math inline">\(Y\)</span> of <span class="math inline">\(X\)</span> (holding constant <span class="math inline">\(D\)</span>) = <span class="math inline">\(\beta_2\)</span>, which does not depend on <span class="math inline">\(D\)</span></p>
<p>To allow the effect of <span class="math inline">\(X\)</span> to depend on <span class="math inline">\(D\)</span>, include the interaction term <span class="math inline">\(D_i \times X_i\)</span> as a regressor:</p>
<p><span class="math display">\[\begin{equation}
Y_i =\beta_0 +\beta_1 D_i + \beta_2 X_i + \beta_3 (D_i \times X_i) + u_i
\end{equation}\]</span></p>
<p>Binary-continuous interactions: the two regression lines
<span class="math display">\[\begin{equation}
Y_i =\beta_0 +\beta_1 D_i + \beta_2 X_i + \beta_3 (D_i \times X_i) + u_i
\end{equation}\]</span>
Observations with <span class="math inline">\(D_i= 0\)</span> (the <span class="math inline">\(D = 0\)</span> group or the <span class="math inline">\(D=0\)</span> regression line):
<span class="math display">\[\begin{equation}
Y_i = \beta_0 + \beta_2 X_i  + u_i
\end{equation}\]</span>
Observations with <span class="math inline">\(D_i= 1\)</span> (the <span class="math inline">\(D = 1\)</span> group or the <span class="math inline">\(D = 1\)</span> regression line):
<span class="math display">\[\begin{eqnarray}
Y_i &amp;=&amp;   \beta_0 + \beta_1 + \beta_2 X_i + \beta_3 X_i + u_i \\
            &amp;=&amp;  (\beta_0 + \beta_1) + (\beta_2 + \beta_3) X_i + u_i
\end{eqnarray}\]</span></p>
<p>Binary-continuous interactions, ctd.</p>
<div class="figure"><span style="display:block;" id="fig:interaction"></span>
<img src="figures/Sheet44.jpg" alt="A non-linear relation"  />
<p class="caption">
Figure 6.7: A non-linear relation
</p>
</div>
<p>Interpreting the coefficients</p>
<p><span class="math display">\[\begin{equation}
Y_i =\beta_0 +\beta_1 D_i + \beta_2 X_i + \beta_3 (D_i \times X_i) + u_i
\end{equation}\]</span></p>
<p>General rule: take the marginal effect of
<span class="math display">\[\begin{equation}
Y =\beta_0 +\beta_1 D + \beta_2 X + \beta_3 (D \times X)
\end{equation}\]</span>
<span class="math display">\[\begin{equation}
\frac{\partial Y}{\partial X} = \beta_2 + \beta_3 D
\end{equation}\]</span>
The effect of <span class="math inline">\(X\)</span> depends on <span class="math inline">\(D\)</span></p>
<p><span class="math inline">\(\beta_3=\)</span> increment to the effect of <span class="math inline">\(X\)</span>, when <span class="math inline">\(D = 1\)</span></p>
<p>Example: <span class="math inline">\(TestScore\)</span>, <span class="math inline">\(STR\)</span>, <span class="math inline">\(HiEL\)</span></p>
<p><span class="math display">\[\begin{equation}
\widehat{TestScore} = 682.2 - 0.97 STR + 5.6 HiEL - 1.28(STR \times HiEL)
\end{equation}\]</span></p>
<p>When <span class="math inline">\(HiEL = 0\)</span>:\</p>
<p><span class="math display">\[\begin{equation}
\widehat{TestScore} = 682.2 - 0.97 STR
\end{equation}\]</span></p>
<p>When <span class="math inline">\(HiEL = 1\)</span>:</p>
<p><span class="math display">\[\begin{eqnarray}
\widehat{TestScore} &amp;=&amp; 682.2 - 0.97 STR + 5.6 - 1.28 STR \\
&amp;=&amp; 687.8 - 2.25 STR
\end{eqnarray}\]</span></p>
<p>Two regression lines: one for each <span class="math inline">\(HiSTR\)</span> group.</p>
<p>Class size reduction is estimated to have a larger effect when the percent of English learners is large.</p>
<p>Example, ctd: Testing hypotheses</p>
<p><span class="math display">\[\begin{equation}
\widehat{TestScore} = 682.2 - 0.97 STR + 5.6 HiEL - 1.28(STR \times HiEL)
\end{equation}\]</span></p>
<p>The two regression lines have the same slope—the coefficient on <span class="math inline">\(STR \times HiEL\)</span> is zero: <span class="math inline">\(t = -1.28/0.97 = -1.32\)</span></p>
<p>The two regression lines have the same intercept—the coefficient on <span class="math inline">\(HiEL\)</span> is zero: <span class="math inline">\(t = -5.6/19.5 = 0.29\)</span></p>
<p>The two regression lines are the same—population coefficient on <span class="math inline">\(HiEL = 0\)</span> and population coefficient on <span class="math inline">\(STR \times HiEL = 0\)</span>: <span class="math inline">\(F = 89.94 (p-value &lt; .001)\)</span>
We reject the joint hypothesis but neither individual hypothesis (how can this be?)</p>
</div>
<div id="interactions-between-two-continuous-variables" class="section level4 hasAnchor" number="6.3.2.3">
<h4><span class="header-section-number">6.3.2.3</span> Interactions between two continuous variables}<a href="modeling.html#interactions-between-two-continuous-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> are continuous</p>
<p>As specified, the effect of <span class="math inline">\(X_1\)</span> doesn’t depend on <span class="math inline">\(X_2\)</span></p>
<p>As specified, the effect of <span class="math inline">\(X_2\)</span> doesn’t depend on <span class="math inline">\(X_1\)</span></p>
<p>To allow the effect of <span class="math inline">\(X_1\)</span> to depend on <span class="math inline">\(X_2\)</span>, include the “interaction term” <span class="math inline">\(X_{1i} \times X_{2i}\)</span> as a regressor:</p>
<p>Interpreting the coefficients:</p>
<p><span class="math display">\[\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 (X_{1i}
\times X_{2i}) + u_i
\end{equation}\]</span></p>
<p>General rule: compare the various cases</p>
<p>Now change <span class="math inline">\(X_1\)</span>:</p>
<p>subtract from each other</p>
<p>The effect of <span class="math inline">\(X_1\)</span> depends on <span class="math inline">\(X_2\)</span> (what we wanted)</p>
<p><span class="math inline">\(\beta_3\)</span> = increment to the effect of <span class="math inline">\(X_1\)</span> from a unit change in <span class="math inline">\(X_2\)</span></p>
</div>
</div>
<div id="logarithmic-transformation" class="section level3 hasAnchor" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Logarithmic transformation<a href="modeling.html#logarithmic-transformation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Logarithmic functions of <span class="math inline">\(Y\)</span> and/or <span class="math inline">\(X\)</span></p>
<p><span class="math inline">\(\ln(X) =\)</span> the natural logarithm of <span class="math inline">\(X\)</span>
Logarithmics permit modeling relations in percentage terms (like elasticities), rather than linearly.</p>
<p>Here’s why:</p>
<p><span class="math display">\[\begin{equation}
\ln(x+\Delta x) - \ln(x) = \ln (1 + \frac{\Delta x}{x}) \cong \frac{\Delta x}{x}
\end{equation}\]</span>
(calculus: <span class="math inline">\(\frac{d \ln(x)}{dx}=\frac{1}{x})\)</span>
Numerically: <span class="math inline">\(\ln(1.01) = .00995 \cong .01\)</span>;
- <span class="math inline">\(\ln(1.10) = .0953 \cong .10\)</span> (sort of)</p>
<p>Rules for natural logarithms
1. <span class="math inline">\(\ln(a\times b)= \ln(a)+\ln(b)\)</span>
2. <span class="math inline">\(\ln(\frac{a}{b}) =\ln(a) - \ln(b)\)</span>
3. <span class="math inline">\(\ln(a^\alpha) = \alpha \ln(a)\)</span></p>
<p>If nonlinear models: try log-linearizing!</p>
<p><span class="math display">\[\begin{equation}
Y = A K^\alpha L^{1-\alpha} \rightarrow \ln(Y) = \ln(A) + \alpha \ln(K) + (1-\alpha) \ln(L)
\end{equation}\]</span>
Note: similar operations to LHS and RHS!</p>
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:logspecifications">Table 6.2: </span>Three logarithmic transformation
</caption>
<thead>
<tr>
<th style="text-align:left;">
Case
</th>
<th style="text-align:left;">
Population regression model
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
linear-log
</td>
<td style="text-align:left;">
<span class="math inline">\(Y_i=\beta_0 + \beta_1 \ln(X_i) + u_i\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
log_linear
</td>
<td style="text-align:left;">
<span class="math inline">\(\ln(Y_i)=\beta_0 + \beta_1 (X_i) + u_i\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
log-log
</td>
<td style="text-align:left;">
<span class="math inline">\(\ln(Y_i)=\beta_0 + \beta_1 \ln(X_i) + u_i\)</span>
</td>
</tr>
</tbody>
</table>
<p>The interpretation of the slope coefficient differs in each case.
The interpretation is finding the marginal effect of X using the first derivative</p>
<div id="linear-log-population-regression-model" class="section level4 hasAnchor" number="6.3.3.1">
<h4><span class="header-section-number">6.3.3.1</span> Linear-log population regression model<a href="modeling.html#linear-log-population-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The linear-log population regression model is specified as:</p>
<p><span class="math display">\[\begin{equation}
    Y = \beta_0 + \beta_1 \ln(X)
\end{equation}\]</span></p>
<p>Now take first derivative:</p>
<p><span class="math display">\[\begin{equation}
    \frac{\partial Y}{\partial X} = \frac{\beta_1}{X}
\end{equation}\]</span>
so
<span class="math display">\[\begin{equation}
    \beta_1  = \frac{\partial Y}{\partial X / X}
\end{equation}\]</span></p>
<p>Example: TestScore vs. ln(Income)</p>
<p>First defining the new regressor, <span class="math inline">\(\ln(Income)\)</span></p>
<p>The model is now linear in <span class="math inline">\(\ln(Income)\)</span>, so the linear-log model can be estimated by OLS:
<span class="math display">\[\begin{equation}
        \widehat{TestScore} = 557.8 + 36.42\times \ln(Income_i)
\end{equation}\]</span>
so a 1% increase in <span class="math inline">\(Income\)</span> is associated with an increase in TestScore of 0.36 points on the test.
Standard errors, confidence intervals, <span class="math inline">\(R^2\)</span>—all the usual tools of regression apply here.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb41-1"><a href="modeling.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> lninc = <span class="fu">ln</span>(avginc)</span>
<span id="cb41-2"><a href="modeling.html#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr lninc, <span class="fu">r</span></span>
<span id="cb41-3"><a href="modeling.html#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="kw">predict</span> testhat</span>
<span id="cb41-4"><a href="modeling.html#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="kw">graph</span> <span class="kw">twoway</span> (<span class="kw">line</span> testhat avginc, <span class="kw">sort</span>) (<span class="kw">scatter</span> testscr avginc)</span></code></pre></div>
<p>And this provides the following <code>STATA</code> output.</p>
<div class="figure"><span style="display:block;" id="fig:scatterlnincome"></span>
<img src="figures/scatterlnincome.png" alt="A non-linear relation" width="660" />
<p class="caption">
Figure 6.8: A non-linear relation
</p>
</div>
</div>
<div id="log-linear-population-regression-model" class="section level4 hasAnchor" number="6.3.3.2">
<h4><span class="header-section-number">6.3.3.2</span> Log-linear population regression model<a href="modeling.html#log-linear-population-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[\begin{equation}
    \ln(Y) = \beta_0 + \beta_1 X
\end{equation}\]</span></p>
<p>Now take first derivative <span class="math inline">\(\frac{\partial Y}{\partial X}\)</span>:
<span class="math display">\[\begin{equation}
    Y = exp( \beta_0 + \beta_1 X )
\end{equation}\]</span>
So
<span class="math display">\[\begin{equation}
    \frac{\partial Y}{\partial X} = \beta_1  exp( \beta_0 + \beta_1 X ) = \beta_1 Y
\end{equation}\]</span>
Thus
<span class="math display">\[\begin{equation}
    \beta_1  = \frac{\partial Y / Y}{\partial X }
\end{equation}\]</span></p>
</div>
<div id="log-log-population-regression-model" class="section level4 hasAnchor" number="6.3.3.3">
<h4><span class="header-section-number">6.3.3.3</span> Log-log population regression model<a href="modeling.html#log-log-population-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[\begin{equation}
    \ln(Y) = \beta_0 + \beta_1 \ln(X)
\end{equation}\]</span></p>
<p>Now take first derivative <span class="math inline">\(\frac{\partial Y}{\partial X}\)</span>:
<span class="math display">\[\begin{equation}
    Y = exp( \beta_0 + \beta_1 \ln(X) )
\end{equation}\]</span>
So
<span class="math display">\[\begin{equation}
    \frac{\partial Y}{\partial X} = \beta_1 /X  exp( \beta_0 + \beta_1 \ln(X) ) = \beta_1 Y /X
\end{equation}\]</span>
Thus an <strong>elasticity</strong>
<span class="math display">\[\begin{equation}
    \beta_1  = \frac{\partial Y / Y}{\partial X / X }
\end{equation}\]</span></p>
<p>Example: ln( TestScore) vs. ln(Income)</p>
<p>First define a new dependent variable, ln(TestScore), and the new regressor, ln(Income)
The model is now a linear regression of ln(TestScore) against ln(Income), which can be estimated by OLS:
<span class="math display">\[\begin{equation}
\widehat{ln(TestScore)} = 6.336 + 0.0554 \times ln(Income_i)
\end{equation}\]</span></p>
<ul>
<li>An 1% increase in <span class="math inline">\(Income\)</span> is associated with an increase of .0554% in <span class="math inline">\(TestScore\)</span> (<span class="math inline">\(Income\)</span> up by a factor of 1.01, <span class="math inline">\(TestScore\)</span> up by a factor of 1.000554)</li>
</ul>
<div class="sourceCode" id="cb42"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb42-1"><a href="modeling.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> lninc = <span class="fu">ln</span>(avginc)</span>
<span id="cb42-2"><a href="modeling.html#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> lntestscr = <span class="fu">ln</span>(testscr)</span>
<span id="cb42-3"><a href="modeling.html#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> lntestscr lninc, <span class="fu">r</span></span>
<span id="cb42-4"><a href="modeling.html#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="kw">predict</span> testhat1</span>
<span id="cb42-5"><a href="modeling.html#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> lntestscr avginc, <span class="fu">r</span></span>
<span id="cb42-6"><a href="modeling.html#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="kw">predict</span> testhat2</span>
<span id="cb42-7"><a href="modeling.html#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="kw">graph</span> <span class="kw">twoway</span> (<span class="kw">line</span> testhat1 avginc, <span class="kw">sort</span>) (<span class="kw">line</span> testhat2 avginc, <span class="kw">sort</span>) (<span class="kw">scatter</span> lntestscr avginc), <span class="bn">legend</span>(<span class="kw">order</span>(1 <span class="st">&quot;log-log specification&quot;</span> 2 <span class="st">&quot;log-linear specification&quot;</span> 3 <span class="st">&quot;Observations&quot;</span>)) </span></code></pre></div>
<p>And this provides the following <code>STATA</code> output.</p>
<div class="figure"><span style="display:block;" id="fig:scattercompare"></span>
<img src="figures/scattercompare.png" alt="A non-linear relation" width="660" />
<p class="caption">
Figure 6.9: A non-linear relation
</p>
</div>
<p>Summary: Logarithmic transformations</p>
<p>Three cases, differing in whether <span class="math inline">\(Y\)</span> and/or <span class="math inline">\(X\)</span> is transformed by taking logarithms.
- The regression is linear in the new variable(s) <span class="math inline">\(\ln(Y)\)</span> and/or <span class="math inline">\(\ln(X)\)</span>, and the coefficients can be estimated by OLS.
- Hypothesis tests and confidence intervals are now implemented and interpreted “as usual”.
- The interpretation of <span class="math inline">\(\beta_1\)</span> differs from case to case.
- Choice of specification should be guided by judgment (which interpretation makes the most sense in your application?), tests, and plotting predicted values</p>
</div>
</div>
</div>
<div id="sec:fixedeffects" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Using fixed effects in panel data<a href="modeling.html#sec:fixedeffects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="conclusion-and-discussion-2" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Conclusion and discussion<a href="modeling.html#conclusion-and-discussion-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="30">
<li id="fn30"><p>With right hand side we mean on the right side of the equal sign <span class="math inline">\(=\)</span>. It is often abbreviated with RHS.<a href="modeling.html#fnref30" class="footnote-back">↩︎</a></p></li>
<li id="fn31"><p>This is not entirely true. Increasing the R<span class="math inline">\(^2\)</span> explains <strong>in-sample</strong> variation better, not necessarily <strong>out-of-sample</strong>. The latter is really what matters for prediction and this is the focus of many machine learning techniques. Note that this argument is directly related with the regression towards the mean argument made in Subsection <a href="univariateregression.html#sec:genesis">5.3.1</a>.<a href="modeling.html#fnref31" class="footnote-back">↩︎</a></p></li>
<li id="fn32"><p><span class="math inline">\(Z\)</span> can be both known or unknown, so that is why we change from <span class="math inline">\(U\)</span> to <span class="math inline">\(Z\)</span><a href="modeling.html#fnref32" class="footnote-back">↩︎</a></p></li>
<li id="fn33"><p>In econometric textbooks, as, e.g, in <span class="citation">Stock, Watson, et al. (2003)</span>, this condition is weakened to only being correlation (<span class="math inline">\(Z\)</span> and <span class="math inline">\(X\)</span> are correlated). However, if the directed arrow goes from <span class="math inline">\(STR\)</span> into <span class="math inline">\(U\)</span> in Figure <a href="modeling.html#fig:unobshet">6.2</a> then that would lead to something else than omitted variables, namely to a difference between a direct (<span class="math inline">\(\text{STR} \longrightarrow \text{testscore}\)</span>) and an indirect effect (<span class="math inline">\(\text{STR} \longrightarrow U \longrightarrow \text{testscore}\)</span>).<a href="modeling.html#fnref33" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="univariateregression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="specification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
