<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Modeling in the Social Sciences | Methods and Techniques for Social and Economic Research: Syllabus</title>
  <meta name="description" content="<p>This syllabus contains lecture notes for the course
Methods and Techniques for Social and Economic Research for the program
Earth, Economics and Sustainability</p>" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Modeling in the Social Sciences | Methods and Techniques for Social and Economic Research: Syllabus" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This syllabus contains lecture notes for the course
Methods and Techniques for Social and Economic Research for the program
Earth, Economics and Sustainability</p>" />
  <meta name="github-repo" content="Thdegraaff/methods" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Modeling in the Social Sciences | Methods and Techniques for Social and Economic Research: Syllabus" />
  
  <meta name="twitter:description" content="<p>This syllabus contains lecture notes for the course
Methods and Techniques for Social and Economic Research for the program
Earth, Economics and Sustainability</p>" />
  

<meta name="author" content="Paul Koster &amp; Thomas de Graaff" />


<meta name="date" content="2022-11-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="univariateregression.html"/>
<link rel="next" href="specification.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what"><i class="fa fa-check"></i>What</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why"><i class="fa fa-check"></i>Why</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#for-whom"><i class="fa fa-check"></i>For Whom</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#theory-models-and-hypotheses"><i class="fa fa-check"></i><b>1.1</b> Theory, Models and Hypotheses</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#doing-research-in-the-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Doing Research (in the Social Sciences)</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#work-tidy"><i class="fa fa-check"></i><b>1.2.1</b> Work tidy</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#know-where-your-stuff-is"><i class="fa fa-check"></i><b>1.2.2</b> Know where your stuff is</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#make-notes"><i class="fa fa-check"></i><b>1.2.3</b> Make notes</a></li>
<li class="chapter" data-level="1.2.4" data-path="introduction.html"><a href="introduction.html#use-a-reference-manager"><i class="fa fa-check"></i><b>1.2.4</b> Use a reference manager!</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#statistical-software"><i class="fa fa-check"></i><b>1.3</b> Statistical software</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#reading-guide"><i class="fa fa-check"></i><b>1.4</b> Reading Guide</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="surplus.html"><a href="surplus.html"><i class="fa fa-check"></i><b>2</b> Introduction to Economic Surplus</a>
<ul>
<li class="chapter" data-level="2.1" data-path="surplus.html"><a href="surplus.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="surplus.html"><a href="surplus.html#background"><i class="fa fa-check"></i><b>2.1.1</b> Background</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="surplus.html"><a href="surplus.html#sec:conchoices"><i class="fa fa-check"></i><b>2.2</b> Consumer choices and consumer value</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="surplus.html"><a href="surplus.html#utility-functions-inverse-demand-and-demand"><i class="fa fa-check"></i><b>2.2.1</b> Utility functions, inverse demand and demand</a></li>
<li class="chapter" data-level="2.2.2" data-path="surplus.html"><a href="surplus.html#examples-of-demand-and-inverse-demand-functions"><i class="fa fa-check"></i><b>2.2.2</b> Examples of demand and inverse demand functions</a></li>
<li class="chapter" data-level="2.2.3" data-path="surplus.html"><a href="surplus.html#consumer-benefits-and-surplus-in-markets"><i class="fa fa-check"></i><b>2.2.3</b> Consumer benefits and surplus in markets</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="surplus.html"><a href="surplus.html#sec:procbeh"><i class="fa fa-check"></i><b>2.3</b> Poducer behaviour and surplus</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="surplus.html"><a href="surplus.html#producer-cost-functions-and-cost-minimisation"><i class="fa fa-check"></i><b>2.3.1</b> Producer cost functions and cost minimisation</a></li>
<li class="chapter" data-level="2.3.2" data-path="surplus.html"><a href="surplus.html#specifying-and-interpreting-the-production-function"><i class="fa fa-check"></i><b>2.3.2</b> Specifying and interpreting the production function</a></li>
<li class="chapter" data-level="2.3.3" data-path="surplus.html"><a href="surplus.html#the-cost-function"><i class="fa fa-check"></i><b>2.3.3</b> The cost function</a></li>
<li class="chapter" data-level="2.3.4" data-path="surplus.html"><a href="surplus.html#special-case-constant-average-and-marginal-costs"><i class="fa fa-check"></i><b>2.3.4</b> Special case: constant average and marginal costs</a></li>
<li class="chapter" data-level="2.3.5" data-path="surplus.html"><a href="surplus.html#empirical-examples"><i class="fa fa-check"></i><b>2.3.5</b> Empirical examples</a></li>
<li class="chapter" data-level="2.3.6" data-path="surplus.html"><a href="surplus.html#from-firm-costs-to-market-inverse-supply-and-supply"><i class="fa fa-check"></i><b>2.3.6</b> From firm costs to market inverse supply and supply</a></li>
<li class="chapter" data-level="2.3.7" data-path="surplus.html"><a href="surplus.html#producer-surplus"><i class="fa fa-check"></i><b>2.3.7</b> Producer surplus</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="surplus.html"><a href="surplus.html#sec:econsurplus"><i class="fa fa-check"></i><b>2.4</b> Analysis of economic surplus</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="surplus.html"><a href="surplus.html#introduction-2"><i class="fa fa-check"></i><b>2.4.1</b> Introduction</a></li>
<li class="chapter" data-level="2.4.2" data-path="surplus.html"><a href="surplus.html#equilibrium"><i class="fa fa-check"></i><b>2.4.2</b> Equilibrium</a></li>
<li class="chapter" data-level="2.4.3" data-path="surplus.html"><a href="surplus.html#calibration-of-equilibrium"><i class="fa fa-check"></i><b>2.4.3</b> Calibration of equilibrium</a></li>
<li class="chapter" data-level="2.4.4" data-path="surplus.html"><a href="surplus.html#analysis-of-economic-surplus"><i class="fa fa-check"></i><b>2.4.4</b> Analysis of economic surplus</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="surplus.html"><a href="surplus.html#sec:extcosts"><i class="fa fa-check"></i><b>2.5</b> External costs and economic surplus</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="surplus.html"><a href="surplus.html#introduction-3"><i class="fa fa-check"></i><b>2.5.1</b> Introduction</a></li>
<li class="chapter" data-level="2.5.2" data-path="surplus.html"><a href="surplus.html#general-analysis-of-the-consumer-externality-tax"><i class="fa fa-check"></i><b>2.5.2</b> General analysis of the consumer externality tax</a></li>
<li class="chapter" data-level="2.5.3" data-path="surplus.html"><a href="surplus.html#consumer-tax-for-specific-inverse-demand-and-supply-functions"><i class="fa fa-check"></i><b>2.5.3</b> Consumer tax for specific inverse demand and supply functions</a></li>
<li class="chapter" data-level="2.5.4" data-path="surplus.html"><a href="surplus.html#stylised-solutions-for-the-case-when-marginal-external-costs-are-proportional-to-the-equilibrium-price."><i class="fa fa-check"></i><b>2.5.4</b> Stylised solutions for the case when marginal external costs are proportional to the equilibrium price.</a></li>
<li class="chapter" data-level="2.5.5" data-path="surplus.html"><a href="surplus.html#analysis-of-changes-in-economic-surplus-due-to-externality-taxation"><i class="fa fa-check"></i><b>2.5.5</b> Analysis of changes in economic surplus due to externality taxation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="surplus.html"><a href="surplus.html#conclusion"><i class="fa fa-check"></i><b>2.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html"><i class="fa fa-check"></i><b>3</b> Behavioural Error and Economic Surplus</a>
<ul>
<li class="chapter" data-level="3.1" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#introduction-4"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#arguments"><i class="fa fa-check"></i><b>3.2</b> Arguments against neo-classical valuation and responses</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#introduction-5"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#discussion-of-premise-a"><i class="fa fa-check"></i><b>3.2.2</b> Discussion of Premise A</a></li>
<li class="chapter" data-level="3.2.3" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#discussion-of-premise-b"><i class="fa fa-check"></i><b>3.2.3</b> Discussion of premise (B)</a></li>
<li class="chapter" data-level="3.2.4" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#discussion-of-premise-c"><i class="fa fa-check"></i><b>3.2.4</b> Discussion of premise C</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#choicemodelserrors"><i class="fa fa-check"></i><b>3.3</b> Behavioural choice models with errors</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#introduction-8"><i class="fa fa-check"></i><b>3.3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.3.2" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#approach1"><i class="fa fa-check"></i><b>3.3.2</b> Approach 1: separated decision and experienced utility</a></li>
<li class="chapter" data-level="3.3.3" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#approach2"><i class="fa fa-check"></i><b>3.3.3</b> Approach 2: satisficing and the inverse demand curve</a></li>
<li class="chapter" data-level="3.3.4" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#approach3"><i class="fa fa-check"></i><b>3.3.4</b> Approach 3: System I and system II thinking</a></li>
<li class="chapter" data-level="3.3.5" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#approach4"><i class="fa fa-check"></i><b>3.3.5</b> Approach 4: direct utility weights.</a></li>
<li class="chapter" data-level="3.3.6" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#conclusion-1"><i class="fa fa-check"></i><b>3.3.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#beherrorsurplus"><i class="fa fa-check"></i><b>3.4</b> Behavioural errors and economic social surplus</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#implications-of-behavioural-errors-for-consumer-surplus"><i class="fa fa-check"></i><b>3.4.1</b> Implications of behavioural errors for consumer surplus</a></li>
<li class="chapter" data-level="3.4.2" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#the-impact-of-behaviour-error-on-supply-decisions-and-producer-surplus"><i class="fa fa-check"></i><b>3.4.2</b> The impact of behaviour error on supply decisions and producer surplus</a></li>
<li class="chapter" data-level="3.4.3" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#totalsurplus"><i class="fa fa-check"></i><b>3.4.3</b> Behavioural error and total economic surplus</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#behavioural-error-and-policy-recommendations"><i class="fa fa-check"></i><b>3.5</b> Behavioural error and policy recommendations</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#behavioural-errors-and-information-provision"><i class="fa fa-check"></i><b>3.5.1</b> Behavioural errors and information provision</a></li>
<li class="chapter" data-level="3.5.2" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#pricing-of-internalities"><i class="fa fa-check"></i><b>3.5.2</b> Pricing of internalities</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#behavioral-errors-and-pricing-of-consumption-externalities"><i class="fa fa-check"></i><b>3.6</b> Behavioral errors and pricing of consumption externalities</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#introduction-9"><i class="fa fa-check"></i><b>3.6.1</b> Introduction</a></li>
<li class="chapter" data-level="3.6.2" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#a-combined-externality-internality-tax"><i class="fa fa-check"></i><b>3.6.2</b> A combined externality-internality tax</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="erroreconsurplus.html"><a href="erroreconsurplus.html#discussion-and-conclusion"><i class="fa fa-check"></i><b>3.7</b> Discussion and conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="moral.html"><a href="moral.html"><i class="fa fa-check"></i><b>4</b> Moral Considerations and Economic Surplus </a>
<ul>
<li class="chapter" data-level="4.1" data-path="moral.html"><a href="moral.html#introduction-10"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="moral.html"><a href="moral.html#dealing-with-moral-considerations-at-the-valuation-stage"><i class="fa fa-check"></i><b>4.2</b> Dealing with moral considerations at the valuation stage</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="moral.html"><a href="moral.html#introduction-11"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="moral.html"><a href="moral.html#ethical-checkbox-approach"><i class="fa fa-check"></i><b>4.2.2</b> Ethical checkbox approach</a></li>
<li class="chapter" data-level="4.2.3" data-path="moral.html"><a href="moral.html#economizing-ethics-approach"><i class="fa fa-check"></i><b>4.2.3</b> Economizing ethics approach</a></li>
<li class="chapter" data-level="4.2.4" data-path="moral.html"><a href="moral.html#ethicizing-economics-approach"><i class="fa fa-check"></i><b>4.2.4</b> Ethicizing economics approach</a></li>
<li class="chapter" data-level="4.2.5" data-path="moral.html"><a href="moral.html#qualitative-valuation-approach"><i class="fa fa-check"></i><b>4.2.5</b> Qualitative valuation approach</a></li>
<li class="chapter" data-level="4.2.6" data-path="moral.html"><a href="moral.html#contributions-of-the-remainder-of-this-chapter"><i class="fa fa-check"></i><b>4.2.6</b> Contributions of the remainder of this chapter</a></li>
<li class="chapter" data-level="4.2.7" data-path="moral.html"><a href="moral.html#scope"><i class="fa fa-check"></i><b>4.2.7</b> Scope</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="moral.html"><a href="moral.html#redefining-the-economic-pie"><i class="fa fa-check"></i><b>4.3</b> Redefining the economic pie</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="moral.html"><a href="moral.html#the-advisory-committee"><i class="fa fa-check"></i><b>4.3.1</b> The advisory committee</a></li>
<li class="chapter" data-level="4.3.2" data-path="moral.html"><a href="moral.html#adjusted-consumer-surplus"><i class="fa fa-check"></i><b>4.3.2</b> Adjusted consumer surplus</a></li>
<li class="chapter" data-level="4.3.3" data-path="moral.html"><a href="moral.html#adjusted-producer-surplus"><i class="fa fa-check"></i><b>4.3.3</b> Adjusted producer surplus</a></li>
<li class="chapter" data-level="4.3.4" data-path="moral.html"><a href="moral.html#adjusted-economic-surplus"><i class="fa fa-check"></i><b>4.3.4</b> Adjusted economic surplus</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="moral.html"><a href="moral.html#taxation-and-moral-consideration"><i class="fa fa-check"></i><b>4.4</b> Taxation and moral consideration</a></li>
<li class="chapter" data-level="4.5" data-path="moral.html"><a href="moral.html#externality-taxation-and-moral-considerations"><i class="fa fa-check"></i><b>4.5</b> Externality taxation and moral considerations</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="moral.html"><a href="moral.html#moral-considerations-related-to-the-kind-of-externality"><i class="fa fa-check"></i><b>4.5.1</b> Moral considerations related to the kind of externality</a></li>
<li class="chapter" data-level="4.5.2" data-path="moral.html"><a href="moral.html#application-pricing-consumer-externalities-that-are-external-to-the-market"><i class="fa fa-check"></i><b>4.5.2</b> Application: pricing consumer externalities that are external to the market</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="moral.html"><a href="moral.html#conclusion-and-discussion"><i class="fa fa-check"></i><b>4.6</b> Conclusion and discussion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="univariateregression.html"><a href="univariateregression.html"><i class="fa fa-check"></i><b>5</b> Regression Analysis in the Social Sciences</a>
<ul>
<li class="chapter" data-level="5.1" data-path="univariateregression.html"><a href="univariateregression.html#introduction-12"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="univariateregression.html"><a href="univariateregression.html#secproblem"><i class="fa fa-check"></i><b>5.2</b> So, what is the problem?</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="univariateregression.html"><a href="univariateregression.html#a-first-encounter-with-stata"><i class="fa fa-check"></i><b>5.2.1</b> A first encounter with <code>STATA</code></a></li>
<li class="chapter" data-level="5.2.2" data-path="univariateregression.html"><a href="univariateregression.html#sec:numevidence"><i class="fa fa-check"></i><b>5.2.2</b> Numerical evidence</a></li>
<li class="chapter" data-level="5.2.3" data-path="univariateregression.html"><a href="univariateregression.html#sec:smart"><i class="fa fa-check"></i><b>5.2.3</b> Always be smart (and a bit lazy)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="univariateregression.html"><a href="univariateregression.html#sec:uniregress"><i class="fa fa-check"></i><b>5.3</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="univariateregression.html"><a href="univariateregression.html#sec:genesis"><i class="fa fa-check"></i><b>5.3.1</b> Genesis: <em>regression towards the mean</em></a></li>
<li class="chapter" data-level="5.3.2" data-path="univariateregression.html"><a href="univariateregression.html#regression-with-one-regressor"><i class="fa fa-check"></i><b>5.3.2</b> Regression with one regressor</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="univariateregression.html"><a href="univariateregression.html#least-squares-assumptions-for-causal-inference"><i class="fa fa-check"></i><b>5.4</b> Least squares assumptions for causal inference</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="univariateregression.html"><a href="univariateregression.html#least-squares-assumption-1-conditional-mean-independence"><i class="fa fa-check"></i><b>5.4.1</b> Least squares assumption 1: conditional mean independence</a></li>
<li class="chapter" data-level="5.4.2" data-path="univariateregression.html"><a href="univariateregression.html#least-squares-assumption-2-independenty-and-identically-distributed"><i class="fa fa-check"></i><b>5.4.2</b> Least squares assumption 2: independenty and identically distributed</a></li>
<li class="chapter" data-level="5.4.3" data-path="univariateregression.html"><a href="univariateregression.html#least-squares-assumption-3-large-outliers-are-rare"><i class="fa fa-check"></i><b>5.4.3</b> Least squares assumption 3: Large outliers are rare</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="univariateregression.html"><a href="univariateregression.html#other-least-squares-assumptions"><i class="fa fa-check"></i><b>5.5</b> Other least squares assumptions</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="univariateregression.html"><a href="univariateregression.html#homoskedasticity"><i class="fa fa-check"></i><b>5.5.1</b> Homoskedasticity</a></li>
<li class="chapter" data-level="5.5.2" data-path="univariateregression.html"><a href="univariateregression.html#normal-distributed-regression-term"><i class="fa fa-check"></i><b>5.5.2</b> Normal distributed regression term</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="univariateregression.html"><a href="univariateregression.html#measures-of-fit"><i class="fa fa-check"></i><b>5.6</b> Measures of fit</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="univariateregression.html"><a href="univariateregression.html#the-regression-r2"><i class="fa fa-check"></i><b>5.6.1</b> The regression <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="5.6.2" data-path="univariateregression.html"><a href="univariateregression.html#the-standard-error-of-the-regression"><i class="fa fa-check"></i><b>5.6.2</b> The Standard Error of the Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="univariateregression.html"><a href="univariateregression.html#conclusion-and-discussion-1"><i class="fa fa-check"></i><b>5.7</b> Conclusion and discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>6</b> Modeling in the Social Sciences</a>
<ul>
<li class="chapter" data-level="6.1" data-path="modeling.html"><a href="modeling.html#sec:morevar"><i class="fa fa-check"></i><b>6.1</b> Why more independent variables?</a></li>
<li class="chapter" data-level="6.2" data-path="modeling.html"><a href="modeling.html#sec:multivariate"><i class="fa fa-check"></i><b>6.2</b> Multivariate regression analysis</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="modeling.html"><a href="modeling.html#measures-of-fit-for-multiple-regression"><i class="fa fa-check"></i><b>6.2.1</b> Measures of fit for multiple regression</a></li>
<li class="chapter" data-level="6.2.2" data-path="modeling.html"><a href="modeling.html#the-least-squares-assumptions-for-multivariate-regression"><i class="fa fa-check"></i><b>6.2.2</b> The least squares assumptions for multivariate regression</a></li>
<li class="chapter" data-level="6.2.3" data-path="modeling.html"><a href="modeling.html#testing-with-multivariate-regression-models"><i class="fa fa-check"></i><b>6.2.3</b> Testing with multivariate regression models</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="modeling.html"><a href="modeling.html#sec:nonlinear"><i class="fa fa-check"></i><b>6.3</b> Non-linear specifications</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="modeling.html"><a href="modeling.html#polynomials"><i class="fa fa-check"></i><b>6.3.1</b> Polynomials</a></li>
<li class="chapter" data-level="6.3.2" data-path="modeling.html"><a href="modeling.html#interaction-variables"><i class="fa fa-check"></i><b>6.3.2</b> Interaction variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="modeling.html"><a href="modeling.html#logarithmic-transformations"><i class="fa fa-check"></i><b>6.3.3</b> Logarithmic transformations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="modeling.html"><a href="modeling.html#sec:fixedeffects"><i class="fa fa-check"></i><b>6.4</b> Using fixed effects in panel data</a></li>
<li class="chapter" data-level="6.5" data-path="modeling.html"><a href="modeling.html#conclusion-and-discussion-2"><i class="fa fa-check"></i><b>6.5</b> Conclusion and discussion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="specification.html"><a href="specification.html"><i class="fa fa-check"></i><b>7</b> Specification and Assessment Issues</a>
<ul>
<li class="chapter" data-level="7.1" data-path="specification.html"><a href="specification.html#sec:specificationmodel"><i class="fa fa-check"></i><b>7.1</b> Specification of your model</a></li>
<li class="chapter" data-level="7.2" data-path="specification.html"><a href="specification.html#sec:presentation"><i class="fa fa-check"></i><b>7.2</b> Presentation of results</a></li>
<li class="chapter" data-level="7.3" data-path="specification.html"><a href="specification.html#sec:sourcesbias"><i class="fa fa-check"></i><b>7.3</b> Potential sources of bias</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="specification.html"><a href="specification.html#threats-to-external-validity"><i class="fa fa-check"></i><b>7.3.1</b> Threats to external validity</a></li>
<li class="chapter" data-level="7.3.2" data-path="specification.html"><a href="specification.html#threats-to-internal-validity"><i class="fa fa-check"></i><b>7.3.2</b> Threats to internal validity</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="specification.html"><a href="specification.html#sec:conclusionspec"><i class="fa fa-check"></i><b>7.4</b> Concluding remarks</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="in-conclusion.html"><a href="in-conclusion.html"><i class="fa fa-check"></i><b>8</b> In conclusion</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appderivation.html"><a href="appderivation.html"><i class="fa fa-check"></i><b>A</b> Derivation of the optimal cost function</a></li>
<li class="chapter" data-level="B" data-path="apperror.html"><a href="apperror.html"><i class="fa fa-check"></i><b>B</b> Taxation in the presence of behavioural error</a></li>
<li class="chapter" data-level="C" data-path="background-calculations-for-chapter-refmoral.html"><a href="background-calculations-for-chapter-refmoral.html"><i class="fa fa-check"></i><b>C</b> Background calculations for Chapter @ref(moral)</a>
<ul>
<li class="chapter" data-level="C.1" data-path="background-calculations-for-chapter-refmoral.html"><a href="background-calculations-for-chapter-refmoral.html#apptax"><i class="fa fa-check"></i><b>C.1</b> Deriving the tax that optimizes adjusted social surplus</a></li>
<li class="chapter" data-level="C.2" data-path="background-calculations-for-chapter-refmoral.html"><a href="background-calculations-for-chapter-refmoral.html#apppricing"><i class="fa fa-check"></i><b>C.2</b> Pricing of a consumption externality</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="appreviewstat.html"><a href="appreviewstat.html"><i class="fa fa-check"></i><b>D</b> Reviewing probability and statistics</a>
<ul>
<li class="chapter" data-level="D.1" data-path="appreviewstat.html"><a href="appreviewstat.html#reviewing-probability"><i class="fa fa-check"></i><b>D.1</b> Reviewing probability</a>
<ul>
<li class="chapter" data-level="D.1.1" data-path="appreviewstat.html"><a href="appreviewstat.html#probability"><i class="fa fa-check"></i><b>D.1.1</b> Probability</a></li>
<li class="chapter" data-level="D.1.2" data-path="appreviewstat.html"><a href="appreviewstat.html#population-random-variables"><i class="fa fa-check"></i><b>D.1.2</b> Population &amp; random variables</a></li>
<li class="chapter" data-level="D.1.3" data-path="appreviewstat.html"><a href="appreviewstat.html#distribution-functions"><i class="fa fa-check"></i><b>D.1.3</b> Distribution functions</a></li>
<li class="chapter" data-level="D.1.4" data-path="appreviewstat.html"><a href="appreviewstat.html#conditional-distributions-and-conditional-means"><i class="fa fa-check"></i><b>D.1.4</b> Conditional distributions and conditional means</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="appreviewstat.html"><a href="appreviewstat.html#secssampling"><i class="fa fa-check"></i><b>D.2</b> Sampling in frequentist statistics</a>
<ul>
<li class="chapter" data-level="D.2.1" data-path="appreviewstat.html"><a href="appreviewstat.html#the-sampling-distribution-of-bary"><i class="fa fa-check"></i><b>D.2.1</b> The sampling distribution of <span class="math inline">\(\bar{Y}\)</span></a></li>
<li class="chapter" data-level="D.2.2" data-path="appreviewstat.html"><a href="appreviewstat.html#example-simple-binomial-random-variables"><i class="fa fa-check"></i><b>D.2.2</b> Example: simple binomial random variables</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Methods and Techniques for Social and Economic Research: Syllabus</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modeling" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Modeling in the Social Sciences<a href="modeling.html#modeling" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In Chapter <a href="univariateregression.html#univariateregression">5</a> we discussed the origins of, working of, and assumptions behind univariate regression. That is, a regression model with only one independent variable <span class="math inline">\(X\)</span> on the right hand side.<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a> However, and especially in the social sciences, you almost always see regressions with many independent variables. Depending on the field, these variables can be called control variables, confounding factors or moderator variables. But why are these variables included? Is it only to improve model performance or are there other reasons? Section <a href="modeling.html#sec:morevar">6.1</a> deals with this question whereafter Section <a href="modeling.html#sec:multivariate">6.2</a> shows how you can include additional variables in a <em>multivariate regression model</em> and especially how you should interpret them. Section <a href="modeling.html#sec:nonlinear">6.3</a> extends the multivariate regression model and shows how you can actually use this model to estimate a broad range of linear and non-linear economic models. Section <a href="modeling.html#sec:fixedeffects">6.4</a> discusses the use of multiple dummy variables (see again Subsection <a href="univariateregression.html#sec:dummy">5.3.2.4</a>) in a way that economists refer to as <em>fixed effects</em>. The last section concludes and provides a further discussion of the benefits and limitations of multivariate regression models.</p>
<div id="sec:morevar" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Why more independent variables?<a href="modeling.html#sec:morevar" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So, why do we include more variables? One possible answer is because it makes a better predictive model. That is, a model that is able to explain the variation in the dependent variable <span class="math inline">\(Y\)</span> better.<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a> So, the R<span class="math inline">\(^2\)</span> increases. But, as argued in Chapter <a href="univariateregression.html#univariateregression">5</a> we are not so much interested in prediction, but more in establishing a <strong>causal</strong> relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. So, if you change <span class="math inline">\(X\)</span> (and only <span class="math inline">\(X\)</span>) does <span class="math inline">\(Y\)</span> changes and then with how much?</p>
<p>Although economists often claim that they are the only (social-)science that focuses on causality and provides a statistical framework for that, there are other approaches to causality as well. One that is often used in other sciences is the approach of the mathematican Judea Pearl <span class="citation">(Pearl 2009)</span>. This approach focuses on the use of Directed Acyclical Graphs (DAGs), which is a graphical visualisation of causality chains (or, what impacts what). We borrow this approach for the most simple setting as explained in Figure <a href="modeling.html#fig:unknown">6.1</a>. Here, we go back to our Californian school district dataset again, where we still are interested in the effect of class size on school performance. So, we suppose that there is an effect from student teacher ratio on test scores as displayed with an directed arrow in Figure <a href="modeling.html#fig:unknown">6.1</a>. We also know that the R<span class="math inline">\(^2\)</span> of that regression model was rather low (5%), so by default there must be other but yet unknown factors, let us name them for now <span class="math inline">\(U\)</span> (often as well referred to as unobservables), that influence test scores as well (so a directed arrow going from <span class="math inline">\(U\)</span> to test scores).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unknown"></span>
<img src="figures/unknown.png" alt="Unrelated omitted variables" width="600px" />
<p class="caption">
Figure 6.1: Unrelated omitted variables
</p>
</div>
<p>Now we are fine with this is as long as <span class="math inline">\(U\)</span> does <strong>not impact</strong> the student teacher ratio. Then, there is still an isolated effect of student teacher ratio on class size and that is exactly what we want to measure. However, if there is a directed arrow going from <span class="math inline">\(U\)</span> into <span class="math inline">\(STR\)</span> as depicted by Figure <a href="modeling.html#fig:unobshet">6.2</a>, then the effect of student teacher ratio is not isolated anymore. Essentially, the effect of student teacher ratio on class size is composed out of two parts:</p>
<ol style="list-style-type: decimal">
<li>The <strong>causal</strong> effect on student teacher ratio on class size captured by the chain <span class="math inline">\(\text{STR} \longrightarrow \text{testscore}\)</span>. The one we are after.</li>
<li>The impact of the unknown variables on test scores. As we have not modeled them in our regression model, the effect is captured by the chain <span class="math inline">\(U \longrightarrow \text{STR} \longrightarrow \text{testscore}\)</span></li>
</ol>
<p>Economists refer to this phenomenon as <strong>omitted variable bias</strong>, whilst in the statistical world, this is as often called confounding variables or the <strong>confounding fork</strong> <span class="citation">(McElreath 2020)</span> and it, unfortunately, occurs very often.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unobshet"></span>
<img src="figures/Unobshet.png" alt="Related omitted variables" width="600px" />
<p class="caption">
Figure 6.2: Related omitted variables
</p>
</div>
<p>So, when <strong>U</strong> is a <em>common</em> cause for both student teacher ratio and test scores there is omitted variable bias. If we go back to our population regression model as follows:
<span class="math display">\[\begin{equation}
Y_i = \beta_0 + \beta_1 X_i + u_i,
\end{equation}\]</span>
then we know that the error <span class="math inline">\(u\)</span> arises because of factors that influence <span class="math inline">\(Y\)</span> but are not included in the regression function; so, there are <em>always</em> omitted variables. But they do not always lead to bias. For omitted variable bias to occur, the omitted factor, let’s call it <span class="math inline">\(Z\)</span><a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a>, must be:</p>
<ol style="list-style-type: decimal">
<li>A <strong>determinant</strong> of <span class="math inline">\(Y\)</span> (i.e. <span class="math inline">\(Z\)</span> is part of <span class="math inline">\(u\)</span>)</li>
<li>A <strong>determinant</strong> of the regressor <span class="math inline">\(X\)</span> (<em>at least</em>, there should hold that <span class="math inline">\(corr(Z,X) \neq 0\)</span>)<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a></li>
</ol>
<p>Thus, both conditions must hold for the omission of <span class="math inline">\(Z\)</span> to result in omitted variable bias.</p>
<p>Now, in our Californian district school dataset we have many more variables. One of them is variable that measures the english language ability (whether the student has English as a second language). Note that in California there are many migrants, especially from Latin-America. Now, you can readily argue that not having English as first language plausibly affects standardized test scores: so, <span class="math inline">\(Z\)</span> is a <strong>determinant</strong> of <span class="math inline">\(Y\)</span>. Moreover, immigrant communities tend to be less affluent and thus have smaller school budgets—and, therefore, higher <span class="math inline">\(STR\)</span>: <span class="math inline">\(Z\)</span> is most likely as well a <strong>determinant</strong> of <span class="math inline">\(X\)</span>.</p>
<p>So, most likely, our original estimation from Chapter <a href="univariateregression.html#univariateregression">5</a>, <span class="math inline">\(\hat{\beta}_1\)</span>, is biased (so not the true causal effect). But can we say something about the direction of that bias? Yes, but the argument tends to become very quickly rather complex. In this case, note that districts with more migrant communities tend to have (<em>i</em>) higher class sizes and (<em>ii</em>) lower test scores. So, to the original estimation they added a <em>negative</em> effect. Thus, following this reasoning, the “true” effect must be less negative. Now, especially with negative signs this becomes rather complex, so if common sense fails you, then there is the following formula:</p>
<p><span class="math display">\[\begin{equation}
\hat{\beta}_1 \overset{p}{\to} \beta_1 + \frac{\sigma_u}{\sigma_X}\rho_{Xu},
\end{equation}\]</span>
where you should focus on the sign of the correlation between <span class="math inline">\(X\)</span> and the regression residual <span class="math inline">\(u\)</span> (all standard errors, <span class="math inline">\(\sigma\)</span>, are always positive by default). Now, the first least squares assumption states that <span class="math inline">\(\rho_{Xu} = 0\)</span>—no correlation between the regressor and the regression residual. But now there is correlation because of omitted variable bias. And because there is a negative relation between immigrants communities and school performance, <span class="math inline">\(\rho_{Xu}\)</span> should be negative. Furthermore, because the original estimation from Chapter <a href="univariateregression.html#univariateregression">5</a> was already negative to begin with the “true” <span class="math inline">\(\beta_1\)</span> should be less negative. In conclusion, districts with more English learning students (<em>i</em>) do worse on standardized tests and (<em>ii</em>) have bigger classes (smaller budgets), so ignoring the English learning factor results in overstating the class size effect (in an absolute sense).</p>
<p>You might wonder whether this is actually going on in the Californian district school data. To see this, Figure <a href="modeling.html#fig:omitca">6.3</a> offers a cross tabulation of test scores by class size and percentage English learners.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:omitca"></span>
<img src="figures/Sheet7.png" alt="Cross tabulation of test scores by class size and percentage English learners" width="800px" />
<p class="caption">
Figure 6.3: Cross tabulation of test scores by class size and percentage English learners
</p>
</div>
<p>Now, the table depicted in Figure <a href="modeling.html#fig:omitca">6.3</a> is complex in its various dimensions. We have our two categories of class size (small and large), together with the difference in test scores, but we now stratify this by four categories of percentage English learners. There are several important observations to make here:</p>
<ol style="list-style-type: decimal">
<li>districts with <em>fewer</em> English Learners (so less migrants) have on average <em>higher</em> test scores (what we assumed above);</li>
<li>districts with <em>fewer</em> English Learners (so less migrants) have <em>smaller</em> classes (what we assumed above);</li>
<li>the effect of class size with comparable percentages English learners is still (mostly negative), but not as much as we compare for all districts together (the <em>Difference</em>-column). This confirms our reasoning that our original estimate was too negative.</li>
</ol>
<p>No, as already mentioned above, omitted variable bias occurs very often. So, how to correct for this such that the bias disappaers. In general, there are three strategies:</p>
<ol style="list-style-type: decimal">
<li>we can run a randomized controlled experiment in which treatment (<span class="math inline">\(STR\)</span>) is randomly assigned: then percentage English learners (<span class="math inline">\(PctEL\)</span>) is still a determinant of test scores, but by construction <span class="math inline">\(PctEL\)</span> should be uncorrelated with <span class="math inline">\(STR\)</span>. Unfortunately, is it very difficult to randomize class size in reality and often this strategy is just not attainable as being too costly or unethical (this accounts for all sciences);</li>
<li>we can adopt the cross tabulation approach of above, with finer gradations of <span class="math inline">\(STR\)</span> and <span class="math inline">\(PctEL\)</span>. Then by construction, within each group all classes have the same <span class="math inline">\(PctEL\)</span> so we control for <span class="math inline">\(PctEL\)</span>. A disadvantages is that one needs many observations, especially when one wants to stratify upon other variables as well;</li>
<li>finally, and perhaps the easiest approach, we can use a population regression model in which the omitted variable (<span class="math inline">\(PctEL\)</span>) is no longer omitted. We just include <span class="math inline">\(PctEL\)</span> as an additional regressor in a multiple regression model. This is what the next section deals with. Obviously, a disadvantage of this approach is that you need observations for the omitted variable (but that also accounts for method 2).</li>
</ol>
</div>
<div id="sec:multivariate" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Multivariate regression analysis<a href="modeling.html#sec:multivariate" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So, if we have information about an important omitted variable, as in the case of the size of migrant communities in the example above, then we can use that information in a multivariate population regression model. In the case of two regressors, that would look like:
<span class="math display">\[\begin{equation}
Y_i =\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i, i=1,\ldots,n
\end{equation}\]</span>
where:</p>
<ul>
<li><span class="math inline">\(Y\)</span> is the dependent variable</li>
<li><span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> are the two independent variables (regressors)</li>
<li><span class="math inline">\((Y_i, X_{1i}, X_{2i})\)</span> denote the i<span class="math inline">\(^{\mathrm{th}}\)</span> observation on <span class="math inline">\(Y\)</span>, <span class="math inline">\(X_1\)</span>, and <span class="math inline">\(X_2\)</span>.</li>
<li><span class="math inline">\(\beta_0\)</span> is the unknown population intercept</li>
<li><span class="math inline">\(\beta_1\)</span> is the effect on <span class="math inline">\(Y\)</span> of a change in <span class="math inline">\(X_1\)</span>, <strong>holding</strong> <span class="math inline">\(X_2\)</span> constant</li>
<li><span class="math inline">\(\beta_2\)</span> is the effect on <span class="math inline">\(Y\)</span> of a change in <span class="math inline">\(X_2\)</span>, <strong>holding</strong> <span class="math inline">\(X_1\)</span> constant</li>
<li><span class="math inline">\(u_i\)</span> is the the regression error (omitted factors)</li>
</ul>
<p>Now, the only element that changes is the interpretation of a parameter, say <span class="math inline">\(\beta_1\)</span>. In this case, it can still be seen as a ‘slope’ parameter, although now in 3-dimensional space, but it now states specifically that the other parameter(s) should be held constant. This does facilitate the interpretation of <span class="math inline">\(\beta_1\)</span>. For example, consider changing <span class="math inline">\(X_1\)</span> by <span class="math inline">\(\Delta X_1\)</span> while holding <span class="math inline">\(X_2\)</span> constant. That means that the population regression line before the change looks like:
<span class="math display">\[\begin{equation}
Y = \beta_0 + \beta_1 X_{1} + \beta_2 X_{2},
\end{equation}\]</span>
whilst the population regression line, after the change, looks like:
<span class="math display">\[\begin{equation}
Y + \Delta Y = \beta_0 + \beta_1 (X_{1} + \Delta X_1) + \beta_2 X_{2}
\end{equation}\]</span>
And if we take the difference, then the interpretation of <span class="math inline">\(\beta_1\)</span> boils down again to the marginal effect:<span class="math inline">\(\Delta Y = \beta_1 \Delta X_1\)</span>. Or, <span class="math inline">\(\beta_1 = \frac{\Delta Y}{\Delta X_1}\)</span> when holding <span class="math inline">\(X_2\)</span> constant and, likewise, <span class="math inline">\(\beta_2 = \frac{\Delta Y}{\Delta X_2}\)</span> when holding <span class="math inline">\(X_1\)</span> constant. <span class="math inline">\(\beta_0\)</span> is now the predicted value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X_1 = X_2 = 0\)</span></p>
<p>If we do this for the the Californian school district data, then the original population regression line was estimated as:
<span class="math display">\[\begin{equation}
\widehat{TestScore} = 698.9- 2.28 STR
\end{equation}\]</span>
But if we now include include percent English Learners in the district (<span class="math inline">\(PctEL\)</span>) to the model then the population regression ‘line’ becomes:
<span class="math display">\[\begin{equation}
\widehat{TestScore} = 686.0- 1.10 STR - 0.65  PctEL
\end{equation}\]</span></p>
<p>Clearly, the effect of student teacher ratio becomes smaller (that is, less negative). That indicates that the original regression suffers from omitted variable bias. And this is what should happen as reasoned above. The <code>STATA</code> syntax for a multivariate regression model is now rather straightforward. You basically add another to the regression equation, as below:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb22-1"><a href="modeling.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr str el_pct, <span class="kw">robust</span></span></code></pre></div>
<pre><code>Linear regression                               Number of obs     =        420
                                                F(2, 417)         =     223.82
                                                Prob &gt; F          =     0.0000
                                                R-squared         =     0.4264
                                                Root MSE          =     14.464

------------------------------------------------------------------------------
             |               Robust
     testscr | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -1.101296   .4328472    -2.54   0.011     -1.95213   -.2504616
      el_pct |  -.6497768   .0310318   -20.94   0.000     -.710775   -.5887786
       _cons |   686.0322   8.728224    78.60   0.000     668.8754     703.189
------------------------------------------------------------------------------</code></pre>
<p>Obviously, the effect of student teacher ration reduces with 50%! The interpretation of the rest of the statistical output, such as measures of fit and test statistics, follows in the subsections below.</p>
<div id="measures-of-fit-for-multiple-regression" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Measures of fit for multiple regression<a href="modeling.html#measures-of-fit-for-multiple-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In multivariate regression models, there are four commonly used measures of fit, three of them we have seen before.</p>
<ol style="list-style-type: decimal">
<li>The standard error of regression or the <span class="math inline">\(SER\)</span> denotes the standard deviation of <span class="math inline">\(\hat{u}_i\)</span> and includes a degrees of freedom correction (degrees of freedom in this case denotes how many variables your have used and typically is denoted with <span class="math inline">\(k\)</span>. The <span class="math inline">\(SER\)</span> is defined as:
<span class="math display">\[\begin{equation}
SER = s_{\hat{u}} = \sqrt{\frac{1}{n-k-1} \sum_{i=1}^n \hat{u}^2_i},
\end{equation}\]</span>
where <span class="math inline">\(k\)</span> is the number of variables (including the constant) use in the regression model. Note that in the univariate regression model <span class="math inline">\(k=2\)</span>—the slope coefficient and the constant.</li>
<li>The root mean square error (RMSE) which denotes as well the stdandard deviation of <span class="math inline">\(\hat{u}_i\)</span> but now without degrees of freedom. We have seen this before in Eq. <a href="univariateregression.html#eq:rmse">(5.14)</a> and does not change.</li>
<li>The <span class="math inline">\(R^2\)</span> which measures the fraction of variance of <span class="math inline">\(Y\)</span> explained by the independent variables. Again, we have seen this one before</li>
<li>The adjusted “adjusted <span class="math inline">\(R^2\)</span>” (or <span class="math inline">\(\bar{R}^2\)</span>) which is equal to the <span class="math inline">\(R^2\)</span> with a degrees-of-freedom correction that adjusts for estimation uncertainty. It can be formulated as:
<span class="math display">\[\begin{equation}
\bar{R}^2 = 1 - \frac{n-1}{n-k-1}\frac{SSR}{TSS}.
\end{equation}\]</span>
Note that using this formulation, in a multivariate setting, it always should hold that <span class="math inline">\(\bar{R}^2 &lt;R^2\)</span>. But why do we care so much for the amount of variables that we use (denoted with <span class="math inline">\(k\)</span>). That is because with each additional variable the <span class="math inline">\(R^2\)</span> always increases. And it is essential to notice that when <span class="math inline">\(k=n\)</span>, the <span class="math inline">\(R^2 = 1\)</span>, so there is no variation left anymore. But that feels like cheating. You just have a parameter for each observation that you have, but such a model must be meaningless. Therefore, you always want to correct for the number of variables that you use.</li>
</ol>
<p>In our Californian school district example that would amount to the following two outcomes. First for the univariate model:
<span class="math display">\[\begin{eqnarray}
TestScore &amp;= &amp;698.9- 2.28  STR \\
&amp;&amp;R^2 = .05, SER = 18.6
\end{eqnarray}\]</span></p>
<p>And then for the multivariate model.</p>
<p><span class="math display">\[\begin{eqnarray}
TestScore &amp;=&amp; 686.0 - 1.10  STR - 0.65 PctEL \\
&amp;&amp;R^2=.426, \bar{R}^2=0.424, SER = 14.5
\end{eqnarray}\]</span></p>
<p>Note that all measures of fit increases. The <span class="math inline">\(\bar{R}^2\)</span> now indicates that 42% of all variation in test scores are explained. That is a <em>huge</em> improvement compared to the 5% explanatory power of the univariate case. That indicates that the <span class="math inline">\(PctEL\)</span> strongly correlates with testscores. But again, we are not so much interested in prediction, but want to find the causal impact of class size instead. Another thing to notice here is that the <span class="math inline">\(R^2\)</span> and the <span class="math inline">\(\bar{R}^2\)</span> are very close. That is because the number of variables is much smaller than the number of observations <span class="math inline">\(k \ll n\)</span>, so that the impact of <span class="math inline">\(k\)</span> is not very big.</p>
<p>A final remark concerns a peculiarity of <code>STATA</code>. In the regression output of above, <code>STATA</code> does not provide the <span class="math inline">\(\bar{R}^2\)</span>. That is because of the option <code>, robust</code>. Without that option, the regression output would give both measures of fit.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb24-1"><a href="modeling.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr str el_pct</span></code></pre></div>
<pre><code>      Source |       SS           df       MS      Number of obs   =       420
-------------+----------------------------------   F(2, 417)       =    155.01
       Model |  64864.3011         2  32432.1506   Prob &gt; F        =    0.0000
    Residual |  87245.2925       417  209.221325   R-squared       =    0.4264
-------------+----------------------------------   Adj R-squared   =    0.4237
       Total |  152109.594       419  363.030056   Root MSE        =    14.464

------------------------------------------------------------------------------
     testscr | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -1.101296   .3802783    -2.90   0.004    -1.848797   -.3537945
      el_pct |  -.6497768   .0393425   -16.52   0.000    -.7271112   -.5724423
       _cons |   686.0322   7.411312    92.57   0.000     671.4641    700.6004
------------------------------------------------------------------------------</code></pre>
<p>Another option is to specifically ask <code>STATA</code> to display the <span class="math inline">\(\bar{R}^2\)</span> by invoking the command <code>display</code>, then some text (text always goes between strings), and finally the thing you want to see (<code>e(r2_a)</code>). Something like:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb26-1"><a href="modeling.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">display</span> <span class="st">&quot;adjusted R2 = &quot;</span> <span class="fu">e</span>(r2_a)</span></code></pre></div>
<pre><code>adjusted R2 = .42368043</code></pre>
</div>
<div id="the-least-squares-assumptions-for-multivariate-regression" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> The least squares assumptions for multivariate regression<a href="modeling.html#the-least-squares-assumptions-for-multivariate-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Thus, it is easy to add other variables, so that the multivariate regression model now looks like:
<span class="math display">\[\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i}+\ldots + \beta_k X_{ki}+u_i, i=1,\ldots,n
\end{equation}\]</span>
Suppose we are interested in <span class="math inline">\(\beta_1\)</span>. How do we then know whether our estimation <span class="math inline">\(\hat{\beta}_1\)</span> is unbiased? For that we again resort to our least squares assumption, some of them will change a bit and we have to add a fourth one:</p>
<ol style="list-style-type: decimal">
<li>The first least squares assumptions changes slightly. Now, we state that the conditional distribution of <span class="math inline">\(u\)</span> given all <span class="math inline">\(X_i\)</span>’s has mean zero, that is, <span class="math inline">\(E(u|X_1 = x_1,\ldots, X_k = x_k) = 0\)</span>. So, <span class="math inline">\(\beta_1\)</span> is biased even another variable <span class="math inline">\(X_k\)</span> is correlated with <span class="math inline">\(u\)</span>. So, only of the variables <span class="math inline">\(X_i\)</span> has to be correlated with <span class="math inline">\(u\)</span> and then all parameters are to a certain extent biased.</li>
<li>The second least squares assumption is more or less as before but now in a multivariate fashion, so the whole set of (X<span class="math inline">\(_{1i}\)</span>,,X<span class="math inline">\(_{ki}\)</span>,Y<span class="math inline">\(_i\)</span>), with <span class="math inline">\(i =1,\ldots,n\)</span>, should be independent and identical distributed (<span class="math inline">\(i.i.d\)</span>).</li>
<li>The third least squares assumptions states again that large outliers are rare for all variables included, so for all <span class="math inline">\(X_1,\ldots, X_k\)</span>, and <span class="math inline">\(Y\)</span>.</li>
<li>The fourth assumption is new and states that there is no perfect multicollinearity. We discuss this further below.</li>
</ol>
<div id="multicollinearity" class="section level4 hasAnchor" number="6.2.2.1">
<h4><span class="header-section-number">6.2.2.1</span> Multicollinearity<a href="modeling.html#multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Multicollinearity comes in two flavours; perfect and imperfect. The former functions as a multivariate least squares assumptions whilst the latter oftentimes gives the largest problems. We start the discussion with perfect multicollinearity and then continue with the case of imperfect multicollinearity.</p>
<div id="perfect-multicollinearity" class="section level5 hasAnchor" number="6.2.2.1.1">
<h5><span class="header-section-number">6.2.2.1.1</span> Perfect multicollinearity<a href="modeling.html#perfect-multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The official definition of perfect multicollinearity is that there is a <strong>perfect linear combination</strong> amongst your variables. That means that there is not one optimal solution, but instead many (actually, infinitely many) more. Let us illustrate this by the following example. Suppose you include <span class="math inline">\(STR\)</span> twice in your regression. Now, <code>STATA</code> produces then the following output:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb28-1"><a href="modeling.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr str str el_pct, <span class="kw">robust</span></span></code></pre></div>
<pre><code>note: str omitted because of collinearity.

Linear regression                               Number of obs     =        420
                                                F(2, 417)         =     223.82
                                                Prob &gt; F          =     0.0000
                                                R-squared         =     0.4264
                                                Root MSE          =     14.464

------------------------------------------------------------------------------
             |               Robust
     testscr | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -1.101296   .4328472    -2.54   0.011     -1.95213   -.2504616
         str |          0  (omitted)
      el_pct |  -.6497768   .0310318   -20.94   0.000     -.710775   -.5887786
       _cons |   686.0322   8.728224    78.60   0.000     668.8754     703.189
------------------------------------------------------------------------------</code></pre>
<p>See that <code>STATA</code> drops one of the <span class="math inline">\(STR\)</span> variables. But why is that? See that the impact of twice this variable should be equivalent to:
<span class="math display">\[\begin{equation}
\beta_1 STR = w_1 \beta_1 STR + w_2 \beta_1 STR = (w_1 + w_2) \beta_1 STR ,
\end{equation}\]</span>
where <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> are weights chosen such that they satisfy the condition that <span class="math inline">\(w_1 + w_2 = 1\)</span>. But there is an infinite number of combinations that satisfy this condition! So, there is not an optimal solution and one of these variables should be dropped.</p>
<p>The violation of no perfect multicollearity often occurs when using dummies (see again Subsection <a href="univariateregression.html#sec:dummy">5.3.2.4</a>). Suppose that we regress <span class="math inline">\(TestScore\)</span> on a constant, <span class="math inline">\(D\)</span>, and <span class="math inline">\(B\)</span>, where:<span class="math inline">\(D_i =1\)</span> if <span class="math inline">\(STR \leq 20\)</span>, <span class="math inline">\(=0\)</span> otherwise ; <span class="math inline">\(B_i =1\)</span> if <span class="math inline">\(STR&gt;20\)</span>, <span class="math inline">\(= 0\)</span> otherwise. This example is slightly more complex as there is no perfect correlation between <span class="math inline">\(B\)</span> and <span class="math inline">\(D\)</span>. However, the model contains as well a constant and that create a perfect linear combination, namely <span class="math inline">\(B_i + D_i = 1\)</span> and that is the definition of a constant (<span class="math inline">\(\beta_1 \times 1\)</span>), so there is perfect multicollinearity in the model.</p>
<p>A different way of seeing this is to consider the following regression model and note that by definition <span class="math inline">\(D_i = 1- B_i\)</span>:</p>
<p><span class="math display">\[\begin{align}
Testscr_i &amp;= \beta_0 + \beta_1 D_i + \beta_2 B_i + u_i\\
          &amp;= \beta_0 + \beta_1 D_i + \beta_2 (1 - D_i) + u_i\\
          &amp;= (\beta_0 + \beta_2) + (\beta_1 - \beta_2) D_i + u_i.
\end{align}\]</span>
Suppose that the true constant equals <span class="math inline">\(680\)</span> and the slope parameter equals <span class="math inline">\(7\)</span>. Then it is not difficult to see that there is an <strong>infinite</strong> amount of combinations possible of values for <span class="math inline">\(\beta_0, \beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> that leads to these numbers.</p>
<p>Now, this example is a special case of the so-called dummy variable trap. Suppose you have a set of multiple binary (dummy) variables, which are mutually exclusive and exhaustive—that is, there are multiple categories and every observation falls in one and only one category (e.g., infant, child, teenager, adult). If you include all these dummy variables and a constant, you will have perfect multicollinearity—the dummy variable trap.</p>
<p>There are possible solutions to the dummy variable trap:</p>
<ol style="list-style-type: decimal">
<li>Omit one of the groups (e.g., the infants), or</li>
<li>Omit the intercept</li>
</ol>
<p>In most cases you omit one of the groups (typically the one with the lowest value). This give the constant then the interpretation of the average value of that left-out category, where the dummy variables are then the relative differences to that left-out category.</p>
<p>Now, perfect multicollinearity usually reflects a mistake in the definitions of the regressors, or an oddity in the data. And, usually this is not a problem, because if you have perfect multicollinearity, your statistical software will let you know—either by crashing or giving an error message or by “dropping” one of the variables arbitrarily and very often the solution to perfect multicollinearity is to modify your list of regressors such that you no longer have perfect multicollinearity.</p>
</div>
<div id="imperfect-multicollinearity" class="section level5 hasAnchor" number="6.2.2.1.2">
<h5><span class="header-section-number">6.2.2.1.2</span> Imperfect multicollinearity<a href="modeling.html#imperfect-multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Now imperfect and perfect multicollinearity are quite different despite the similarity of the names. Imperfect multicollinearity, namely, occurs when two or more regressors are very highly correlated. And if two regressors are very highly correlated, then their scatterplot will pretty much look like a straight line—they are collinear—but unless the correlation is exactly <span class="math inline">\(\pm\)</span> 1, that collinearity is imperfect. What this implies is that one or more of the regression coefficients will be imprecisely estimated. Why is that? That is because of the definition of the coefficient in a multivariate regression model. Namely, the coefficient on <span class="math inline">\(X_1\)</span> is the effect of <span class="math inline">\(X_1\)</span> <strong>holding <span class="math inline">\(X_2\)</span> constant</strong>, but if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are highly correlated, then there is very little variation in <span class="math inline">\(X_1\)</span> once <span class="math inline">\(X_2\)</span> is held constant. That means that the data are pretty much uninformative about what happens when <span class="math inline">\(X_1\)</span> changes but <span class="math inline">\(X_2\)</span> doesn’t, so the variance of the OLS estimator of the coefficient on <span class="math inline">\(X_1\)</span> will be large. And this results in large standard errors for one or more of the OLS coefficients. But often this is very hard to detect. Are standard errors high because of imperfect multicollinearity, because the number of observations is very low, or because there is large variation in the data? The answer to this unfortunately boils down to reasoning, but before you start estimating your statistical models it always good to look at scatterplots and correlations between variables.</p>
<p>But what is a high correlation? With a reasonable amount of observations all correlations below <span class="math inline">\(0.9\)</span> can be considered fine. In practice, only correlations between variables higher than say <span class="math inline">\(0.95\)</span> start to impose problems.</p>
</div>
</div>
</div>
<div id="testing-with-multivariate-regression-models" class="section level3 hasAnchor" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> Testing with multivariate regression models<a href="modeling.html#testing-with-multivariate-regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="hypothesis-tests-and-confidence-intervals-for-a-single-coefficient-in-multiple-regression" class="section level4 hasAnchor" number="6.2.3.1">
<h4><span class="header-section-number">6.2.3.1</span> Hypothesis tests and confidence intervals for a single coefficient in multiple regression<a href="modeling.html#hypothesis-tests-and-confidence-intervals-for-a-single-coefficient-in-multiple-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recall from Subsection <a href="univariateregression.html#sec:unitesting">5.3.2.2</a> that for hypothesis testing in a classical statistical framework we make use of the fact that <span class="math inline">\(\frac{\hat{\beta}_1- E(\hat{\beta}_1)}{\sqrt{var(\hat{\beta}_1)}}\)</span> is approximately distributed as <span class="math inline">\(N(0,1)\)</span> according to the Central Limit theorem. Thus hypotheses on <span class="math inline">\(\beta_1\)</span> can be tested using the usual <span class="math inline">\(t\)</span>-statistic, and confidence intervals are constructed as <span class="math inline">\(\{\hat{\beta}_1 \pm 1.96 SE (\hat{\beta}_1)\}\)</span>. And this finding carries over to the multivariate setting where for <span class="math inline">\(\beta_2,\ldots, \beta_k\)</span> we make use of the same framework. One thing to keep in mind is that <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_2\)</span> are generally not independently distributed—so neither are their <span class="math inline">\(t\)</span>-statistics (more on this later).</p>
<p>Now, if we return to our Californian school district data set then we find that for the univariate case holds:</p>
<p><span class="math display">\[\begin{equation}
TestScore =\underbrace{698.9}_{10.4} - \underbrace{2.28}_{0.52}  STR,
\end{equation}\]</span></p>
<p>And the population regression “line” for the multivariate case is estimated as:
<span class="math display" id="eq:testmulti">\[\begin{equation}
TestScore = \underbrace{686.0}_{8.7} - \underbrace{1.10}_{0.43} STR - \underbrace{0.650}_{0.031} PctEL
    \tag{6.1}
\end{equation}\]</span></p>
<p>Remember, the coefficient on <span class="math inline">\(STR\)</span> in Eq. <a href="modeling.html#eq:testmulti">(6.1)</a> is the effect on <span class="math inline">\(TestScores\)</span> of a unit change in <span class="math inline">\(STR\)</span>, holding constant the percentage of English Learners in the district. The corresponding 95% confidence interval for coefficient on <span class="math inline">\(STR\)</span> in (2) is <span class="math inline">\(\{-1.10 \pm 1.96 \times 0.43\} = (-1.95,-0.26)\)</span>. And the <span class="math inline">\(t\)</span>-statistic testing <span class="math inline">\(\beta_{STR} = 0\)</span> is <span class="math inline">\(t = -1.10/0.43 = -2.54\)</span>, so we reject the null-hypothesis at the 5% significance level. More evidence for the strength of the <span class="math inline">\(PctEL\)</span> variable can be seen from the fact that, under the null-hypothesis of <span class="math inline">\(\beta_2 = 0\)</span>, the following must hold: <span class="math inline">\(t\text{-statistic} = \frac{\hat{\beta_1}}{\sigma_{\hat{\beta_1}}} = \frac{0.65}{0.03} = 21.7\)</span>, which is a very high number for a <span class="math inline">\(t\)</span>-statistic.</p>
</div>
<div id="tests-of-joint-hypotheses" class="section level4 hasAnchor" number="6.2.3.2">
<h4><span class="header-section-number">6.2.3.2</span> Tests of joint hypotheses<a href="modeling.html#tests-of-joint-hypotheses" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>So, testing of single coefficients is just as before. Now in the Californian school district dataset there is as well a variable called <span class="math inline">\(Expn\)</span> denoting the expenditures per pupil. Consider the following population
regression model:
<span class="math display">\[\begin{equation}
TestScore_i = \beta0 + \beta_1 STR_i + \beta_2 Expn_i + \beta_3PctEL_i + u_i
\end{equation}\]</span>
The null hypothesis that “school resources don’t matter” and the alternative that they do, corresponds to:</p>
<ul>
<li><span class="math inline">\(H_0:\beta_1 =0\)</span> and <span class="math inline">\(\beta_2 =0\)</span> vs</li>
<li><span class="math inline">\(H_1:\)</span> either <span class="math inline">\(\beta_1 \neq 0\)</span> or <span class="math inline">\(\beta_2 \neq 0\)</span> or both</li>
</ul>
<p>This is a joint hypothesis specifying a value for two or more coefficients. That is, it imposes a restriction on two or more coefficients. In general, a joint hypothesis will involve <span class="math inline">\(q\)</span> restrictions. In the example above, <span class="math inline">\(q = 2\)</span>, and the two restrictions are <span class="math inline">\(\beta_1 = 0\)</span> and <span class="math inline">\(\beta_2 = 0\)</span>. A “common sense” idea is to reject if either of the individual <span class="math inline">\(t\)</span>-statistics exceeds 1.96 in absolute value. But this “one at a time” test isn’t valid: the resulting test rejects too often under the null hypothesis (more than 5%)! That is because the <span class="math inline">\(t\)</span>-statistics themselves are often not independent. Instead, we need a <span class="math inline">\(F\)</span>-statistic, which tests all parts of a joint hypothesis at once. Unfortunately, these types of formulas can become quickly rather complex. Consider the <span class="math inline">\(F\)</span>-test for the special case of the joint hypothesis <span class="math inline">\(\beta_1 = \beta_{1,0}\)</span> and <span class="math inline">\(\beta_2 = \beta_{2,0}\)</span> in a regression with two regressors:</p>
<p><span class="math display">\[\begin{equation}
F = \frac{1}{2} \left(\frac{t_1^2 + t_2^2 - 2\hat{\rho}_{t_1,t_2}t_1 t_2}{1-\hat{\rho}^2_{t_1 t_2}}  \right)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat{\rho}_{t_1,t_2}\)</span> estimates the correlation between <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span>. Reject when <span class="math inline">\(F\)</span> is large (typically to be determined from large statistical tables). The F-statistic is large when <span class="math inline">\(t_1\)</span> and/or <span class="math inline">\(t_2\)</span> is large and the F-statistic corrects (in just the right way) for the correlation between <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span>. The formula for more than two <span class="math inline">\(\beta\)</span>’s is nasty unless you use matrix algebra. There is a nice large-sample (<span class="math inline">\(n&gt;50\)</span>) approximate distribution, which is the tail probability of the <span class="math inline">\(\chi^2_q /q\)</span> distribution beyond the <span class="math inline">\(F\)</span>-statistic actually computed.</p>
<p>Now, <code>STATA</code> does this in a much easier way by invoking the <code>test</code> command <strong>right</strong> after the regression. So, for example, we want to test the joint hypothesis that the population coefficients on <span class="math inline">\(STR\)</span> and expenditures per pupil (<span class="math inline">\(expn\)</span>) are both zero, against the alternative that at least one of the population coefficients is nonzero.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb30-1"><a href="modeling.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr str expn_stu el_pct, <span class="fu">r</span> </span>
<span id="cb30-2"><a href="modeling.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="kw">test</span> str expn_stu</span></code></pre></div>
<pre><code>Linear regression                               Number of obs     =        420
                                                F(3, 416)         =     147.20
                                                Prob &gt; F          =     0.0000
                                                R-squared         =     0.4366
                                                Root MSE          =     14.353

------------------------------------------------------------------------------
             |               Robust
     testscr | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -.2863992   .4820728    -0.59   0.553    -1.234002     .661203
    expn_stu |   .0038679   .0015807     2.45   0.015     .0007607    .0069751
      el_pct |  -.6560227   .0317844   -20.64   0.000    -.7185008   -.5935446
       _cons |   649.5779   15.45834    42.02   0.000     619.1917    679.9641
------------------------------------------------------------------------------


 ( 1)  str = 0
 ( 2)  expn_stu = 0

       F(  2,   416) =    5.43
            Prob &gt; F =    0.0047</code></pre>
<p>The output shows an <span class="math inline">\(F\)</span>-statistic with <span class="math inline">\(q=2\)</span> restrictions with outcome 5.43. Do not directly interpret this number, but know that <span class="math inline">\(\text{Prob} &gt; F = 0.0047\)</span> gives the probability that under the null-hypothesis this outcome is produced. So the joint null-hypothesis that both types of expenditures are zero (at the same time), can be rejected at a 5% (and a 1%) significance level. Other types of joint tests can easily be constructed as well. For example, when you want to know whether both coefficient add up to 1, then you would state <code>test str + expn_stu = 1</code>. The final point to make is the <span class="math inline">\(F\)</span>-test in the regression output itself. Here, that is for example <code>F(3, 416) = 147.20</code>. This is a joint test that all variables, except the constant, have no impact. So, <span class="math inline">\(\beta_i = 0\)</span> for all <span class="math inline">\(i\)</span> at the <strong>same time</strong>. It not often that you come across a general regression <span class="math inline">\(F\)</span>-test that does not reject the null-hypothesis. It namely implies that your independent variables do not contain any information about the dependent variable.</p>
<p>And with the <span class="math inline">\(F\)</span>-test, we now have discussed all regression outcome components displayed by <code>STATA</code>. Most of this information you do not need for your report but we will come back later to this.</p>
</div>
</div>
</div>
<div id="sec:nonlinear" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Non-linear specifications<a href="modeling.html#sec:nonlinear" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The model we are using is coined the <em>linear</em> regression model, and, indeed, one of the underlying assumptions is that the relations between the independent and dependent are linear. Consider the relation again between test scores and class sizes in the Californian school district data. Using the following code (note now the <code>twoway</code> command that ‘binds’ a scatter plot with a population regression line):</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb32-1"><a href="modeling.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">graph</span> <span class="kw">twoway</span> (<span class="kw">lfit</span> testscr str) (<span class="kw">scatter</span> testscr str)</span></code></pre></div>
<p>Which provides the following <code>STATA</code> output.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterlfitcaschool"></span>
<img src="figures/scatterlfit.png" alt="A linear relation" width="600px" />
<p class="caption">
Figure 6.4: A linear relation
</p>
</div>
<p>Indeed, there might be evidence that the relation depicted in Figure <a href="modeling.html#fig:scatterlfitcaschool">6.4</a>—if anything—is linear. But, clearly that is not the case for the relation between test scores and average district income. Namely, the syntax below:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb33-1"><a href="modeling.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">graph</span> <span class="kw">twoway</span> (<span class="kw">lfit</span> testscr avginc) (<span class="kw">scatter</span> testscr avginc)</span></code></pre></div>
provides the following <code>STATA</code> output.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterincome"></span>
<img src="figures/scatterincome.png" alt="A non-linear relation" width="600px" />
<p class="caption">
Figure 6.5: A non-linear relation
</p>
</div>
<p>Figure <a href="modeling.html#fig:scatterincome">6.5</a> shows a non-linear relation, where the effect of income tapers off (note the resemble with Figure <a href="introduction.html#fig:marginalutility">1.1</a>)—or, there is a marginal decreasing effect of average district income on average school test scores. Thus, in affluent neighborhood test scores are higher, but increasingly less so. Of course, you can still try to estimate this with a linear population regression line as in Figure <a href="modeling.html#fig:scatterincome">6.5</a>, but this introduces a <strong>bias</strong>. The estimate does not capture that what you want. Namely, it now holds that <span class="math inline">\(E(u \mid X = x) \neq 0\)</span>, because for small <span class="math inline">\(X\)</span>, say <span class="math inline">\(X&lt;10\)</span>, the residuals are negative, for medium sized <span class="math inline">\(X\)</span>s most residuals are positive and for large <span class="math inline">\(X&gt;40\)</span> all residuals are negative again. So, there is a clear relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(u\)</span> and they fail to be independent. This particular form of bias is coined <strong>specification bias</strong>. There is another issue here and that is that the effect on <span class="math inline">\(Y\)</span> of a change in <span class="math inline">\(X\)</span> depends on the value of <span class="math inline">\(X\)</span>—that is, the <em>marginal</em> effect of <span class="math inline">\(X\)</span> is not constant.</p>
<p>To remedy the specification bias, we will use nonlinear regression population regression <strong>functions</strong> of <span class="math inline">\(X\)</span>, or we estimate a regression function that is nonlinear in <span class="math inline">\(X\)</span>. Here, it is important to see that we do so by <em>transforming</em> <span class="math inline">\(X\)</span>, so the population regression ‘line’. The estimator still remains a linear regression model.</p>
<p>We will analyse below two complementary and often adopted approaches:</p>
<ol style="list-style-type: decimal">
<li>Using <strong>polynomials</strong> to transform <span class="math inline">\(X\)</span>. That means that the effect is approximated by a quadratic, cubic, or higher-degree polynomial. This approach as well governs to an extent so-called interaction effects which is a special case, where we multiply two different variables.</li>
<li>Using <strong>logarithmic</strong> transformations of <span class="math inline">\(X\)</span>, where <span class="math inline">\(Y\)</span> and/or <span class="math inline">\(X\)</span> is transformed by taking its logarithm. Here, the main focus is on the interpretation of the <span class="math inline">\(\hat{\beta}\)</span>s, as they change from a unit increase interpretation to a percentages interpretation which often can be found useful.</li>
</ol>
<div id="polynomials" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Polynomials<a href="modeling.html#polynomials" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our first approach to non-linear specification is applying polynomials of the variables that we suspect has a non-linear impact. If that is the independent variable <span class="math inline">\(X\)</span>, the we can construct the following <em>linear regression</em> model by using polynomials:
<span class="math display" id="eq:poly">\[\begin{equation}
Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X^2_i + \ldots + \beta_r X_i^r + u_i
\tag{6.2}
\end{equation}\]</span>
Note again that this is just the linear regression model—except that the regressors are powers of <span class="math inline">\(X\)</span>! So, in effect we transform the data—actually create new variables <span class="math inline">\(X^r\)</span>—, but the specification in parameters remains linear. Estimation, hypothesis testing, etc. proceeds as in the multiple regression model using OLS. However, the coefficients are now a bit more difficult to interpret. Consider the example of above about the relation between test scores average district income, where <span class="math inline">\(Income_i\)</span> is defined as the average district income in the <span class="math inline">\(i^{\mathrm{th}}\)</span> district (thousands of dollars per capita). For a quadratic specification, we specify the linear regression model as below:
<span class="math display">\[\begin{equation}
TestScore_i = \beta_0 + \beta_1 Income_i + \beta_2 (Income_i)^2 + u_i
\end{equation}\]</span>
For a cubic specification the linear regression model becomes:
<span class="math display">\[\begin{equation}
TestScore_i = \beta_0 + \beta_1 Income_i + \beta_2 (Income_i)^2 +
\beta_3 (Income_i)^3 + u_i
\end{equation}\]</span></p>
<p>First, we focus on the estimation of the quadratic function. In <code>STATA</code> this would look like:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb34-1"><a href="modeling.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr c.avginc##c.avginc, <span class="fu">r</span></span></code></pre></div>
<pre><code>Linear regression                               Number of obs     =        420
                                                F(2, 417)         =     428.52
                                                Prob &gt; F          =     0.0000
                                                R-squared         =     0.5562
                                                Root MSE          =     12.724

-----------------------------------------------------------------------------------
                  |               Robust
          testscr | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]
------------------+----------------------------------------------------------------
           avginc |   3.850995   .2680941    14.36   0.000      3.32401    4.377979
                  |
c.avginc#c.avginc |  -.0423085   .0047803    -8.85   0.000     -.051705   -.0329119
                  |
            _cons |   607.3017   2.901754   209.29   0.000     601.5978    613.0056
-----------------------------------------------------------------------------------</code></pre>
<p>Now, it is straightforward to test the null-hypothesis of linearity against the alternative that the regression function is a quadratic. Namely, we only have to consider the <span class="math inline">\(t\)</span>-statistic of the quadratic term. And that is larger than 1.96, so against a 5% significance level we reject the null-hypothesis of linearity.</p>
<p>Note by the way the syntax <code>c.avginc##c.avginc</code> which seems a bit strange. However, this particular line of code is very useful for later tabulation, plotting and other manipulations of the output. In this way <code>STATA</code> knows that there should be a quadratic effect of the same variable (<code>avginc</code>). The syntax <code>c.</code> denotes that the variable should be considered as continuous instead of as an integer (try it and behold the horrible output). There are four useful operators that you want to know when working with polynomials and interaction effect:</p>
<ul>
<li><code>i.</code> operator: this specifies that the following variable is an integers and should be considered on all its level. This actually create indicator or dummies variables</li>
<li><code>c.</code> operator: this specifies that the following variable is a continuous variables and should be treated as continuous.</li>
<li><code>#</code> binary operator that specifies an interaction between two variables</li>
<li><code>##</code> binary operator that specifies both interaction between two variables and the individual variable effect</li>
</ul>
<p>Plotting, non-linear population regression lines are a bit tricky. Namely, you want to combine a pylonomial with a linear dimension. One way of doing this is as follows:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb36-1"><a href="modeling.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">predict</span> hat1 </span>
<span id="cb36-2"><a href="modeling.html#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="kw">scatter</span> (testscr avginc) || (<span class="kw">line</span> hat1 avginc, <span class="kw">sort</span>)</span></code></pre></div>
<p>where after the regression we <strong>predict</strong> the test scores (and name it something like <code>hat1</code>) and then we ask for a line of the prediction for each value of average district income. Note, though, that we have to <code>sort</code> the prediction from small to large to get a smooth line. And this provides the nice curved population regression line in the following <code>STATA</code> output.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterqua"></span>
<img src="figures/scatterqua.png" alt="A non-linear relation" width="600px" />
<p class="caption">
Figure 6.6: A non-linear relation
</p>
</div>
<p>But what is now the marginal effect of average district income. That, now, depends on itself. Namely, <span class="math inline">\(\frac{\partial \text{testscore}}{\partial \text{income}} = \beta_1 + \beta_2 \text{income}\)</span>. Another way of seeing this is to compute the effects for different values of <span class="math inline">\(X\)</span>
<span class="math display">\[\begin{equation}
\widehat{TestScore_i} = 607.3 + 3.85 Income_i - 0.0423(Income_i)^2
\end{equation}\]</span>
The predicted change in test scores for a change in income from $5,000 per capita to $6,000 per capita then amounts to:
<span class="math display">\[\begin{eqnarray}
\Delta \widehat{TestScore} &amp;=&amp; 607.3 + 3.85 \times 6 -  0.0423 \times 6^2 \\
&amp;&amp; - (607.3 + 3.85\times 5 - 0.0423\times 5^2)\\
&amp;=&amp;3.4
\end{eqnarray}\]</span></p>
<p>And if calculate the predicted effects for different values of <span class="math inline">\(X\)</span>, then we get the following table:</p>
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:effectqua">Table 6.1: </span>Effect of <span class="math inline">\(X\)</span>
</caption>
<thead>
<tr>
<th style="text-align:left;">
Change in Income (1000 dollar per capita)
</th>
<th style="text-align:left;">
<span class="math inline">\(\Delta \widehat{TestScore}\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
from 5 to 6
</td>
<td style="text-align:left;">
3.4
</td>
</tr>
<tr>
<td style="text-align:left;">
from 25 to 26
</td>
<td style="text-align:left;">
1.7
</td>
</tr>
<tr>
<td style="text-align:left;">
from 45 to 46
</td>
<td style="text-align:left;">
0.0
</td>
</tr>
</tbody>
</table>
<p>Thus, the effect of a change in income is greater at low than high income levels (perhaps, a declining marginal benefit of an increase in school budgets?). But, be careful here! What is the effect of a change from 65 to 66? That is quite negative and already Figure <a href="modeling.html#fig:scatterqua">6.6</a> shows that a quadratic specification start to decline again the value of about 50; and perhaps that is not the behaviour that you want. So, with polynomials it is essential not to extrapolate outside the range of the data (and still interpret the outcome).</p>
<p>The estimation of a cubic specification is straightforward:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb37-1"><a href="modeling.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr c.avginc##c.avginc##c.avginc, <span class="fu">r</span></span></code></pre></div>
<pre><code>Linear regression                               Number of obs     =        420
                                                F(3, 416)         =     270.18
                                                Prob &gt; F          =     0.0000
                                                R-squared         =     0.5584
                                                Root MSE          =     12.707

--------------------------------------------------------------------------------------------
                           |               Robust
                   testscr | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]
---------------------------+----------------------------------------------------------------
                    avginc |   5.018677   .7073504     7.10   0.000      3.62825    6.409103
                           |
         c.avginc#c.avginc |  -.0958052   .0289537    -3.31   0.001     -.152719   -.0388913
                           |
c.avginc#c.avginc#c.avginc |   .0006855   .0003471     1.98   0.049     3.26e-06    .0013677
                           |
                     _cons |    600.079   5.102062   117.61   0.000     590.0499     610.108
--------------------------------------------------------------------------------------------</code></pre>
<p>Where if we now want to test the null- hypothesis of linearity, then we have to have invoke an <span class="math inline">\(F\)</span>-test. Namely, the alternative hypothesis is that the population regression is quadratic and/or cubic, that is, it is a polynomial of degree up to 3, so:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: Coefficients on <span class="math inline">\(Income^2\)</span> and <span class="math inline">\(Income^3 = 0\)</span></li>
<li><span class="math inline">\(H_1\)</span>: at least one of these coefficients is nonzero.</li>
</ul>
<p>And the outcome below shows that the null-hypothesis that the population regression is linear is rejected at the 5% (and 1%) significance level against the alternative that it is a polynomial of degree up to 3.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb39-1"><a href="modeling.html#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">test</span> avginc#avginc avginc#avginc#avginc  </span></code></pre></div>
<pre><code> ( 1)  c.avginc#c.avginc = 0
 ( 2)  c.avginc#c.avginc#c.avginc = 0

       F(  2,   416) =   37.69
            Prob &gt; F =    0.0000</code></pre>
</div>
<div id="interaction-variables" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Interaction variables<a href="modeling.html#interaction-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using interaction variables is a special case of polynomial effects. Namely, instead of multiply a variable with itself <span class="math inline">\(X\times X = X^2\)</span>, you now multiple a variable with another variable. And you want to do this to take into account interactions between independent variables. Assume, for example, that a class size reduction is more effective in some circumstances than in others (which is quite conceivable). Perhaps smaller classes help more if there are many English learners (i.e., large migrant communities), who need more individual attention. That is, <span class="math inline">\(\frac{\partial TestScore}{\partial STR}\)</span> might depend on <span class="math inline">\(PctEL\)</span>. More generally, this subsection looks into the fact that the marginal effect of <span class="math inline">\(\frac{\partial Y}{\partial X_1}\)</span> might depend on some other variable <span class="math inline">\(X_2\)</span>.</p>
<div id="interactions-between-two-binary-variables" class="section level4 hasAnchor" number="6.3.2.1">
<h4><span class="header-section-number">6.3.2.1</span> Interactions between two binary variables<a href="modeling.html#interactions-between-two-binary-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>First, we look into the simplest (and perhaps most insightful) case of two binary (dummy variables). Consider therefore the following linear regression model:
<span class="math display">\[\begin{equation}
Y_i =\beta_0 +\beta_1 D_{1i} + \beta_2 D_{2i} +u_i,
\end{equation}\]</span>
where both <span class="math inline">\(D_{1i}\)</span> and $ D_{2i}$ are now considered to be binary. Now, of course, <span class="math inline">\(\beta_1\)</span> is the effect of changing <span class="math inline">\(D_1=0\)</span> to <span class="math inline">\(D_1=1\)</span>. So, in this specification, this effect doesn’t depend on the value of <span class="math inline">\(D_2\)</span>. To allow the effect of changing <span class="math inline">\(D_1\)</span> to depend on <span class="math inline">\(D_2\)</span>, we have to include the interaction term <span class="math inline">\(D_{1i} \times D_{2i}\)</span> as a regressor:
<span class="math display">\[\begin{equation}
Y_i =\beta_0 +\beta_1 D_{1i} + \beta_2 D_{2i} + \beta_3 (D_{1i} \times D_{2i}) + u_i
\end{equation}\]</span></p>
<p>To interpret now the coefficient <span class="math inline">\(\beta_1\)</span> we compare the two cases for <span class="math inline">\(D_1=0\)</span> to <span class="math inline">\(D_1=1\)</span>”
<span class="math display">\[\begin{eqnarray}
E(Y_i|D_{1i}=0, D_{2i}=d_2) &amp;=&amp; \beta_0 + \beta_2 d_2 \\
E(Y_i|D_{1i}=1, D_{2i}=d_2) &amp;=&amp; \beta_0 + \beta_1 + \beta_2 d_2 + \beta_3 d_2
\end{eqnarray}\]</span>
If we now subtract them from each other:
<span class="math display">\[\begin{equation}
E(Y_i|D_{1i}=1, D_{2i}=d2) - E(Y_i|D_{1i}=0, D_{2i}=d_2) = \beta_1 + \beta_3 d_2
\end{equation}\]</span>
then we have the marginal effect of <span class="math inline">\(D_1\)</span> which now depends on <span class="math inline">\(d_2\)</span>. The interpretation of <span class="math inline">\(\beta_3\)</span> boils down to being incremental to the effect of <span class="math inline">\(D_1\)</span>, when <span class="math inline">\(D_2 = 1\)</span></p>
<p>Let us go back to our Californian school district example with the following variables to be used: test scores, student teacher ratio, and English learners. Let:
<span class="math display">\[\begin{eqnarray}
HiSTR &amp;=&amp; 1 \text{ if } STR \geq 20 \text{ and } HiEL = 1 \text{ if }
PctEL \geq 10 \\
HiSTR &amp;=&amp; 0 \text{ if } STR &lt; 20 \text{ and } HiEL = 0 \text{ if }
PctEL &lt; 10 \\
\end{eqnarray}\]</span>
And if we have the estimation results we get the following outcome.
<span class="math display">\[\begin{equation}
\widehat{TestScore} = 664.1 - 18.2 HiEL - 1.9 HiSTR - 3.5(HiSTR \times
HiEL)
\end{equation}\]</span>
So, how to interpret the various parameters? Perhaps the simple way is to construct the following two-by-two table:</p>
<table>
<caption>
<span id="tab:intdummies">Table 6.2: </span>Interpretation of interaction effects with dummies
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
<span class="math inline">\(HiEL = 0\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(HiEL = 1\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(HiSTR = 0\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(664.1\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(664.1 - 18.2 = 645.9\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(HiSTR = 1\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(664.1 - 1.9 = 662.2\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(664.1 - 1.9 - 18.2 - 3.5= 640.5\)</span>
</td>
</tr>
</tbody>
</table>
<p>Now, Table <a href="modeling.html#tab:intdummies">6.2</a> specifies for each combination (and there are exactly four of them) of <span class="math inline">\(HiSTR\)</span> and <span class="math inline">\(HiEL\)</span> the average expected test score outcome. Clearly, there are different ‘marginal’ effects of <span class="math inline">\(HiSTR\)</span>. Namely, the effect of <span class="math inline">\(HiSTR\)</span> when <span class="math inline">\(HiEL = 0\)</span> is <span class="math inline">\(-1.9\)</span>, whilst the effect of <span class="math inline">\(HiSTR\)</span> when <span class="math inline">\(HiEL = 1\)</span> is <span class="math inline">\(-1.9 - 3.5 = -5.4\)</span>. This points out that a class size reduction is estimated to have a bigger effect when the percent of English learners is large. However, when you estimate this in <code>STATA</code> then you see that this interaction is not statistically significant, because the <span class="math inline">\(t\)</span>-statistic equals <span class="math inline">\(3.5/3.1 = 1.1\)</span></p>
</div>
<div id="interactions-between-continuous-and-binary-variables" class="section level4 hasAnchor" number="6.3.2.2">
<h4><span class="header-section-number">6.3.2.2</span> Interactions between continuous and binary variables<a href="modeling.html#interactions-between-continuous-and-binary-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The second case we consider is between a continuous and a binary variable. First assume the following regression model:
<span class="math display">\[\begin{equation}
Y_i =\beta_0 + \beta_1 X_i + \beta_2 D_i + +u_i,
\end{equation}\]</span>
where <span class="math inline">\(D_i\)</span> is a binary variable and <span class="math inline">\(X\)</span> is a continuous variable. As specified above, the effect on <span class="math inline">\(Y\)</span> of <span class="math inline">\(X\)</span> (holding <span class="math inline">\(D\)</span> constant) = <span class="math inline">\(\beta_1\)</span>, which does not depend on <span class="math inline">\(D\)</span>. To allow the effect of <span class="math inline">\(X\)</span> to depend on <span class="math inline">\(D\)</span>, we can include the interaction term <span class="math inline">\(D_i \times X_i\)</span> as a regressor:
<span class="math display">\[\begin{equation}
Y_i =\beta_0 + \beta_1 X_i + \beta_2 D_i  + \beta_3 (D_i \times X_i) + u_i
\end{equation}\]</span></p>
<p>What this binary-continuous interaction does is essential create two different population regression lines. Namely, for observations with <span class="math inline">\(D_i= 0\)</span> (the <span class="math inline">\(D = 0\)</span> group or the <span class="math inline">\(D=0\)</span> regression line) there is:
<span class="math display">\[\begin{equation}
Y_i = \beta_0 + \beta_1 X_i  + u_i,
\end{equation}\]</span>
Whilst for observations with <span class="math inline">\(D_i= 1\)</span> (the <span class="math inline">\(D = 1\)</span> group or the <span class="math inline">\(D = 1\)</span> regression line) the regression line comes down to:
<span class="math display">\[\begin{eqnarray}
Y_i &amp;=&amp;   \beta_0 + \beta_2 + \beta_1 X_i + \beta_3 X_i + u_i \\
            &amp;=&amp;  (\beta_0 + \beta_2) + (\beta_1 + \beta_3) X_i + u_i
\end{eqnarray}\]</span></p>
And these two population regression lines might both differ in the level (the constant) and in the slope of the line. So, there are three possibilities as depicted in Figure <a href="modeling.html#fig:interaction">6.7</a>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:interaction"></span>
<img src="figures/Sheet44.jpg" alt="Three possible binary-continuous interaction outcomes" width="600px" />
<p class="caption">
Figure 6.7: Three possible binary-continuous interaction outcomes
</p>
</div>
<p>In the first panel (a), <span class="math inline">\(\beta_3 = 0\)</span>, so there is only a level effect. In the second panel (b), both <span class="math inline">\(\beta_2\)</span> and <span class="math inline">\(\beta_3\)</span> are not 0, so there is both a level and a slope effect. The last panel indicates that <span class="math inline">\(\beta_2 = 0\)</span>, meaning that there is only a slope effect. But how to interpreting the coefficients now? Therefore, we take the marginal effect of
<span class="math display">\[\begin{equation}
Y =\beta_0  + \beta_1 X  +\beta_2 D+ \beta_3 (D \times X)
\end{equation}\]</span>
which yields:
<span class="math display">\[\begin{equation}
\frac{\partial Y}{\partial X} = \beta_1 + \beta_3 D
\end{equation}\]</span>
Thus, the effect of <span class="math inline">\(X\)</span> depends on <span class="math inline">\(D\)</span> and <span class="math inline">\(\beta_3\)</span> is the increment to the effect of <span class="math inline">\(X\)</span>, when <span class="math inline">\(D = 1\)</span> (a slope effect)</p>
<p>To see this in our Californian school district example we now use the variables test scores, student teacher ratio and the as previously defined dummy variable <span class="math inline">\(HiEL\)</span> as:
<span class="math display">\[\begin{equation}
\widehat{TestScore} = 682.2 - 0.97 STR + 5.6 HiEL - 1.28(STR \times HiEL)
\end{equation}\]</span>
Now when <span class="math inline">\(HiEL = 0\)</span> the population regression line amounts to:
<span class="math display">\[\begin{equation}
\widehat{TestScore} = 682.2 - 0.97 STR
\end{equation}\]</span>
And when <span class="math inline">\(HiEL = 1\)</span> the population regression line is:
<span class="math display">\[\begin{eqnarray}
\widehat{TestScore} &amp;=&amp; 682.2 - 0.97 STR + 5.6 - 1.28 STR \\
&amp;=&amp; 687.8 - 2.25 STR
\end{eqnarray}\]</span>
Thus we have two regression lines: one for each <span class="math inline">\(HiSTR\)</span> group. And the conclusion is that a class size reduction is estimated to have a larger effect when the percent of English learners (migrant communities) is large.</p>
<p>Hypothesis testing is a before. To test whether the two regression lines have the same slope, the null-hypothesis boils down to the coefficient of <span class="math inline">\(STR \times HiEL\)</span> being zero: the <span class="math inline">\(t\)</span>-statistic of this one become <span class="math inline">\(-1.28/0.97 = -1.32\)</span> and thus we do not reject this test. To test whether the two regression lines have the same intercept, the null-hypothesis becomes the coefficient of <span class="math inline">\(HiEL\)</span> being zero, yielding: <span class="math inline">\(t = -5.6/19.5 = 0.29\)</span>, so we do not reject that null-hypothesis either. Interestingly, the null-hypothesis that the two regression lines are the same—population coefficient on <span class="math inline">\(HiEL = 0\)</span> and population coefficient on yields <span class="math inline">\(STR \times HiEL = 0\)</span>: <span class="math inline">\(F = 89.94 (p-value &lt; .001)\)</span>. So, we reject the joint hypothesis but neither individual hypothesis.</p>
</div>
<div id="interactions-between-two-continuous-variables" class="section level4 hasAnchor" number="6.3.2.3">
<h4><span class="header-section-number">6.3.2.3</span> Interactions between two continuous variables<a href="modeling.html#interactions-between-two-continuous-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The last case are interaction between two continuous variables and that is always a difficult case of interpret. Starting again with the model:
<span class="math display">\[\begin{equation}
Y_i =\beta_0 + \beta1 X_{1i} +\beta_2 {X_{2i}} +u_i,
\end{equation}\]</span>
where both <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> are continuous and as specified, the effect of <span class="math inline">\(X_1\)</span> doesn’t depend on <span class="math inline">\(X_2\)</span> and the effect of <span class="math inline">\(X_2\)</span> doesn’t depend on <span class="math inline">\(X_1\)</span>. Now, to allow the effect of <span class="math inline">\(X_1\)</span> to depend on <span class="math inline">\(X_2\)</span>, we include the interaction term <span class="math inline">\(X_{1i} \times X_{2i}\)</span> as a regressor. Where, to interpret the coefficients, we take the first derivative of <span class="math inline">\(X_1\)</span> in:
<span class="math display">\[\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 (X_{1i}
\times X_{2i}) + u_i
\end{equation}\]</span>
which yields:
<span class="math display">\[\begin{equation}
\frac{\partial Y}{\partial X} = \beta_1 + \beta_3 X_2
\end{equation}\]</span>
where <span class="math inline">\(\beta_3\)</span> should be interpreted as the increment to the effect of <span class="math inline">\(X_1\)</span> from a unit change in <span class="math inline">\(X_2\)</span>.</p>
</div>
</div>
<div id="logarithmic-transformations" class="section level3 hasAnchor" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Logarithmic transformations<a href="modeling.html#logarithmic-transformations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To incorporate non-linear effect, very often logarithmic transformations are used of <span class="math inline">\(Y\)</span> and/or <span class="math inline">\(X\)</span>, where typically we use <span class="math inline">\(\ln(X)\)</span> as the natural logarithm of <span class="math inline">\(X\)</span>. One feature of logarithmic transformations is that they permit modeling relations in percentage terms (like elasticities), rather than linearly. That is because:
<span class="math display">\[\begin{equation}
\ln(x+\Delta x) - \ln(x) = \ln (1 + \frac{\Delta x}{x}) \cong \frac{\Delta x}{x}
\end{equation}\]</span>
Note that this is an approximation, but from calculus we know that <span class="math inline">\(\frac{d \ln(x)}{dx}=\frac{1}{x})\)</span>. And the above approximation works quite well for small numbers. For example, numerically: <span class="math inline">\(\ln(1.01) = .00995 \cong .01\)</span> and <span class="math inline">\(\ln(1.10) = .0953 \cong .10\)</span>, where the latter is still rather close. Now remember the following rules for natural logarithms
1. <span class="math inline">\(\ln(a\times b)= \ln(a)+\ln(b)\)</span>
2. <span class="math inline">\(\ln(\frac{a}{b}) =\ln(a) - \ln(b)\)</span>
3. <span class="math inline">\(\ln(a^\alpha) = \alpha \ln(a)\)</span>
4. <span class="math inline">\(\ln(e^X) = X\)</span>.</p>
<p>When you encounter a nonlinear model such as the ones adopted in Chapter <a href="surplus.html#surplus">2</a> a strategy that often works is log-linearization. That works as follows
<span class="math display">\[\begin{equation}
Y = A K^\alpha L^{1-\alpha} \rightarrow \ln(Y) = \ln(A) + \alpha \ln(K) + (1-\alpha) \ln(L),
\end{equation}\]</span>
where you take the natural logarithm on both sides. There are three different cases of logarithmic regression models as specified in Table <a href="#tab:logspecification"><strong>??</strong></a>.</p>
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:logspecifications">Table 6.3: </span>Three logarithmic transformation
</caption>
<thead>
<tr>
<th style="text-align:left;">
Case
</th>
<th style="text-align:left;">
Population regression model
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
linear-log
</td>
<td style="text-align:left;">
<span class="math inline">\(Y_i=\beta_0 + \beta_1 \ln(X_i) + u_i\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
log-linear
</td>
<td style="text-align:left;">
<span class="math inline">\(\ln(Y_i)=\beta_0 + \beta_1 (X_i) + u_i\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
log-log
</td>
<td style="text-align:left;">
<span class="math inline">\(\ln(Y_i)=\beta_0 + \beta_1 \ln(X_i) + u_i\)</span>
</td>
</tr>
</tbody>
</table>
<p>Though statistical testing remains the same, the interpretation of the slope coefficient differs in each case. To derive the interpretationwe want to find the marginal effect of <span class="math inline">\(X\)</span> using the first derivative.</p>
<div id="linear-log-population-regression-model" class="section level4 hasAnchor" number="6.3.3.1">
<h4><span class="header-section-number">6.3.3.1</span> Linear-log population regression model<a href="modeling.html#linear-log-population-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The linear-log population regression model is specified as:
<span class="math display">\[\begin{equation}
    Y = \beta_0 + \beta_1 \ln(X)
\end{equation}\]</span>
Now take the first derivative:
<span class="math display">\[\begin{equation}
    \frac{\partial Y}{\partial X} = \frac{\beta_1}{X}
\end{equation}\]</span>
so
<span class="math display">\[\begin{equation}
    \beta_1  = \frac{\partial Y}{\partial X / X}
\end{equation}\]</span>
In this case that means that <span class="math inline">\(\beta_1\)</span> should be interpreted as the absolute change of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X\)</span> changes with <span class="math inline">\(\beta_1/100\)</span> percent. To illustrate this, consider the case where we take natural logarithm od district income, so we define the new regressor as, <span class="math inline">\(\ln(Income)\)</span></p>
<p>The model is now linear in <span class="math inline">\(\ln(Income)\)</span>, so the linear-log model can be estimated by OLS, which yields
<span class="math display">\[\begin{equation}
        \widehat{TestScore} = 557.8 + 36.42\times \ln(Income_i)
\end{equation}\]</span>
so an 1% increase in <span class="math inline">\(Income\)</span> is associated with an increase in test scores of 0.36 points on the test. And again, standard errors, confidence intervals, <span class="math inline">\(R^2\)</span>—all the usual tools of regression apply here. But the difficulty in plottin the new regression line remains. Consider the following <code>STATA</code> syntax, where we first have to define the new regressor by invoking the <code>generate</code> command.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb41-1"><a href="modeling.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> lninc = <span class="fu">ln</span>(avginc)</span>
<span id="cb41-2"><a href="modeling.html#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> testscr lninc, <span class="fu">r</span></span>
<span id="cb41-3"><a href="modeling.html#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="kw">predict</span> testhat</span>
<span id="cb41-4"><a href="modeling.html#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="kw">graph</span> <span class="kw">twoway</span> (<span class="kw">line</span> testhat avginc, <span class="kw">sort</span>) (<span class="kw">scatter</span> testscr avginc)</span></code></pre></div>
This now provides the following <code>STATA</code> output.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterlnincome"></span>
<img src="figures/scatterlnincome.png" alt="A non-linear relation" width="600px" />
<p class="caption">
Figure 6.8: A non-linear relation
</p>
</div>
<p>When you compare <a href="modeling.html#fig:scatterlnincome">6.8</a> with <a href="modeling.html#fig:scatterqua">6.6</a> then you notice that in the case of logarithm the population remains increasing (but less and less steep). This can be considered as an advantage when you want to estimate decreasing (or increasing) return.</p>
</div>
<div id="log-linear-population-regression-model" class="section level4 hasAnchor" number="6.3.3.2">
<h4><span class="header-section-number">6.3.3.2</span> Log-linear population regression model<a href="modeling.html#log-linear-population-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The second case we consider is the log-linear population regression model, as specified by:
<span class="math display">\[\begin{equation}
    \ln(Y) = \beta_0 + \beta_1 X
\end{equation}\]</span>
To find the interpretation of <span class="math inline">\(\beta1_1\)</span>, we again take the first derivative <span class="math inline">\(\frac{\partial Y}{\partial X}\)</span>, but first transform the model like this:
<span class="math display">\[\begin{equation}
    Y = exp( \beta_0 + \beta_1 X )
\end{equation}\]</span>
then take the first derivative:
<span class="math display">\[\begin{equation}
    \frac{\partial Y}{\partial X} = \beta_1  exp( \beta_0 + \beta_1 X ) = \beta_1 Y
\end{equation}\]</span>
and collec terms
<span class="math display">\[\begin{equation}
    \beta_1  = \frac{\partial Y / Y}{\partial X }
\end{equation}\]</span></p>
<p>The interpretation of <span class="math inline">\(\beta_1\)</span> now is that one unit change in <span class="math inline">\(X\)</span> causes a <span class="math inline">\(\beta_1\)</span> percentage in <span class="math inline">\(Y\)</span></p>
</div>
<div id="log-log-population-regression-model" class="section level4 hasAnchor" number="6.3.3.3">
<h4><span class="header-section-number">6.3.3.3</span> Log-log population regression model<a href="modeling.html#log-log-population-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Finally, we have our third case, being the log-log population regression model as specified by:
<span class="math display">\[\begin{equation}
    \ln(Y) = \beta_0 + \beta_1 \ln(X)
\end{equation}\]</span></p>
<p>To find the interpretation of <span class="math inline">\(\beta1_1\)</span>, we again take the first derivative <span class="math inline">\(\frac{\partial Y}{\partial X}\)</span>, but first transform the model like this:
<span class="math display">\[\begin{equation}
    Y = exp( \beta_0 + \beta_1 \ln(X) )
\end{equation}\]</span>
So
<span class="math display">\[\begin{equation}
    \frac{\partial Y}{\partial X} = \beta_1 /X  exp( \beta_0 + \beta_1 \ln(X) ) = \beta_1 Y /X
\end{equation}\]</span>
and after collecting terms we end up with an <strong>elasticity</strong>:
<span class="math display">\[\begin{equation}
    \beta_1  = \frac{\partial Y / Y}{\partial X / X }
\end{equation}\]</span></p>
<p>As an example consider the case when we want to regress ln(test scores) on ln(income). To do so, we first define a new dependent variable, ln(TestScore), and a new regressor, ln(Income)
The model is now a linear regression of ln(TestScore) against ln(Income), which can be estimated by OLS as follows
<span class="math display">\[\begin{equation}
\widehat{ln(TestScore)} = 6.336 + 0.0554 \times ln(Income_i),
\end{equation}\]</span>
where the interpretation is that an 1% increase in <span class="math inline">\(Income\)</span> is associated with an increase of .0554% in <span class="math inline">\(TestScore\)</span> (<span class="math inline">\(Income\)</span> up by a factor of 1.01, <span class="math inline">\(TestScore\)</span> up by a factor of 1.000554)</p>
<p>Suppose that we now want to plot both the log-linear and the log-log specification, then we can use the following syntax:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb42-1"><a href="modeling.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> lninc = <span class="fu">ln</span>(avginc)</span>
<span id="cb42-2"><a href="modeling.html#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> lntestscr = <span class="fu">ln</span>(testscr)</span>
<span id="cb42-3"><a href="modeling.html#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> lntestscr lninc, <span class="fu">r</span></span>
<span id="cb42-4"><a href="modeling.html#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="kw">predict</span> testhat1</span>
<span id="cb42-5"><a href="modeling.html#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> lntestscr avginc, <span class="fu">r</span></span>
<span id="cb42-6"><a href="modeling.html#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="kw">predict</span> testhat2</span>
<span id="cb42-7"><a href="modeling.html#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="kw">graph</span> <span class="kw">twoway</span> (<span class="kw">line</span> testhat1 avginc, <span class="kw">sort</span>) (<span class="kw">line</span> testhat2 avginc, <span class="kw">sort</span>) (<span class="kw">scatter</span> lntestscr avginc), <span class="bn">legend</span>(<span class="kw">order</span>(1 <span class="st">&quot;log-log specification&quot;</span> 2 <span class="st">&quot;log-linear specification&quot;</span> 3 <span class="st">&quot;Observations&quot;</span>)) </span></code></pre></div>
<p>which provides the following <code>STATA</code> output.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scattercompare"></span>
<img src="figures/scattercompare.png" alt="A non-linear relation" width="600px" />
<p class="caption">
Figure 6.9: A non-linear relation
</p>
</div>
<p>Note that the <span class="math inline">\(y\)</span>-axis is on a logarithmic scale here, thus the log-linear specification is now a linear line.</p>
</div>
<div id="summary-logarithmic-transformations" class="section level4 hasAnchor" number="6.3.3.4">
<h4><span class="header-section-number">6.3.3.4</span> Summary: logarithmic transformations<a href="modeling.html#summary-logarithmic-transformations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We have seen three different cases of logarithmic specification, differing in whether <span class="math inline">\(Y\)</span> and/or <span class="math inline">\(X\)</span> is transformed by taking logarithms. Now, the regression is linear in the new variable(s) <span class="math inline">\(\ln(Y)\)</span> and/or <span class="math inline">\(\ln(X)\)</span>, and the coefficients can be estimated by OLS where hypothesis tests and confidence intervals are now implemented and interpreted ‘as usual’. Only the interpretation of the coefficients differs from case to case and is directly related to percentage changes (growth) and elasticities. Oftentimes, the choice of specification, however, should be guided by judgment (which interpretation makes the most sense in your application?), tests, and plotting predicted values. Sometimes, though, you have a structural economic model such as Equation <a href="surplus.html#eq:directutility">(2.3)</a>, which defines the type of specification you should use. Finally, see that in economics many models exists with decreasing or increasing return to scale and that these are very closely related with logarithmic specifications.</p>
</div>
</div>
</div>
<div id="sec:fixedeffects" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Using fixed effects in panel data<a href="modeling.html#sec:fixedeffects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Multivariate regression is a powerfull tool for controlling for the effect of variables for which we have data. But often we do not have data on what we suspect might be important—data, such as individual characteristics like ambition, intelligence, drive or stamina. Or regional of country data, where the type of soil, the ruggedness (hilliness), or population density determine to a large extent the behaviour of people living on it. If we do not have this type of data, then it not always the case that everything is lost. Especially, when we have repeated observations, so observations of the same entity throughout time. This is referred to as panel data and requires one additional subscript <span class="math inline">\(t\)</span> as in <span class="math inline">\(X_{it}\)</span> indicating the observation <span class="math inline">\(X\)</span> on individual made at time <span class="math inline">\(t\)</span>. To understand why this sometimes works, we temporarily change to another dataset and that is the ‘fatality’ data collected by <span class="citation">Levitt and Porter (2001)</span> and deals with the relation between drunk driving and fatal accidents in the States of the US between 1982 and 1988. For this particular example we look at the impact of the ‘beer tax’, measured as the real tax in dollars on a case of beer, on ‘fatality’, measured as the number of annual traffic deaths per 10,000 people in the population of each stata. For this we first read the data and manipulate the mortality variable</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb43-1"><a href="modeling.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">use</span> <span class="st">&quot;./data/fatality.dta&quot;</span>, <span class="kw">clear</span></span>
<span id="cb43-2"><a href="modeling.html#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> fatality = allmort/pop * 10000</span></code></pre></div>
<p>and then run a simple regression:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb44-1"><a href="modeling.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">regress</span> fatality beertax, <span class="kw">robust</span></span></code></pre></div>
<pre><code>Linear regression                               Number of obs     =        336
                                                F(1, 334)         =      47.59
                                                Prob &gt; F          =     0.0000
                                                R-squared         =     0.0934
                                                Root MSE          =     .54374

------------------------------------------------------------------------------
             |               Robust
    fatality | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
     beertax |   .3646054   .0528524     6.90   0.000     .2606399     .468571
       _cons |   1.853308   .0471297    39.32   0.000     1.760599    1.946016
------------------------------------------------------------------------------</code></pre>
<p>But these outcomes are very strange. For every dollar increase in tax, number of fatal accidents per 10,000 people increases with 0.36, which is statistically significantly different from 0. What is going on here. Most likely this effect is biased because of omitted variable bias. States in the US differ widely in terms of population density, environment, institutions, religion, poverty, and so on and so forth. And Those state characteristics might influence both the variables beertax and fatality.</p>
<p>Fortunately, for each state we have yearly data. So, that is 7 observations per stata and we can make use of that by using fixed effects, which is a very common technique in the social sciences—especially in economics. We model the use of fixed effects in this example as follows:
<span class="math display">\[\begin{equation}
\text{fatality}_{it} = \beta_0 + \beta_1\text{beertax}_{it} + \beta_3 S_1 + \ldots + \beta_51 S_{48} + u_{it},
\end{equation}\]</span>
where <span class="math inline">\(S_i\)</span> denote indicator (dummies) for each state which constitute the fixed effects. In total there are 48 states in this dataset, so we have 48 dummies. Note that these fixed effects only depend on the state variation, not on time variation. So, essentially what these fixed effects capture is all state specific characteristics which are constant over time. And most of the characteristics’ examples given above do not vary that much over time, so by using these state fixed effects we can <strong>control</strong> for them. In <code>STATA</code> you can estimate this in a straightforward way as <code>regress fatality beertax i.state, robust</code>, but this lots of statistical output that you are usually not interested in. Almost just as easy would be is to invoke the <code>areg</code> command, where you specifically state that the state variable should be used as dummies but not shown using <code>absorb(state)</code>:
and then run a simple regression:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb46-1"><a href="modeling.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">areg</span> fatality beertax, absorb(state) <span class="kw">robust</span></span></code></pre></div>
<pre><code>Linear regression, absorbing indicators             Number of obs     =    336
Absorbed variable: state                            No. of categories =     48
                                                    F(1, 287)         =  10.41
                                                    Prob &gt; F          = 0.0014
                                                    R-squared         = 0.9050
                                                    Adj R-squared     = 0.8891
                                                    Root MSE          = 0.1899

------------------------------------------------------------------------------
             |               Robust
    fatality | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
     beertax |  -.6558737   .2032797    -3.23   0.001    -1.055982   -.2557655
       _cons |   2.377075   .1051516    22.61   0.000     2.170109    2.584041
------------------------------------------------------------------------------</code></pre>
<p>Now, see what happens with the coefficient of the beer tax variable. It changes sign! So from positive it becomes negative. That is how <strong>disruptive</strong> omitted variable bias can be. Also see that by including all these state fixed effects, the <span class="math inline">\(\bar{R^2}\)</span> now increase enormously to 91%, which does make sense because the states explain the variation in fatality rate to a large extent (e.g., compare Kansas with Connecticut).</p>
<p>This is just a snapshot of the use of fixed effects in panel data, but for now this is enough. But for now, know that the use of fixed effects can go a long way in addressing omitted variable bias.</p>
</div>
<div id="conclusion-and-discussion-2" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Conclusion and discussion<a href="modeling.html#conclusion-and-discussion-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="30">
<li id="fn30"><p>With right hand side we mean on the right side of the equal sign <span class="math inline">\(=\)</span>. It is often abbreviated with RHS.<a href="modeling.html#fnref30" class="footnote-back">↩︎</a></p></li>
<li id="fn31"><p>This is not entirely true. Increasing the R<span class="math inline">\(^2\)</span> explains <strong>in-sample</strong> variation better, not necessarily <strong>out-of-sample</strong>. The latter is really what matters for prediction and this is the focus of many machine learning techniques. Note that this argument is directly related with the regression towards the mean argument made in Subsection <a href="univariateregression.html#sec:genesis">5.3.1</a>.<a href="modeling.html#fnref31" class="footnote-back">↩︎</a></p></li>
<li id="fn32"><p><span class="math inline">\(Z\)</span> can be both known or unknown, so that is why we change from <span class="math inline">\(U\)</span> to <span class="math inline">\(Z\)</span><a href="modeling.html#fnref32" class="footnote-back">↩︎</a></p></li>
<li id="fn33"><p>In econometric textbooks, as, e.g, in <span class="citation">Stock, Watson, et al. (2003)</span>, this condition is weakened to only being correlation (<span class="math inline">\(Z\)</span> and <span class="math inline">\(X\)</span> are correlated). However, if the directed arrow goes from <span class="math inline">\(STR\)</span> into <span class="math inline">\(U\)</span> in Figure <a href="modeling.html#fig:unobshet">6.2</a> then that would lead to something else than omitted variables, namely to a difference between a direct (<span class="math inline">\(\text{STR} \longrightarrow \text{testscore}\)</span>) and an indirect effect (<span class="math inline">\(\text{STR} \longrightarrow U \longrightarrow \text{testscore}\)</span>).<a href="modeling.html#fnref33" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="univariateregression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="specification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
