% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Methods and Techniques for Social and Economic Research: Syllabus},
  pdfauthor={Paul Koster \& Thomas de Graaff},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Methods and Techniques for Social and Economic Research: Syllabus}
\author{Paul Koster \& Thomas de Graaff}
\date{2022-10-29}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\hypertarget{what}{%
\section*{What}\label{what}}
\addcontentsline{toc}{section}{What}

This syllabus provides the course material for the course \textbf{Methods and Techiques for Social and Economic Research}. With this course we would like to bridge the gap between on the one hand applied statistics and (micro-economic) modeling and on the other hand putting all this in practice when performing empirical research. As such this course can as well be seen as preparation for the bachelor thesis. But above all, the course aims to provide students with some tools that we see as very useful for research; not only in the socio-economic sciences but outside them as well.

As we only have a limited amount of time available for this course, the amount of topics we can deal with is by nature restricted. We decided to focus in the first three week on economic welfare from a behavioral perspective. Not only is the concept of economic welfare central to all most all economic theories, the behavioral perspective allows for more \emph{reflection} on the neo-classical assumptions typically made in introductionary economic courses. The last three parts of the course move on to empirical analysis by focusing on the basics of applied econometrics and as such builds upon the foundations of the statistics course in the first period of the second year. But now we challenge the student to build more elaborate statistical models (based partly upon the theory of the first three weeks of this course) where specific attention is given to \emph{presentation} and \emph{interpretation} of the results.

\hypertarget{why}{%
\section*{Why}\label{why}}
\addcontentsline{toc}{section}{Why}

Although there are many and very good introductory textbooks on economic models and applied econometrics, the combination of the two is seldom seen. Apart from that there are two reasons why we wanted to write our own material. First, usually less time is spent on why certain, and a first sight very restrictive, \emph{assumptions} are made. We want to bridge that gap and provide the student with more intuition on where models, evidence, and finally perhaps the ``truth'' (if there is such a thing) comes from. Second, how to \emph{present} statistical evidence and the \emph{interpretation} of that evidence is very important but usually not given much attention.

\hypertarget{for-whom}{%
\section*{For Whom}\label{for-whom}}
\addcontentsline{toc}{section}{For Whom}

This syllabus assumes that the reader has a basic working knowledge of introductory economics, statistics and some calculus (typically those method courses students enjoy in their first year). The book can however be read as stand-alone, although that requires some more attentive reading and practising. Where we think it is necessary we provide (references to) background material. For the course \textbf{Methods and Techiques for Social and Economic Research} the syllabus should be read \textbf{in total} and it might be wise to read the relevant material \emph{before} the respective lecture. The big advantage of course is that lectures and reading material now really go one-on-one.

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

In this first introductory chapter, we will lay out the relations between theory development and theory testing as they are the cornerstones of scientific progress. After all, a theory or idea can only be scientific if the theory can be tested and, if need be, refuted. If the theory cannot be tested then it is not science. We will also explain the basic workflow of scientific research and the tools needed with specific emphasis on research in the social sciences. We end with a reading guide where we discuss each chapter in this syllabus and the relations between the chapters.

\hypertarget{theory-models-and-hypotheses}{%
\section{Theory, Models and Hypotheses}\label{theory-models-and-hypotheses}}

In 2021, Guido Imbens, Joshua Angrist and David Card received the Nobel price for economics (officially \emph{The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel}). The field they work in is applied econometrics with specific focus on finding \textbf{causal} relations. That means that with data they want to test whether phenomenon \(X\) has an effect on phenomenon \(Y\). And that is what most applied econometric work nowadays is focused on. And we will focus on that as well in Chapter \ref{univariateregression}, \ref{modeling} and \ref{specification}. But how do you know what to test, or, in other words, where do phenomena \(X\) and \(Y\) come from? Those phenomena and possible relations which originate from scientific theories as you will have in all disciplines. And those theories are typically casts in models---usually in a very abstract manner. Models come in the form of computer simulations (such as with agent-based modeling), real physical models (as with displays), but often models are formulated in mathematical notation with the aim of being as precise, lucid and clear as possible. Chapter \ref{surplus}, \ref{erroreconsurplus} and \ref{moral} will indeed deal with a specific subset of economic models formulated with mathematical notation. But note that those models are not necessarily theory. Theory is the underlying set of relations and assumptions that can say something about the specific structure of models. But very often theory can lead to \textbf{multiple} types of models, each perhaps highlighting different aspects of the underlying theory. An example of such a theory is the Law of Diminishing Marginal Utility: each additional unit of the same good is appreciated less by consumers. This theory can be expressed in many mathematical ways but the underlying concept as displayed in Figure \ref{fig:marginalutility} always remains the same. This type of function, increasing but slower and slower, belongs to the family of \emph{concave} functions. A function with exponential growth (such as \(e^{gt}\) belongs to the family of \emph{convex} functions).

\begin{figure}

{\centering \includegraphics[width=600px]{_main_files/figure-latex/marginalutility-1} 

}

\caption{Law of Diminishing Marginal Utility}\label{fig:marginalutility}
\end{figure}

So how to relate this with each other in scientific research? Well, when doing research you are interested in something that is not yet known (the research gap). Your aim is to (partly) fill this research gap by answering a research question. To answer this research question you need theory (a theoretical framework); what do you need to assume, what are the most important (moderator) variables, how do they relate with each other, and so on and so forth. From this theory you construct a model. Not necessarily a mathematical one. For example, you can also make a model in a Geographical Information System environment where you visualize layers of information that you think are most relevant based upon theory (in this case often previous scientific literature). Or you make a simulation model examining risks of flooding by rivers. The final step is the stage where your model should provide you with some answers. Sometimes they are concerned with optimality (what is the best location of a new road in a GIS environment), prediction (where are river dikes most vulnerable), or with establishing a (causal) relation. And it is the latter that the second part of this course deals with. How can we know that there is a relation between phenomenon \(X\) and phenomenon \(Y\) and how do we know whether that relation is causal?

For that we use applied econometrics (which is a form of applied statistics but then in the social-economic sciences domain---the exact difference will be discussed in Chapter \ref{univariateregression}). And to establish a, hopefully causal, relation, we test our models with empirical data. Be aware, though, that the applied econometrics materials we teach in this course (and in all introductory courses of applied statistics and econometrics all over the world---the ``101'' courses) is based on so-called frequentist statistics (we will revisit its basic assumptions in Chapter \ref{univariateregression}). The exact definition is not important for now but know that it is intrinsically related with hypothesis testing.

And hypothesis testing is most often associated with that scientific philosopher---and perhaps the only one you know---Karl Popper (as displayed in \ref{fig:popper}).

\begin{figure}

{\centering \includegraphics[width=300px]{./figures/popper} 

}

\caption{Karl Popper}\label{fig:popper}
\end{figure}

Popper was a so-called empiricist and claimed that theories in the empirical sciences (that includes most of the social sciences) can never be proven, only rejected. That is why you can reject a null-hypothesis (\(H_0\)), but \textbf{never} accept the alternative hypothesis (\(H_a\)). And this is very much related with the theoretical framework behind frequentist statistics. Loosely speaking, in frequentist statistics you construct a world where \(H_0\) is true and try to reject that world with data (we will come back to this in Chapter \ref{univariateregression})---but that world does not say something the validity of the alternative hypothesis. Now, Popper never claimed that rejecting one null-hypothesis will reject a whole theory. For that you need a larger body of evidence, including results from all sorts of studies---not only statistical ones, leading to a \emph{general} consensus among the scientific community (Popper 2005).

In truth, although the scientific approach of working with null-hypotheses is a very valuable one (and remarkably practical), it does not lead to definitive answers. That is because contradicting models can lead to similar null-hypotheses. Moreover, often the direct connection between research question and null-hypothesis is not a direct one. Consider that you want to know the effect of \(X\) on \(Y\), so your research question is: ``What is the effect of \(X\) on \(Y\)?''. But, in a frequentist world you only reject hypotheses---thus leading to results in the line of: ``The effect of \(X\) on \(Y\) is \emph{not} \(\ldots\)''. This makes the evidence for your research question at least circumstantial and in the best cases indirect. The bottom-line here is that one needs to be careful in drawing conclusions based on null-hypotheses (and in a broader sense based on models in general). Scientific research typically advances very slowly---but hopefully in a robust and parsimonious way!

\hypertarget{doing-research-in-the-social-sciences}{%
\section{Doing Research (in the Social Sciences)}\label{doing-research-in-the-social-sciences}}

It is remarkable that, although in principle students are (should be) prepared for scientific research, they receive little guidance in \emph{how} they should do scientific research. What are the tips \& tricks of the trade and what---and, more importantly why---should you use with respect to specific (types of) applications and what is the relation between them. In our view most of this should belong in your first course upon entering the university (with the appropriate course title ``Research Methods 101''). And some of it you indeed have learned in your first year, but in our experience students still lack ``operational'' knowledge. Therefore, we discuss below the four elements we think are among the most important---at least for this course. There are others, but for now this will do.

\hypertarget{work-tidy}{%
\subsection{Work tidy}\label{work-tidy}}

Our first and most important tip is to work tidy. Try to make your work look \textbf{good}. And with work we mean everything you submit (such as tutorials, papers, examinations, and theses). And that is because lecturers are just like people and often think from primary instincts with their reptilian brain: if it doesn't look good, not much time is spent on arguing and thinking as well! Moreover, when your work is difficult to read, lecturers get annoyed. Making your work look good and in the same time more lucid and transparent also serves a higher purpose as it is then easier to detect mistakes. Namely, everyone makes mistakes. The important thing is to detect them early, learn from them and remedy them. This advances science in general and is a very important feature of the scientific process. Chapter \ref{specification} will spent additional time on working tidy and making it looking good.

\hypertarget{know-where-your-stuff-is}{%
\subsection{Know where your stuff is}\label{know-where-your-stuff-is}}

A second very straightforward tip is to be organised and to know where your stuff is. Often, students come to us for help with all their files piled on a stack on their desktop and facing difficulties finding where their work is. It is always advisable to use a folder structure and have one folder for one project (or for one course). And to the use sub-folders for data, text, code, pdf's and so forth. A second tip for organisation is to think about versioning. As the well-known Figure \ref{fig:version} shows the number of versions of one file very quickly can get out of hand. Think at least about a consistent naming structure (perhaps with the date involved such as \texttt{paper\_mt\_20221215.doc}).

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/version} 

}

\caption{Version confusion}\label{fig:version}
\end{figure}

\hypertarget{make-notes}{%
\subsection{Make notes}\label{make-notes}}

One skill that in our opinion is given not too much attention is making \emph{useful} notes. It has been proven that writing things down is beneficial; not only for remembering but also for understanding. And that seems to be best just by using a pen as this slows writing down and you have to think what to write down. Underlining or marking is useful less beneficial than writing accompanying notes. But when should you write notes? Well, when attending lectures of course but also when reading. To leverage your notes as much as possible it is important that you have a system where you can \emph{retrieve} your notes and compare them with other notes. The latter is the hardest part, but is in the long run the most rewarding as new connections are created between lectures, courses, books, and years. You do not need any fancy tools for this (there is literally a ton of applications to be found on internet), Microsoft's Onenote or Evernote are more than good enough. Where the workflow typically is to first use pen and paper to \emph{capture} notes and thereafter rewrite and organise your notes in a notes system.

\hypertarget{use-a-reference-manager}{%
\subsection{Use a reference manager!}\label{use-a-reference-manager}}

Perhaps the tool that has the quickest pay-off is a reference manager. For those of you who are not using one yet: \textbf{do} it. Why? Because you never have to think about your reference list again. All reference managers come with plugins for Word or other text-editors (or type-setters such as LaTeX) that enable you to \emph{automatically} generate reference list based upon in-text citations which the reference manager can also provide. You only need less than an hour to set it up, but you very quickly become more efficient (and thus \emph{save time} in future work). There are many reference managers out there, but we advise Zotero as it is open source. There is both a cloud and desktop version and it comes with a handy tutorial. It also provides a plugin for your browser to automatically import the bibliographic details of the paper you are reading at the moment.

\hypertarget{statistical-software}{%
\section{Statistical software}\label{statistical-software}}

As quantitative research becomes more and more important in the social sciences you need software to \textbf{manage} your data and provides statistical and applied econometric \textbf{analyses}. Ideally, an open-source package is used (such as \texttt{R} or \texttt{Python}), but they have a steep-learning curve and do not work immediately out-of-the-box. The statistical software we use in this course is \texttt{STATA} and is more intuitive (compared to \texttt{R} or \texttt{Python} that is!) and, above all, all economists use it. So, the user base is large and that is important, because for each problem there is much material to be found on internet (including videos). Be aware though that there is one disadvantage in using \texttt{STATA} and that is that it is not open-source.

\hypertarget{reading-guide}{%
\section{Reading Guide}\label{reading-guide}}

This course will not concern itself with theory as such, but more with methods to describe that theory (the models) and how to test that theory (the applied econometrics). Chapters \ref{surplus}---\ref{moral} deal with economic model. First, Chapter \ref{surplus} revisits basic economic theory again with an emphasis on measuring (economic) welfare. Subsequently, Chapter \ref{erroreconsurplus} challenges this framework and especially the assumption that all actors make complete rational decisions (the \emph{homo economicus} assumptions). What happens when you introduce behavioral errors---such at not enough time to consider all choices---in the model? Chapter \ref{moral} then introduces moral considerations---based on rather recent developments---to the model and starts to ask normative questions (how should welfare be distributed?).

Chapter \ref{univariateregression} constitutes a break from theoretical modeling to empirical modeling and introduces the basic concepts of applied econometrics in the form of univariate regression. Chapter \ref{modeling} extends this framework to a multivariate regression setting, but in the same deals as well with the translation of theoretical (socio-economic) models to empirical models that are testable. Chapter \ref{specification} discusses how to \emph{specify} your model---which variables should you include and which variables not---and how to present your findings to a wider audience (that includes assessors). The final chapter summarizes and provides a general discussion.

\hypertarget{surplus}{%
\chapter{Introduction to Economic Surplus}\label{surplus}}

Practical cost-benefit analysis is widely used in applied economic analysis and often employs a definition of economic surplus that adds up consumer and producer surplus and potential external costs. This chapter discusses the behavioral assumptions underlying this calculation and show how to operationalize the definition of economic surplus to derive economic surplus changes and externality taxes. It employs a mathematical approach suitable applied to economic partial equilibrium analysis.

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

\hypertarget{background}{%
\subsection{Background}\label{background}}

This chapter introduces the classical economic perspective on economic surplus. Economic surplus is a quantitative estimate of the value that markets generate for society according to economists. Besides analysing the impacts of economic policies for societies (do policies have effects and do they work as intended?), one of the core tasks of economists is to give recommendations about what is supposed to be valuable for society (do policies add value?).

Economic analyses of the value that markets generate also are important in debates about sustainability. What economists contribute to this debate is that they seek to analyse behavioural choices of consumers and producers using statistical techniques and mathematical modelling and that they come up with quantitative estimates of value. The contribution of this chapter is that we move from conceptual analysis of market figures to mathematical descriptions and connections to statistical measurement. We do so for partial equilibrium analysis, meaning that we focus on the analysis of a market for a single good (with multiple good, issues of substitution and complementarity comes into play).

Four conceptual pillars of economic analysis are important for applied economic analysis of surplus and this chapter aims to emphasize the structural relation between these pillars along the road:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  \textbf{Optimization}: market actors choose their best (feasible) option.
\item
  \textbf{Equilibrium}: market actors choose their best feasible option when interacting with others.
\item
  \textbf{Empiricism}: behavioural models need to be tested with data.
\item
  \textbf{Choice-based normativity}: choices express value of market actors and value of market actors is equal to value for society.
\end{enumerate}

The first core principle states that consumers and producers optimize when they make their choices. For consumers this implies it is assumed that they maximize their utility. A mathematical description of utility is captured by the direct utility function which maps consumption quantities and remaining money to some cardinal number. We will discuss the mathematical behavioural model underlying consumer optimization in Section \ref{sec:conchoices} and discuss the implications for consumer value in markets. We also show at some points how this model of consumer behaviour relates to pillar (iii).

For producers, pillar (i) implies that they minimize their costs given a production target when they are assumed to be price-takers. Section \ref{sec:procbeh} discusses the basic cost-minimization model of producers and the implications for producer value in markets.

The second core principle states that equilibrium in markets results from the behavioral choices and \emph{interaction} of consumers and producers. Section \ref{sec:econsurplus} of these lecture notes discuss partial equilibrium and implications for total economic value in markets. The discussion in these sections of the lecture notes is conceptually speaking rather basic as substitution to other markets is ignored. Some complexity is added by connecting equilibrium to the third pillar of empiricism. It is very important for you to understand the core concepts and assumptions and the calibration underlying a basic partial equilibrium model in order to understand the extensions that are provided in the weeks that follow.

The third pillar is discussed along the way. Empirical analysis can be used to estimate the structural parameters of observed demand functions and production functions using regression techniques. These techniques relate mathematical narratives of choice behaviour to empirical data. The mathematical structure that is imposed sometimes requires that parameters in regression have particular values. Hypothesis testing can be used to see which model fits the data better. Calibration can be used to connect estimated elasticities from the literature to market demand and supply functions. Examples are provided to show how you can obtain quantitative estimates of total economic value.

Section \ref{sec:extcosts} applies the model of market value and introduces the idea of external costs. This is a key concept in environmental economics and analyses of sustainability. From an economic perspective, external costs resulting from market trade lower the value of a market and can be a motivating factor for regulatory intervention of a government. For a classical economist a sustainable market is a market where external costs are correctly priced. Section \ref{sec:extcosts} discusses the regulation of an environmental externality in a market and thereby provides the necessary bridge between the economic theory and your bachelor program. Analytical examples are provided to show how you can obtain quantitative estimates of total economic value.

We will come back to the four pillars in the chapters that follow where we zoom in on the impact of bounded rationality and other views on normativity.

\hypertarget{sec:conchoices}{%
\section{Consumer choices and consumer value}\label{sec:conchoices}}

\hypertarget{utility-functions-inverse-demand-and-demand}{%
\subsection{Utility functions, inverse demand and demand}\label{utility-functions-inverse-demand-and-demand}}

Why do consumers choose to consume particular meat products? What is the consumer value of newly developed vegan product? How does households' demand for solar panels respond to price decreases? To study these research questions, it is helpful to develop a model of consumer choice. For economists, models of choice are mathematical narratives about how consumers make their choices. The story of consumer choice by economists assumes that consumers optimize the choice value that they derive from the consumption of market goods given constraints they face. In this section the consumer story is partly simplified and we investigate the decision of the consumer to spend money on the market good versus spending money on other goods or services given a budget constraint. We therefore do not study market goods that are close complements or close substitutes.

Value is mathematically conceptualized using a direct utility function that depends on the consumption levels. The direct utility function of the consumer is defined by \(U(Q,G)\), where \(Q\) is the demand for the market good and \(G\) is the demand for the outside good. The price of the market good is given by \(p\) and the price of the outside good is given by \(p_G\). Both prices are given for the consumer and cannot be directly affected by their behaviour. This assumption rules out forms of bilateral bargaining about the market price between consumers and producers.

The price for the outside good can be interpreted as the costs or price index for a basket of goods for the consumers, where the market good is excluded from this basket. When prices of products in the basket increase over time, this will impact the price of the outside good.

Net spendable income of the consumer is assumed to be exogenous and given by \(Y\). This income is after tax income and can be used freely by the consumer. Total spending of the consumer is given by \(pQ+p_G G\), because it is assumed that all net income is spent either on the market good or the outside good. The budget constraint of the individual is then given by:
\begin{equation}
Y - pQ - p_G G = 0.
\label{eq:budgetconstraint}
\end{equation}
From this budget constraint we can derive the consumed quantity of the outside good \(G\):

\begin{equation}
G = \frac{Y-pQ}{p_G}
\label{eq:outsidegood}
\end{equation}

The quantity consumed of the outside good increases in the income level \(Y\), decreases in the price of the basket of goods \(p_G\), and decreases in consumer spending \(pQ\) on the market good. Substituting this quantity in the direct utility gives us an expression that only has the quantity level of the market good as a choice variable:

\begin{equation}
U \left(Q, \frac{Y-pQ}{p_G} \right) 
\end{equation}

The optimal choice of the quantity of the market good is governed by the utility of consumption (first argument) and the disutility of spending money on the market good (second argument). Optimal consumption can be derived by setting the total derivative with respect to the market quantity to 0:

\begin{equation}
\frac{dU}{dQ} = U_Q - \frac{p}{p_G}U_G = 0
\end{equation}

Here, \(U_Q=\frac{\partial U}{\partial Q}\) is the marginal utility of consuming the market good. Here we use the notation \(\frac{dU}{dQ}\) for the total derivative (both arguments of the utility function), and the notation \(\frac{\partial U}{\partial Q}\) for the partial derivative with respect to \(Q\) (only the first argument). Marginal utility of consumption is the slope of the direct utility function with respect to the first argument \(Q\). Furthermore, \(U_G=\frac{\partial U}{\partial G}\) is the marginal utility of consumption of the outside good. This is equal to the price of the outside good multiplied with the marginal utility of income \(p_G \frac{\partial U}{\partial Y}\). The second part of the first-order condition therefore captures the disutility of paying a price \(p\) for the market good. It also accounts for the relative price of the market good compared to the price for the basket of goods. From the first-order condition an implicit expression for the inverse demand curve can be derived. This expression shows that the ratio of prices is equal to the ratio of marginal utilities.

\begin{equation}
\frac{p}{p_G} = \frac{U_Q}{U_G} \longleftrightarrow p = p_G\frac{U_Q}{U_G}
\end{equation}

An implicit solution means that the price \(p\) potentially appears at the right-hand side of the equation. This inverse demand curve is the maximum marginal willingness to pay (MWTP) for the good. It depends on the relative size of the marginal utilities and increases in the price of the basket of goods. For many practical purposes the price of the basket of goods is normalized to \(p_G=1\) resulting in inverse demand as a function of the ratio of marginal utilities:

\begin{equation} 
p = \frac{U_Q}{U_G}
\end{equation}

The MWTP can be employed as a measure of consumer benefits. Much of the analytical work of demand analysists focusses on particular specifications that result in explicit solutions for the inverse demand curve. These explicit solutions have only exogenous parameters and quantity \(Q\) at the right-hand side and can be analysed using empirical data.

\hypertarget{examples-of-demand-and-inverse-demand-functions}{%
\subsection{Examples of demand and inverse demand functions}\label{examples-of-demand-and-inverse-demand-functions}}

To illustrate the model, we provide an example of three particular utility functions. First, we discuss a direct utility function without income effects and exponential marginal utility of consumption. Second, we discuss the well-known Cobb-Douglas utility function. This utility function captures non-linear impacts of net spendable income on direct utility. Third, we discuss a specification of the direct utility function that captures minimum threshold levels for consumption. Such a specification is particularly relevant for particular basic goods that a consumer cannot do without.

\hypertarget{the-exponential-linear-utility-function}{%
\subsubsection{The exponential-linear utility function}\label{the-exponential-linear-utility-function}}

Suppose direct utility is specified as:

\begin{equation}
U(Q,G) = \frac{A}{\alpha}\left( 1- e^{-\alpha Q} \right ) + BG
\label{eq:directutility}
\end{equation}

The marginal utility of consumption is given by the partial derivative with respect to the market good:

\begin{equation}
\frac{\partial U}{\partial Q} = Ae^{-\alpha Q} > 0.
\end{equation}

This marginal utility is positive implying that direct utility increases when consumption of the market good increases. The marginal utility of the outside good is given by the partial derivative with respect to \(G\):

\begin{equation}
\frac{\partial U}{\partial G} = B > 0.
\end{equation}

It is also positive implying more of the outside good is considered to be better.

The direct utility is concave\footnote{Remember, concave function have decreasing marginal returns such as in Figure \ref{fig:marginalutility}} in the consumption of the market good and linear in the consumption of the outside good. This can be proved by investigating the second-order partial derivatives which are negative for consumption (first argument) and 0 for the outside good (second argument):

\begin{equation}
\frac{\partial^2 U}{\partial Q^2} = - \alpha Ae^{-\alpha Q} < 0, \qquad \frac{\partial^2 U}{\partial G^2} = 0.
\end{equation}

The intuitive appeal of the assumption of concavity is that the additional utility gained from consuming an additional unit of the market good is decreasing in the consumption level. This implies that the \emph{marginal utility} of consuming the market good is positive, but decreasing. For example, people might enjoy eating meat, but the additional enjoyment of meat consumption decreases in the consumption level. At some point meat saturation kicks in and marginal utility becomes (close to) 0.

In line with pillar (i), the consumer is assumed to maximize utility while accounting for the budget constraint. From the budget constraint we can obtain the consumption level of the outside good:

\begin{equation}
Y - pQ - G = 0 \longleftrightarrow G= Y - pQ.
\end{equation}

Substituting gives:

\begin{equation}
U(Q,G)=\frac{A}{\alpha}(1-e^{-\alpha Q})+B(Y - p Q).
\end{equation}

The first-order condition for utility maximisation is given by the total derivative of the direct utility function with respect to the market good:

\begin{equation}
\frac{dU}{dQ} = Ae^{\alpha Q} - pB.
\label{eq:foc}
\end{equation}

The first positive part is the marginal utility of consumption which is positive and the second negative part the marginal utility of payments for the market good. From the first-order condition one can solve for the inverse demand curve which expresses price \(p\) as a function of quantity \(Q\). Furthermore, one can also solve for the demand curve which expresses quantity as a function of the price. In order to have a unique solution we have to check the second-order total derivative:

\begin{equation}
\frac{dU^2}{dQ^2} = -\alpha Ae^{\alpha Q} < 0.
\end{equation}

This second-order total derivative is negative, implying that the optimum is a unique maximum. We start with solving for the inverse demand curve \(p(Q)\)\footnote{Recall that an inverse demand curve is a function of \(Q\) that determines \(p\)}, which is given by:\footnote{Recall that for an optimal utility, which is what we aim for, the condition \(\frac{dU}{dQ} = 0\) should hold, so from \eqref{eq:foc} we then know that \(Ae^{\alpha Q} = pB\) leading to \eqref{eq:optimalinversedemand}}

\begin{equation}
p(Q)=\frac{A}{B} e^{-\alpha Q}.
\label{eq:optimalinversedemand}
\end{equation}

Now we have an explicit solution for the MWTP because the right-hand side of this equation only depends on \(Q\) and exogenous utility parameters. At \(Q=0\), the MWTP is highest and equal to \(\frac{A}{B}\). For higher values of \(Q\), the MWTP decreases which is in line with decreasing marginal benefits of consumption. The parameter \(\alpha\) governs the steepness of this decrease. The higher the parameter \(\alpha\) is, the steeper the decrease of the marginal benefits.

Using the first-order condition for utility maximisation \eqref{eq:foc}, one can also derive the demand curve (again, by setting \(\frac{dU}{dQ} = 0\) and then isolating \(Q\)):

\begin{equation}
Q(p)=\frac{1}{\alpha}\ln(A) - \frac{1}{\alpha}\ln(p) - \frac{1}{\alpha}\ln(B)  
\end{equation}

This expression shows that demand for the market good:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Decreases in the price \(p\) as \(\frac{\partial Q}{\partial p} =-\frac{1}{\alpha}\frac{1}{p}<0\);
\item
  Increases in the utility parameter \(A\) as \(\frac{\partial Q}{\partial A} =\frac{1}{\alpha} \frac{1}{A} >0\);
\item
  Decreases in the utility parameter \(B\) as \(\frac{\partial Q}{\partial A} =-\frac{1}{\alpha} \frac{1}{B} <0\);
\item
  Decreases in the utility parameter \(\alpha\) as \(\frac{\partial Q}{\partial \alpha} =-\frac{1}{\alpha^2} \left( \ln(A) - \ln(p) - \ln (B) \right) <0\);
\end{enumerate}

As this point it is useful to see how this analytical result can be connected to the third pillar: \emph{empiricism}. The demand function can be empirically estimated using statistical techniques and data on quantities and prices. Suppose an empirical economist estimates the following linear-log regression:

\begin{equation}
Q = \beta_0 + \beta_1 \ln p + \epsilon.
\label{eq:empiricalinversedemand}
\end{equation}

From equation \eqref{eq:empiricalinversedemand} one can derive some of the utility parameters. The parameter \(\beta_1\) is expected to be negative as demand decreases in the price. The mathematical model suggests that this parameter should be equal to \(\frac{1}{\alpha}\). We can therefore write \(\alpha = -\frac{1}{\beta_1} > 0\). The constant parameter \(\beta_0\) is---according to the behavioural model---equal to \(\beta_0 = \frac{1}{\alpha}\ln(\frac{A}{B})\). We can therefore write \(\ln \frac{A}{B} = \alpha \beta_0\) or \(\frac{A}{B} = e^{\alpha \beta_0} = e^{-\frac{1}{\beta_1}\beta_0}\).

The estimated parameters resulting from the regression equation therefore have a one-to-one correspondence to the underlying utility maximisation problem. An assumption that needs to be made is that the error \(\epsilon\) in the regression equation is irrelevant for the analysis and captures particular measurement errors at the side of the researcher. If this is not the case, errors capture another part of the marginal utility and things become more complicated.\footnote{An interesting question that gives a hint for solving this issue is: could you propose a direct utility function that deals with \(\epsilon\) as a parameter of the direct utility function?} Dealing with unknown regression noise therefore already leads to interesting epistemological questions for applied economic research that we will assume away for now.

Furthermore, the behavioural model analyses individual choice behaviour and therefore the measurement of prices and demand should ideally be done at the individual level to stay close to the behavioural mathematical narrative that is developed.

\hypertarget{cobb-douglas}{%
\subsubsection{The Cobb-Douglas utility function}\label{cobb-douglas}}

One of the disadvantages of the introduced direct utility function of the previous section, is that the marginal utility of consumption of the outside good is assumed to be constant for each consumption level of the market good. The behavioural narrative therefore does not account for the plausible intuition that consumers with a lower income derive more utility from an income increase compared to consumers with a lower income (see for example Layard, Mayraz, and Nickell (2008)). In order to account for this, we propose a Cobb-Douglas type utility function that captures decreasing marginal utility for the market good and the outside good and directly substitute the expression for the outside good:\footnote{It is also possible to specify this utility function as: \(U(Q,G)=Z Q^A G^B\). Log-linearizing gives the utility function used in the text and has no impact on the first-order condition. The constant \(Z\) is irrelevant for the analysis of demand.}

\begin{equation}
U(Q,G) = A\ln(Q) + B\ln(G) = A \ln(Q) + B \ln (Y - p Q). 
\end{equation}

The first-order condition is now given by the following total derivative:

\begin{equation}
\frac{d U}{d Q} = \frac{A}{Q} - \frac{B p}{Y - p Q} = 0. 
\label{eq:cdfoc}
\end{equation}

Solving this first-order condition for the quantity level gives the following demand function:\footnote{This solution is unique. You can check this by proving that the total second-order derivative is negative.}

\begin{equation}
Q(p) = \frac{A}{A + B}\frac{Y}{p}.
\end{equation}

The demand curve depends on the exogenous utility parameters \(A\) and \(B\), increases in the income level \(Y\) and decreases in the price \(p\). Higher income levels therefore result in higher demand for the market good. The consumer spends \(p Q(p)=\frac{A}{A+B} Y\) on the market good. From the budget constraint we can work back the spending on the outside good which are equal to \(Y-pQ=\frac{B}{A+B} Y\). This is an attractive aspect of the Cobb-Douglas specification: expenditure shares on the market good are directly related to relative utility parameters \(\frac{A}{A+B}\).

For empirical purposes, the demand curve can be estimated using econometric techniques. Log-linearizing the demand curve gives the following regression equation:

\begin{equation}
\ln Q = \ln(\frac{A}{A+B}) + \ln Y - \ln p + \epsilon.
\end{equation}

The Cobb-Douglas specification of utility therefore \emph{requires} that the parameters for income and price in the demand function are \(1\) and \(-1\) respectively. This can be tested using statistical hypothesis testing. The inverse demand curve for this particular utility function is given by:

\begin{equation}
p(Q) = \frac{A}{A + B}\frac{Y}{Q}.
\end{equation}

A disadvantage of this specification from a behavioural perspective is that the MWTP for the first marginal unit of the good is infinite. This is because when \(Q \longrightarrow 0\) then \(p(Q) \longrightarrow \infty\). An advantage of this specification is that the MWTP for the market good is increasing in the net spendable income \(Y\). For environmentally sustainable goods it is expected that this will be a plausible assumption.

\hypertarget{a-utility-function-that-allows-for-minimum-consumption-tresholds}{%
\subsubsection{A utility function that allows for minimum consumption tresholds}\label{a-utility-function-that-allows-for-minimum-consumption-tresholds}}

We continue the discussion on demand models with a third example. It might be that individuals need to consume a minimum amount of particular basic goods such as food or energy. The Stone-Geary utility function is able to deal with this. Again, we use a log-linearized version of the utility function:

\begin{equation}
U(Q,G) = A \ln (Q - \underline{Q}) + B \ln (G - \underline{G}),
\end{equation}

In this equation \(\underline{Q} <Y\) is the minimum amount needed of the market good and \(\underline{G}<Y\) the minimum amount needed for the outside good. This minimum amount of the outside good can be interpreted as consumer spending needed for basic goods which are not the market good. Utility goes to minus infinity when consumption is lower or equal to these minimum amounts. Substituting \(G=Y-pQ\) from the budget constraint gives:

\begin{equation}
U(Q,G) = A \ln (Q - \underline{Q}) + B \ln (Y - p Q - \underline{G}),
\end{equation}

The first-order condition is given by the following total derivative:

\begin{equation}
\frac{d U }{d Q} = \frac{A}{Q - \underline{Q}} - \frac{B p}{Y - pQ - \underline{G}} = 0, 
\end{equation}
which can be written as:

\begin{equation}
bpQ - bp\underline{Q} = AY - ApQ - A\underline{G}. 
\end{equation}

Rearranging gives:
\begin{equation}
Qp(B+ A) = AY = Bp\underline{Q} - A\underline{G}.
\end{equation}

Solving for the demand gives:
\begin{equation}
Q(p) = \frac{A}{A+B}\frac{Y}{p} + \frac{B}{A+B}\underline{Q} - \frac{A}{A+B}\frac{1}{p}\underline{G}.
\end{equation}

The first term of the demand function is equal to the standard Cobb-Douglas demand derived in the previous section. The second term accounts for the threshold consumption of the market good. Market demand increases in the threshold demand \(\underline{Q}\). The third term shows that market demand decreases in the threshold demand \(\underline{G}\) as more money is needed to consume basic goods in the basket of outside goods. When the market good is not necessary and \(\underline{Q} = 0\), market demand can be written as:

\begin{equation}
Q(p) = \frac{A}{A+B}\frac{Y}{p} - \frac{A}{A+B}\frac{1}{p}\underline{G} = \frac{A}{A+B}\frac{1}{p}\left(Y - \underline{G} \right).
\end{equation}

Log-linearizing this demand function gives the following regression equation:

\begin{equation}
\ln Q = \ln \frac{A}{A+B} + \ln(Y - \underline{G}) - \ln p + \epsilon.
\end{equation}

Demand is not linear in the log of the net spendable income anymore as first the necessary spending on the outside good should be accounted for. The price coefficient and the coefficient for \(\ln (Y - \underline{G})\) still enter with a coefficient equal to \(1\). For applied work this means that if the Stone-Geary model of behaviour is correct, one first should subtract spending on basic goods \(\underline{G}\) from the net spendable income, and introduce the logged variable as an explanatory variable in the regression.

\hypertarget{demand-modelling-conclusion}{%
\subsubsection{Demand modelling: conclusion}\label{demand-modelling-conclusion}}

The take away of these three examples is that particular specifications of the direct utility function result in equations for individual demand curves that can be empirically operationalized and estimated making additional assumptions. Some specifications of the direct utility function are behaviourally more plausible whereas other specifications might be easier to operationalize using empirical data. This is the reason that economists prefer to have multiple modelling options in their toolbox.

\hypertarget{consumer-benefits-and-surplus-in-markets}{%
\subsection{Consumer benefits and surplus in markets}\label{consumer-benefits-and-surplus-in-markets}}

\hypertarget{market-demand-and-elasticities}{%
\subsubsection{Market demand and elasticities}\label{market-demand-and-elasticities}}

The behavioural models of the previous section can be viewed as models of individual choice behaviour. The demand and inverse demand functions are therefore in principal specified at the individual level. For the analysis of markets, these functions are aggregated into a market (inverse) demand function. This implies that when we analyse market quantities and prices in regression analysis, additional assumptions should be made to relate the results of these regressions to the underlying distribution of utility parameters in the population. Developing models that account for a correct mathematical aggregation to market demand curves often requires stringent assumptions on preferences.

Consider for example the Cobb-Douglas demand that is developed in section \ref{cobb-douglas} and denote \(Q_n (p)=\frac{A_n}{A_n+B_n} Y_n \frac{1}{p}\) as the individual demand. Assume we have a population of \(N\) individuals. The market demand can be given as the sum of the individual demands:

\begin{equation}
Q^D(p) = \sum_{n= 1}^{N}\frac{A_n}{A_n+B_n} Y_n \frac{1}{p}=\frac{1}{p} \sum_{n= 1}^{N}\frac{A_n}{A_n+B_n} Y_n.
\end{equation}

Although aggregation is possible, it requires additional analytical assumption to be neatly done.

Because of complexity we avoid this aggregation discussion by neglecting the behavioural model underlying the decision of the population of consumers. We continue by directly interpreting the market inverse demand curve that is estimated in regressions as the total MWTP for the market good and assume the regression constant adjusts for aggregation issues. From now on we therefore use \(p(Q)\) to denote the market inverse demand curve. This market inverse demand curve measures what the price of the market good would have to be for \(Q\) units of it to be demanded (Varian (2003), 2006, p.268). Consumers can buy the good multiple times and are sorted \emph{along} the inverse demand curve on the basis of their MWTP where the intuition for the derivation of the MWTP stems from the behavioural model that is developed.

The MWTPs estimated from regressions of market demand can then be interpreted as measures of consumer benefits. The total consumer benefits for the market good are in turn given by the sum of the benefits of all consumers who participate in the market. Besides looking at market data, one could also operationalise the intuition of MWTP using survey research. In a survey we could for example ask people to express their WTP for an additional unit of consumption and sort the WTPs to obtain the inverse demand function. For example, we could ask the following question:

\begin{quote}
What would be your maximum willingness to pay for an additional unit of these vegan chicken nuggets?
\end{quote}

The answers to these questions can serve as an input to derive the inverse demand curve and the demand curve.

For applied work we are often interested in the price elasticity of market demand. This is defined by:\footnote{Note that the \(\epsilon\) here is different from that what we normally use in regression equations.}

\begin{equation}
\epsilon = \frac{\partial Q^D}{\partial p} \frac{p}{Q^D(p)}.
\end{equation}

The definition of an elasticity is as follows: for a 1\% increase in the market price, the market demand will change with \(\epsilon\)\%. For many market goods, this elasticity will be negative as higher prices result in lower demand. When the elasticity is lower than \(-1\), the market good has elastic demand. When the elasticity is in between 0 and \(-1\), it has inelastic demand.

We illustrate the concept of price elasticity of demand using an example. Brons et al. (2002) give an overview of price elasticities of demand for air travel. Figure \ref{fig:brons} below is from their paper.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/brons} 

}

\caption{Overview of studies on the price elasticities of market demand for air travel}\label{fig:brons}
\end{figure}

For short distance air travel the median value of the price elasticity is \(-1.5\). This implies that for short distance travel a \(1\)\% increase in the price will reduce air travel for these trips with \(1.5\)\%. For medium-distance trips the median price elasticity of market demand is given by \(-1.4\) implying that a \(1\)\% increase in the price reduces demand with \(1.4\)\%. For long-distance flights the median is \(-0.8\), implying that a \(1\)\% increase in the price reduces demand with \(0.8\)\%. For the assumed numbers above, long-distance flight demand is therefore inelastic whereas short distance and medium distance flights have elastic demands. are therefore less sensitive to the price compared to travellers.

Another example of estimated price elasticities comes from a meta-analysis of Andreyeva, Long, and Brownell (2010). They give a summary of estimates of absolute price elasticities of demand for food categories in the US (displayed in Figure \ref{fig:foodelas}). All food categories have inelastic demand as elasticities are in between \(0\) and \(-1\). The estimated price elasticities are between \(-0.27\) (eggs) and \(-0.81\) (food away from home). A \(1\)\% increase in the price of eggs therefore results in \(0.27\)\% lower demand. For beef the estimated elasticity is \(-0.75\), implying that a \(1\)\% increase in the price for beef results in a \(0.75\)\% lower demand. Uncertainty in the estimates is reported with the 95\% confidence intervals.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/foodelasticities} 

}

\caption{Price elasticity estimates for food categories. }\label{fig:foodelas}
\end{figure}

\hypertarget{deriving-consumer-benefits}{%
\subsubsection{Deriving consumer benefits}\label{deriving-consumer-benefits}}

Total consumer benefits in the market are given by all the consumer benefits of those who buy the market good. An analogous continuous representation of total consumer benefits is given by the integral under the \emph{inverse market demand function} from 0 to the equilibrium quantity:

\begin{equation}
CB = \int_0^{Q\ast} p(Q) dQ.
\end{equation}

For example, when the inverse demand curve is given by \(p(Q)=\frac{A}{B} e^{-\alpha Q}\), consumer benefits can be derived analytically by using the primitive function of the inverse demand function:

\begin{align}
CB &= \int_0^{Q^\ast}  \frac{A}{B} e^{-\alpha Q} = \left[-\frac{1}{\alpha}\frac{A}{B}e^{-\alpha Q} + z  \right]_0^{Q^\ast} \\
&= \left(-\frac{1}{\alpha}\frac{A}{B}e^{-\alpha Q^\ast} + z \right) - \left(-\frac{1}{\alpha}\frac{A}{B}e^{-\alpha 0} + z \right)\\
&=\left(-\frac{1}{\alpha}\frac{A}{B}e^{-\alpha Q^\ast} \right) - \left(-\frac{1}{\alpha}\frac{A}{B} 1 \right) \\
&=\left(-\frac{1}{\alpha}\frac{A}{B}e^{-\alpha Q^\ast} \right) +\frac{1}{\alpha}\frac{A}{B}  \\
&= - \frac{1}{\alpha}\frac{A}{B}\left[e^{-\alpha Q^\ast} -1 \right] =  \frac{1}{\alpha}\frac{A}{B}\left[ 1 - e^{-\alpha Q^\ast}\right].
\end{align}

The process of the derivation of the primitive function can be viewed as reverse differentiation: the derivative of the primitive function gives back the original function (in our case the inverse demand function). It therefore can only be derived up to a particular constant \(z\). The primitive function should be evaluated at the equilibrium demand \(Q\ast\) and the point \(0\). The difference between these function evaluations gives the area under the inverse demand function from \(0\) to \(Q\ast\). The third step shows that the constant \(z\) then drops out and is therefore irrelevant for the outcomes.

The final result shows that consumer benefits can be numerically estimated when the parameters \(\alpha\), \(\frac{A}{B}\) and \(Q\ast\) are known. From our earlier discussion on this inverse demand function, we had \(\alpha=-\frac{1}{\beta_1} >0\) and \(\frac{A}{B}=e^{-\frac{\beta_0}{\beta_1}} >0\). As \(0 < e^{-\alpha Q\ast} <1\). Therefore, in line with intuition, consumer benefits always will be positive.

Substituting the regression parameters in the expression for the consumer benefits gives an estimate for consumer benefits as a function of the observed/derived equilibrium quantity and the regression parameters of the market demand function:

\begin{equation}
\widehat{CB} = -\hat{\beta}_1 e^{-\frac{\beta_0}{\beta_1}}\left[1- e^{-\frac{Q\ast}{\beta_1}} \right].
\end{equation}

Using regression techniques in combination with assumptions on the market inverse demand curve, economists are therefore able to estimate the consumer benefits of consumption of a market good. It is important to account for the dimensions of this expression. When consumption levels are given \emph{per year} and prices are given \emph{in euros}, the consumer benefits for the particular market are given \emph{in euros per year}.

For example, consider the market for vegan chick nuggets. Assume that the total market quantity for these nuggets is given by \(200,000\) pieces sold per year in The Netherlands. The estimated parameters are given by \(\hat{\beta}_1=-0.08\) and \(\beta_0 =1.44\). Substituting in the equation for the consumer benefits gives:

\begin{equation}
\widehat{CB} = 0.08 e^{-\frac{1.44}{-0.08}}\left[1- e^{-\frac{200,000}{-0.08}} \right] = 5,252,798 \text{ euros per year}.
\end{equation}

For this example, the market for vegan nuggets in the Netherlands therefore results in consumer benefits of about \(5.25\) million euros per year.

\hypertarget{deriving-changes-in-consumer-benefits.}{%
\subsubsection{Deriving changes in consumer benefits.}\label{deriving-changes-in-consumer-benefits.}}

Sometimes researchers are interested in changes in consumer benefits rather than the level of consumer benefits. This is especially relevant for particular policy recommendations/interventions where the change in value compared to doing nothing is analysed. Suppose there is a change in equilibrium quantity from \(Q^\ast\) to \(Q^{\ast\ast}\). The change in consumer benefits can then be derived using rules of integrals and is given by:

\begin{equation}
 \Delta CB = \int_0^{Q^{\ast\ast}} p(Q)dQ -  \int_0^{Q^\ast} p(Q)dQ =  \int_{Q^\ast}^{Q^{\ast\ast}} p(Q)dQ.
\end{equation}

Suppose we employ a stable Cobb-Douglas inverse demand curve over time which is derived in the previous section and defined as: \(p(Q)=\frac{A}{A+B}\frac{Y}{Q}\). The change in consumer benefits can then be derived analytically:

\begin{align}
 \Delta CB &= \int_{Q^\ast}^{Q^{\ast\ast}} \frac{A}{A+B}\frac{Y}{Q} dQ = \left[\frac{A}{A+B} Y \ln Q = z  \right]^{Q^{\ast\ast}}_{Q^\ast} = \frac{A}{A+B} Y \left[\ln Q^{\ast\ast} - \ln Q^\ast \right] \\
 &= \frac{A}{A+B} Y \left[\ln \left(\frac{Q^{\ast\ast}}{Q^\ast}\right) \right].
\end{align}

This uses the fact that the primitive function of \(\frac{1}{Q}\) is given by \(\ln(Q)\). The last step is taken using rules of working with logarithms: \(ln(x)-ln(y)=ln(x/y)\). Again, the primitive function is derived of which the derivative gives back the inverse demand function. The change in consumer benefits is positive when \(Q^{\ast\ast}>Q^\ast\), and negative when \(Q^{\ast\ast}<Q^\ast\). For this particular specification of the utility function the change is proportionally increasing in the income level. In order to operationalise this equation, one has to account for the dimension of the quantity level and the dimension of the income variable. For the budget constraint the net spendable income is relevant. Suppose income in the regression is given by net spendable income per month and is equal to \(Y=3,500\) euros.

Furthermore, suppose there is an increase in the consumption of vegan nuggets from \(200,000\) pieces per year in 2019 to \(250,000\) pieces per year in 2020. For the analysis of consumer benefits, we need the equality \(\frac{A}{A+B}=e^{\beta_0}\), where \(\beta_0\) is the constant of the regression. An econometrician estimates the following regression:

\begin{equation}
 Q = \beta_0 + \beta_1 \ln p + \beta_2 \ln Y + \epsilon, 
\end{equation}
and finds \(beta_0 = 5.2\), \(\beta_1 =-1.02\) and \(\beta_2 = 0.97\).

The constant is therefore estimated at \(beta_0=5.2\). The parameters before the logged price and the logged income are not significantly different from \(1\). Therefore, we can assume Cobb-Douglas preferences describe choice behaviour correctly. The next step is to substitute the numerical values for \(\beta_0\), \(Q^\ast\) and \(Q^{\ast\ast}\) in the expression for the change in consumer benefits. This results in:

\begin{equation}
\Delta CB = e^{5.2} 3500 \left[\ln \frac{250000}{200000} \right] = 141574 \text{ euros per year}.
\end{equation}

This example shows how regression results can be connected to empirical estimates of consumer benefits.

\hypertarget{deriving-consumer-surplus}{%
\subsubsection{Deriving consumer surplus}\label{deriving-consumer-surplus}}

Until now we have not accounted for the fact that the consumer also pays a price for the product. Consumer surplus is defined as the total consumer benefits minus the total consumer costs:

\begin{equation}
CS = \int^{Q^\ast_0} p(Q)dQ - p^\ast Q^\ast.
\end{equation}

Here \(p^\ast\) is the equilibrium price that is observed and \(Q^\ast\) the equilibrium quantity. In order to calculate consumer surplus for our vegan chicken nuggets example, this requires input on the equilibrium price. Suppose the equilibrium price is equal to \(2\) euros per unit. Consumer surplus for The Netherlands is then given by:

\begin{equation}
CS=5,252,798-2 \times 200,000=1,252,798 \text{ euros per year}.
\end{equation}

To conclude: the mathematical analysis of consumer choice behaviour is useful to structure our thinking about consumer behaviour and the origins of the market inverse demand curve. This market inverse demand curve can be interpreted as the MWTP of a group of consumers with MWTPs sorted from high to low and can be estimated using regression techniques or studied using survey questions. Mathematical modelling can be employed to relate estimated parameters in regressions to the calculation of (changes) in consumer benefits where it is important to account for the dimensions of the variables.

\hypertarget{sec:procbeh}{%
\section{Poducer behaviour and surplus}\label{sec:procbeh}}

\hypertarget{producer-cost-functions-and-cost-minimisation}{%
\subsection{Producer cost functions and cost minimisation}\label{producer-cost-functions-and-cost-minimisation}}

The next step is to analyse the behaviour of producers in the market. This section discusses the classical economic model that helps to understand the economic aspects of decisions of producers. It is assumed that producers minimize their costs given a production target. We start simple by assuming that there are two inputs for the production function: capital \(K\) and labour \(L\). The \emph{production function} gives the production output as a function of the inputs: \(f(K,L)\). The production costs for the firm are given by the \emph{cost function} \(C(K,L)\) which shows the costs for a given combination of inputs. This cost function captures the costs of capital---which are usually dependent on interest rates---and the costs of labour which can be interpreted as wages and/or overhead.

Suppose the firm seeks to produce a target quantity for the market equal to \(Q\). According to the model, the firm then will minimize the costs with respect to capital \(K\) and labour \(L\), but given the production constraint that the target quantity is equal to the production output. Formally, this can be written as:

\begin{equation}
\displaystyle{\min_{K,L}  C(K,L)} \text{ subject to }f(K,L) = Q.
\end{equation}

As there is a constrained involved, this minimisation problem can be analysed using Lagrangian techniques. This technique rewrites the constrained optimisation problem as an unconstrained optimisation problem. First, the Lagrangian function \(H\) needs to be defined. The first part of this Lagrangian function gives the goal function that needs to be minimized or maximized. The second part of this function adds a new variable \(\lambda\) to the optimisation problem and multiplies this variable with the constraint:

\begin{equation}
 H = C(K,L) + \lambda(Q - f(K,L)).
\end{equation}

The original optimisation problem of the firm is then mathematically reformulated as an unconstrained problem:

\begin{equation}
\displaystyle{\min_{\lambda, K,L} H}.
\end{equation}

To solve this unconstrained minimisation problem, we need to analyse the familiar first-order conditions with respect to capital, labour and the new variable \(\lambda\) (which is the so-called Lagrangian multiplier). The three first-order conditions are given by:

\begin{align}
\frac{\partial H}{\partial \lambda} &=Q - f(K,L) = 0,\\
\frac{\partial H}{\partial K} &= \frac{\partial C}{\partial K} - \lambda\frac{\partial f}{\partial K}= 0,\\
\frac{\partial H}{\partial L} &= \frac{\partial C}{\partial L} - \lambda\frac{\partial f}{\partial L}= 0.\\
\end{align}

The first first-order condition with respect to the Lagrangian multiplier guarantees that in equilibrium, the production target is always reached. The reason is that this condition states that the production target \(Q\) should be equal to the production level \(f(K,L)\). The second condition guarantees optimal choice of capital by equation the marginal costs (or `price') of capital with the marginal benefits. The third condition has a similar structure and equates the marginal costs of labour with the marginal benefits of labour. Assume a linear cost function with input prices equal to \(r\) for capital and \(\omega\) for labour. When the cost function is linear in inputs and input prices, the second and the third first-order condition can be rewritten:

\begin{equation}
\frac{r}{\omega} = \frac{\frac{\partial f}{\partial K}}{\frac{\partial f}{\partial L}}.
\end{equation}

Or in other words: the ratio of input prices is equal to the ratio of marginal productivities of the two respective inputs, which is the marginal rate of substitution. This result shares similarities with the result for consumer optimisation where the ratio of prices is equal to the ratio of marginal \emph{utilities}.

Before moving on to an analytical example it is useful to reflect on the idea of the cost function. In the analysis it is assumed that the cost function only depends on the inputs. This assumption implies that firms neglect potential negative costs of production as the cost function itself is not a function of \(Q\).

\hypertarget{specifying-and-interpreting-the-production-function}{%
\subsection{Specifying and interpreting the production function}\label{specifying-and-interpreting-the-production-function}}

To make things a bit less abstract we develop an example assuming a Cobb-Douglas type of production function and a linear cost function with input prices equal to \(r\) for capital and \(\omega\) for labour. Before moving into technical derivations, it is useful to interpret the production function in more detail in order to better understand what it captures. The Cobb-Douglas type production function is given by:

\begin{equation}
f(K,L) = TK^{\alpha_K}L^{\alpha_L} \qquad T >0.
\end{equation}

This function has several properties. First, both inputs are \(necessary\) in order to have positive production. When there is no capital investment or no labour available, production is not possible and equals \(0\). Second, the parameter \(T>0\) captures the state of overall production technology of the firm. For a given combination of inputs \(K\) and \(L\), a higher parameter \(T\) implies higher production levels. Improvements in overall technology therefore can result in higher production levels at the same level of inputs. Third, together with the parameter \(T\), and the input levels \(K\) and \(L\), \(\alpha_K\) and \(\alpha_L\) determine the marginal production impact of increasing input levels. It is plausible to assume that production is increasing in the input levels. The first derivatives with respect to the inputs show that production is increasing in the inputs:

\begin{align}
\frac{\partial f}{\partial K} &= \frac{\alpha_K}{ K} TK^{\alpha_K}L^{\alpha_L} >0 \\
\frac{\partial f}{\partial L} &= \frac{\alpha_L}{ L} TK^{\alpha_K}L^{\alpha_L} >0 \\
\end{align}
To have increasing production in the inputs \(K\) and \(L\) the parameters \(\alpha_K\) and \(\alpha_L\) should therefore be positive.

Fourth, the production function should plausibly be concave in the level of each production input. An additional unit of an input will have lower impacts when there is already a high level of the input present. This can be investigated by looking at the second derivatives:

\begin{align}
\frac{\partial^2 f}{\partial K^2} &= \frac{\alpha_K(\alpha_K -1)}{ K^2} TK^{\alpha_K}L^{\alpha_L} < 0 \\
\frac{\partial^2 f}{\partial L^2} &= \frac{\alpha_L(\alpha_L - 1)}{ L^2} TK^{\alpha_K}L^{\alpha_L} < 0 \\
\end{align}
In order to have concave impacts we should make the assumptions \(\alpha_K<1\) and \(\alpha_L<1\).
Fifth, the inputs in the Cobb-Douglas production function are complements. We can investigate this mathematically by looking at the cross-derivatives of the production function which are given by:

\begin{equation}
\frac{\partial^2 f}{\partial K \partial L} = \frac{\partial^2 f}{\partial L \partial K} = \frac{\alpha_K}{K}\frac{\alpha_L}{L} TK^{\alpha_K}L^{\alpha_L} >0
\end{equation}

These cross-derivatives are positive showing that the marginal productivity of the inputs increases in the levels of the other inputs. Therefore, the inputs are complements.

Sixth, the substitution parameters \(\alpha_L\) and \(\alpha_K\) of the production function have a very useful economic interpretation. They are the elasticity of production with respect to the inputs. This elasticity is defined as the percentage change in production for a percentage change in one of the inputs. For example, for labour it is given by:

\begin{equation}
\frac{\partial f}{\partial L} \frac{L}{f(K,L)} = \frac{\alpha_L}{L}TK^{\alpha_K}L^{\alpha_L}\frac{L}{TK^{\alpha_K}L^{\alpha_L}} =\alpha_L.
\end{equation}

Similarly, for capital we find:

\begin{equation}
\frac{\partial f}{\partial K} \frac{K}{f(K,L)} = \frac{\alpha_K}{K}TK^{\alpha_K}L^{\alpha_L}\frac{K}{TK^{\alpha_K}L^{\alpha_L}} =\alpha_K.
\end{equation}

In the next section we will show that these elasticities play a key role in the determination of the shape of the cost function with respect to quantity produced.

Seventh, we can investigate whether there are increasing, decreasing or constant returns to scale in production levels. When we multiply all the inputs with a fixed factor \(t>1\) (for example multiplying the levels of the inputs with 2), the production function can be written as:

\begin{equation}
f(K,L) = T(tK)^{\alpha_K}(tL)^{\alpha_L} = t^{\alpha_K + \alpha_L}TK^{\alpha_K}L^{\alpha_L}
\end{equation}

Multiplying the inputs with a factor \(t\), therefore leads to a multiplication of production of \(t^{\alpha_K + \alpha_L}\).\footnote{We therefore say that the production function is homogenous of degree \(\alpha_K + \alpha_L\).} When \(\alpha_K + \alpha_L <1\), there will be decreasing returns to scale in production as a multiplication of the inputs with a factor \(t\), leads to a multiplication of production less than \(t\). When \(\alpha_K + \alpha_L <1\), there is constant returns to scale in production and multiplication of the inputs with \(t\) leads to multiplication of production levels with \(t\). When \(\alpha_K + \alpha_L >1\), there will be increasing returns to scale in production as a multiplication of the inputs with a factor \(t\), leads to a multiplication of production with more than \(t\). Again the input elasticities play a key role here.

\hypertarget{the-cost-function}{%
\subsection{The cost function}\label{the-cost-function}}

The next step is to specify the cost function of the firm and derive the optimal cost function. We do this, because often only the costs of the firm can be observed (but not the optimal inputs). Furthermore, the cost optimal cost function plays a key role for the market behaviour of the firm.

It is assumed to be linear in expenditures on the inputs (for our case labour and capital): \(c(K,L)=rK+\omega L\). This is a reasonable assumption unless the firm can negotiate lower prices for higher amounts of the inputs. In empirical work researchers sometimes specify input prices as a function of the total market demand for these inputs (see Pillai (2015)). Furthermore, it is assumed that costs related to production which fall outside the firm are not included. If the firm does take these costs into account, an additional term related to the target quantity can be added to the cost function and the choice of optimal target quantity can be included in the analysis.

The Lagrangian function that describes the constrained optimisation problem of the firm as an unconstrained optimisation problem can be written as:

\begin{equation}
H = rK+\omega L + \lambda (Q - TK^{\alpha_K}L^{\alpha_L})
\end{equation}

The three first-order conditions are given by:

\begin{align}
\frac{\partial H}{\partial \lambda} &=Q - TK^{\alpha_K}L^{\alpha_L} = 0,\\
\frac{\partial H}{\partial K} &= r - \lambda\frac{\partial \alpha_K}{\partial K}TK^{\alpha_K}L^{\alpha_L}= 0,\\
\frac{\partial H}{\partial L} &= \omega - \lambda\frac{\alpha_L}{\partial L}TK^{\alpha_K}L^{\alpha_L}= 0.\\
\end{align}

The first condition ensures that the production target is reached. The second condition equates the marginal costs of capital to the marginal benefits of capital. The third condition equates the marginal costs of labour to the marginal benefits of labour. In Appendix \ref{appderivation} we show that the optimal capital and labour are given by:

\begin{align}
K^\ast &= \frac{Q}{T}^{\frac{1}{\alpha_K+\alpha_L}}\frac{\omega}{r}^{\frac{\alpha_L}{\alpha_K+\alpha_L}}\frac{\alpha_K}{\alpha_L}^{\frac{\alpha_L}{\alpha_K+\alpha_L}},\\
L^\ast &= \frac{Q}{T}^{\frac{1}{\alpha_K+\alpha_L}}\frac{r}{\omega}^{\frac{\alpha_K}{\alpha_K+\alpha_L}}\frac{\alpha_L}{\alpha_K}^{\frac{\alpha_K}{\alpha_K+\alpha_L}},\\
\end{align}

It might be useful for you to program these expressions in order to see how the target quantity, the technology parameters, the input prices and the input elasticities relate to the optimally chosen inputs. First, the optimal inputs are non-linear in the target quantity and the shape depends on the sum of the input elasticities. With increasing returns in production, the optimal inputs are convex in the target quantity. With constant returns, the optimal inputs are linear in the target quantity and with decreasing returns in production, the inputs are concave in the target quantity.

Second, the optimal inputs depend on the relative input prices, decrease in the own input price and increase in the input price of the other input. For the optimal choice of capital--- \(\frac{\alpha_L}{\alpha_K+\alpha_L}\) is the elasticity with respect to the input price \(r\): when the interest rate increases with 1\% the optimal capital will decrease with \(\frac{\alpha_L}{\alpha_K+\alpha_L}\)\%. Furthermore, \(\frac{\alpha_L}{\alpha_K+\alpha_L}>0\)is the elasticity of optimal capital with respect to labour prices. When labour prices are interpreted as wages this gives useful insights in how wage changes impact input substitution. Therefore, there is an important one-to-one relationship between the production elasticities and the elasticity of input prices.

Third, when technology improves and \(T\) increases, the optimal inputs decrease as production per unit of input is higher. Suppose production becomes \(z\) times more effective due to an increase in \(T\) from \(T\) to \(zT\). Optimal inputs then change with a factor \(\left(\frac{1}{z}\right)^{\frac{1}{\alpha_K+ \alpha_L}}\). For example, when production becomes \(50\)\% more effective due to newly developed robot technology, and \(\alpha_K+\alpha_L=1\), this implies that the new inputs are \(\left(\frac{1}{1.5}\right)^{\frac{1}{1}} = \frac{2}{3}\) of the old inputs. Robot technology therefore is expected to decrease both capital and labour.

Fourth, the optimal inputs relate in a complicated way to the input elasticities via the multiplication factor. For constant returns, \(\alpha_K+\alpha_L = 1\) the expressions simplify and are given by:

\begin{align}
K^\ast &= \frac{Q}{T}\frac{\omega}{r}^{\alpha_L}\frac{\alpha_K}{\alpha_L}^{\alpha_L},\\
L^\ast &= \frac{Q}{T}\frac{r}{\omega}^{\alpha_K}\frac{\alpha_L}{\alpha_K}^{\alpha_K},\\
\end{align}

Substituting the optimal inputs in the cost function gives the optimal costs for the firm as a function of the technology parameter, the production elasticities and the input prices:

\begin{equation}
C(K^\ast, L^\ast) = rK^\ast + \omega L^\ast = \left(\frac{Q}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}} \omega^{\frac{\alpha_L}{\alpha_K + \alpha_L}} r^{\frac{\alpha_K}{\alpha_K + \alpha_L}}\left(\left(\frac{\alpha_K}{\alpha_L}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}} + (\left(\frac{\alpha_L}{\alpha_K}\right)^{\frac{\alpha_K}{\alpha_K + \alpha_L}} \right). 
\end{equation}

Before moving on it is useful to draw some conclusions from this mathematical expression of the cost function.

First, the expression depends on the target quantity in potential non-linear ways. The shape of the costs function with respect to the target quantity is governed by \(\frac{1}{\alpha_K+ \alpha_L}\) and therefore depends on the sum of the input elasticities. For increasing returns in production, the sum of the input elasticities is larger than 1 and the costs are concave in the target quantity. The costs are convex in the target quantity for decreasing returns in production. The special case of constant returns in production, \(\alpha_K+ \alpha_L = 1\), gives a linear cost function in the target quantity. The sum of the input elasticities of the production function therefore determine the shape of the cost function with respect to the quantity level.

Second, the derived cost function shows an inverse relation with the technology parameter \(T\). The higher the level of production technology, the lower the (marginal) costs of production. The downward impact of technology improvements on costs is stronger when the sum of the parameters \(\alpha_K+ \alpha_L\). The elasticity for this parameter is given by \(\frac{-1}{\alpha_K+ \alpha_L}\), showing that for a \(1\)\% increase in the technology parameter, the costs decrease with \(\frac{1}{\alpha_K+ \alpha_L}\).

Third, the cost function depends in a non-linear way on the input prices \(r\) and \(\omega\). When wages are higher, costs of the firm will increase where the strength of the increase depends on the relative size of the technology parameters \(\alpha_K\) and \(\alpha_L\). For example, the elasticity of costs with respect to wages is given by \(\frac{\alpha_L}{\alpha_K + \alpha_L}\): for a 1\% increase in the wages, costs of the firm increase with \(\frac{\alpha_L}{\alpha_K + \alpha_L}\)\%. For capital, a \(1\)\% increase in the capital price leads to a \(\frac{\alpha_K}{\alpha_K + \alpha_L}\)\% increase in the costs of the firm.

Fourth, the technology parameters \(\alpha_K\) and \(\alpha_L\) enter the cost function in a complicated non-linear way in the last term of the cost function. When researchers log-linearize the cost function this part ends up in the regression constant.

Fifth, from the cost function we can derive the average costs and the marginal costs:

\begin{align}
AC &= \frac{C(K^\ast, L^\ast)}{Q} =\left(\frac{1}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}}Q^{\frac{1- \alpha_K - \alpha_L}{\alpha_K + \alpha_L}} \omega^{\frac{\alpha_L}{\alpha_K + \alpha_L}} r^{\frac{\alpha_K}{\alpha_K + \alpha_L}}\left(\left(\frac{\alpha_K}{\alpha_L}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}} + (\left(\frac{\alpha_L}{\alpha_K}\right)^{\frac{\alpha_K}{\alpha_K + \alpha_L}} \right), \\
MC &= \frac{\partial C(K^\ast, L^\ast)}{\partial Q} = \frac{1}{\alpha_K + \alpha_L}\left(\frac{1}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}}Q^{\frac{1- \alpha_K - \alpha_L}{\alpha_K + \alpha_L}} \omega^{\frac{\alpha_L}{\alpha_K + \alpha_L}} r^{\frac{\alpha_K}{\alpha_K + \alpha_L}}\left(\left(\frac{\alpha_K}{\alpha_L}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}} + (\left(\frac{\alpha_L}{\alpha_K}\right)^{\frac{\alpha_K}{\alpha_K + \alpha_L}} \right)
\end{align}

Marginal and average costs are independent of the target quantity when there are constant returns to scale. For increasing returns, the average costs decrease in the target quantity and for decreasing returns average costs increase. For constant returns to scale the average and marginal costs curves are equal. This shows again the important role of the input elasticities.

At the end of this sub-section it is useful to discuss the producer results of pillar (1.) in relation to the third pillar of economic analysis: empiricism. There are two ways to investigate the structural parameters of the cost function. First, empirical economists have employed regression estimates of the Cobb-Douglas production function using realized quantities of firms as a function of the firms' inputs. This can be operationalized by log-linearizing the production function:

\begin{equation}
\ln Q = \ln T + \alpha_K \ln K + \alpha_L \ln L + \epsilon
\end{equation}

Such a regression requires firm data on production quantities and levels of inputs. Statistical tests can be performed in order see whether the cost function is significantly different from linear (in quantity \(Q\)). These tests use \(\alpha_K + \alpha_L=1\) as the null hypothesis.

The second strategy is to run regressions directly on the cost functions. For example, Pillai (2015)) estimates the average production cost function for the market for solar panels using average cost levels and data on input prices, realized quantities and input levels. Log-linearizing the average cost function gives the following equation:

\begin{align}
\ln AC =& -\frac{1}{\alpha_K + \alpha_L} + \frac{1 - \alpha_K - \alpha_L}{\alpha_K + \alpha_L} \ln Q + \frac{\alpha_L}{\alpha_K + \alpha_L }\ln \omega +  \frac{\alpha_K}{\alpha_K + \alpha_L} \ln r + \\
&\left( \left( \frac{\alpha_K}{\alpha_L} \right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}} + \left(\frac{\alpha_L}{\alpha_K} \right)^\frac{\alpha_K}{\alpha_K + \alpha_L} \right)
\end{align}

The first and the last term of this equation can be combined in a regression constant.

The regression equation that is estimated is then given by:

\begin{equation}
\ln AC = \beta_0 + \beta_1 \ln Q + \beta_2 \ln \omega + \beta_3 \ln r + \epsilon.
\end{equation}

Hypothesis testing can be performed to see whether parameters are in accordance with the Cobb-Douglas specification. For example, theory dictates that for Cobb-Douglas to hold we must have \(\beta_2 + \beta_3 = 1\) and \(\beta_1 + \beta_2 + \beta_3 = 0\). If these test fail, then the Cobb-Douglas production function is not a good approximation. Increasing returns imply that \(\beta_1<0\), decreasing returns imply \(\beta_1 >0\) and constant returns in production imply \(\beta_1 = 0\).

When Cobb-Douglas is appropriate, we can retrieve the structural parameters of the production function from the regression estimates:

\begin{align}
\alpha_K +\alpha_L &= \frac{1 - \beta_1}{\beta_1},\\
\alpha_L &= \beta_2 \frac{1 - \beta_1}{\beta_1},\\
\alpha_K &= \beta_3 \frac{1 - \beta_1}{\beta_1},\\
T &= \frac{1 - \beta_1}{\beta_1}\left[\left(\frac{\beta_3}{\beta_2} \right)^{\beta_2} +\left(\frac{\beta_2}{\beta_3} \right)^{\beta_3} - \beta_0\right]
\end{align}

\hypertarget{special-case-constant-average-and-marginal-costs}{%
\subsection{Special case: constant average and marginal costs}\label{special-case-constant-average-and-marginal-costs}}

The general cost function that is derived above allows us to derive simpler cases by making additional assumptions. When regression shows that it is plausible to assume that there are constant returns to scale in production, \(\alpha_K + \alpha_L =1\), and the cost function simplifies and can be written as:

\begin{equation}
C(K^\ast, L^\ast) = \frac{1}{T}Qr^{\alpha_K}\omega^{1-\alpha_K}.
\end{equation}

\emph{Average costs} are then given by \(\frac{C(K^\ast,L^\ast)}{Q}\) and are constant given the assumptions made in this subsection. \emph{Marginal costs} are given by the first derivative of the optimal costs with respect to the target quantity and are equal to the average costs:

\begin{equation}
MC = AC = \frac{\partial C(K^\ast,L^\ast)}{\partial Q} = \frac{C(K^\ast,L^\ast)}{Q}  = \frac{1}{T}r^{\alpha_K}\omega^{1-\alpha_K}.
\end{equation}

This simple expression accounts for the substitution of inputs of a firm and allows us to analyse exogenous changes in marginal costs due to changes in technology (via \(T\)), strength of substitution between inputs (via \(\alpha_K\)) and changing input prices (via \(r\) and \(\omega\)). Because---given the assumptions---optimal costs are linear in the target quantity, marginal costs for the firm are constant per unit of production. However, this does not imply that marginal costs cannot be different for different firms as the input elasticities and the technology parameter might be different for different firms.

\hypertarget{empirical-examples}{%
\subsection{Empirical examples}\label{empirical-examples}}

This section discusses two examples. First, we discuss one of the key sustainability innovations in the energy market: the production of solar panels. A second example is given for the production function of housing. Both examples are related to the key sustainability issues of climate change and spatial planning.

When there are exogenous improvements in production technology over time it is expected that the technology parameter \(T\) will increase over time. Indeed, such a result has been found in the literature on solar panels. Pillai (2015) finds an average 21\% annual reduction in average production costs over the time period 2005--2012.\footnote{They also show that average costs are decreasing in quantity levels.}

For solar panels, it is expected that capital, materials and labour are the main inputs. According to Pillai (2015) (p.289), capital and materials are the main inputs and firms substitute these inputs in order to optimize production. The input prices for materials and capital therefore make an important contribution on the average and marginal costs of solar panels (see Pillai (2015), Table 4). For example, the decreasing input price for polysilicon material inputs contributes to 7\% of the cost efficiency improvement over the time period 2005--2012. This result shows that input prices can be key drivers for average cost reductions of sustainable energy solutions and that the discussion on producer behaviour is related to recent research on sustainability innovations.

The second example that we discuss is related to sustainable spatial planning and more specifically to the production function of housing which is studied by Epple, Gordon, and Sieg (2010). They estimated the production function for new housing for the greater metropolitan area of Pittsburgh. Their Cobb-Douglas function includes land inputs \(R\) and non-land inputs \(M\) such as building materials and capital (note that in their paper they use \(L\) for land inputs, but we already used \(L\) for labour, so use \(R\) instead). Firms can for example choose to substitute land and materials by building houses with multiple floors. \$epple2010new estimate the following production function:\footnote{It is not entirely clear from their paper what the dimension of the production function is and what the constant 1.38 means.}

\begin{equation}
f(R,M) = 1.38 R^{0.144} M^{0.856}.
\end{equation}

The elasticity of production with respect to land is therefore given by \(0.144\) which means that for a 1\% increase in the amount of land inputs, housing production increases with \(0.144\)\%. Because the elasticities add up to 1, there are constant returns to scale. The marginal cost function and the average cost function for the metropolitan Pittsburgh area are constant and can be derived as a function of the input prices for land (\(p_R\)) and non-land inputs (\(p_M\)):

\begin{equation}
MC = AC = \frac{\partial C(K^\ast,L^\ast)}{\partial Q} = \frac{C(K^\ast,L^\ast)}{Q}  = \frac{1}{1.38}p_R^{0.144}p_M^{1- 0.144}.
\end{equation}

Usually the price of land is different for different locations resulting in marginal costs of building houses which are potentially different over space. Another recent study by Combes, Duranton, and Gobillon (2021) estimate the production function for newly built single-family homes for France:

\begin{quote}
``or newly built single-family homes in France, the production function for housing is close to constant returns and is well, though not perfectly, approximated by a Cobb-Douglas function with a capital elasticity of 0.65.'' (Combes et al.~2021, abstract)
\end{quote}

They show that the Cobb-Douglas production function gives a reasonable (but not perfect) approximation and that the capital elasticity is estimated at 0.65. The elasticity estimate for land is around 0.35, as the paper shows constant returns is a reasonable approximation. This elasticity estimate for land is higher than the value found for Pittsburgh. These two examples show how closely linked recent empirical results are to the theory of production functions and cost functions.

The theory of production functions can also be linked to recent developments on circular economy. In the context of housing production there can be shortage of building materials. It is expected that this will impact the input prices for building materials and thereby leads to input substitution and higher marginal production costs of housing. When the housing market works in a more circular way, materials of old houses will be re-used and re-offered on the market. This implies that the total supply of housing production inputs will be higher when competitive circular material use is possible.

\hypertarget{from-firm-costs-to-market-inverse-supply-and-supply}{%
\subsection{From firm costs to market inverse supply and supply}\label{from-firm-costs-to-market-inverse-supply-and-supply}}

The next step is to relate the firm marginal costs to the market inverse supply curve. Again, the aggregation of marginal costs curves into an inverse supply curve is not trivial, but the intuition is similar as with consumer analysis. Aggregation can be done by sorting heterogeneous firms according to their marginal costs per unit of the market good. This gives the market inverse supply curve \(s(Q)\) for the market good which express what the price should be for which an amount \(Q\) of the market good is supplied. Marginal costs for the firm can be constant per unit of production, but this does not imply that there is no heterogeneity in marginal costs for different firms. For example, the firms might have similar input prices, but different input elasticities and technology parameters. The inverse of the inverse supply curve is the supply curve \(Q^S(p)\), which shows how much is supplied at the market for a given level of the market price \(p\). Empirical economists regularly estimate the price elasticity of supply which is defined as:

\begin{equation}
\kappa = \frac{\partial Q^S}{\partial p}\frac{p}{Q^{S(p)}}.
\end{equation}

For a 1\% increase in the market price, the market supply will change with \(\kappa\)\%. When the slope of the supply function \(\frac{\partial Q^S}{\partial p}\) is very large, the supply function is steeply increasing in the price and supply is elastic. Supply then responds strongly to price changes. An empirical example of supply elasticities can be given using the work of Green, Malpezzi, and Mayo (2005) on housing supply in the US. The table in Figure \ref{fig:green} is from their paper and shows price elasticities of supply in the range of \(-0.30\) (Miami) up to \(29.9\) (Dallas). These estimates are therefore widely varying over space.

Not all estimates are significant (confidence intervals are not reported). For Pittsburgh, the elasticity is estimated at 1.43 and significant at the 1\% level. This implies that a 1\% increase in the price of housing will lead to 1.43\% increase in the housing supply in Pittsburgh. For Atlanta, housing supply is more elastic: a 1\% increase in the price of housing leads to an estimated 21.6\% increase in housing supply. Later we will see how elasticities are related to policies related to prices.

\begin{figure}

{\centering \includegraphics[width=400px]{./figures/green} 

}

\caption{Price elasticities of housing supply }\label{fig:green}
\end{figure}

\hypertarget{producer-surplus}{%
\subsection{Producer surplus}\label{producer-surplus}}

For the analysis of producer surplus, it is assumed that all firms ask the same price for their product.\footnote{Here it becomes a bit tricky when we have heterogeneous firms as there is no real price competition going on despite the fact that low marginal costs firms can offer lower prices than high marginal cost firms. We deliberately ignore profit maximizing behaviour, imperfect substitutability between products among other issues. In theory, free entry and competition would lead to marginal costs equal to the \emph{marginal costs of the firm with the lowest costs} and profits equal to 0. Despite all these issues, the model might serve as a reasonable approximation for medium term analysis of economic surplus.} Total revenues in the market are then given by \(p^\ast Q^\ast\). Given these assumptions, there will be firms in the market who make a profit. Low-marginal cost firms do not compete other firms out of the market by offering lower prices and firms do not propose customer specific prices. In the absence of fixed costs, total costs in the market are given by the area under the supply curve. Producer surplus can be mathematically described as follows:

\begin{equation}
PS  = p^\ast Q^\ast - \int_0^{Q^\ast} s(Q)dQ.
\end{equation}

Using information on the marginal cost curve we can derive producer surplus using integral calculus. Suppose the inverse supply curve is independent of the market quantity and given by:

\begin{equation}
s(Q) = \frac{B}{T}r^{\alpha_K}\omega^{1-\alpha_L}.
\end{equation}

Here, \(B\) is a proportional constant that captures the aggregation of firms.

Substituting in the expression for producer surplus then results in:

\begin{align}
PS  &= p^\ast Q^\ast - \int_0^{Q^\ast} \frac{B}{T}r^{\alpha_K}\omega^{1-\alpha_L} dQ =  p^\ast Q\ast \left[Q \frac{B}{T}r^{\alpha_K}\omega^{1-\alpha_L} + z\right]^{Q^\ast}_0 \\
&= p^\ast Q^\ast - Q^\ast \frac{B}{T}r^{\alpha_K}\omega^{1-\alpha_L}.
\end{align}

This requires input on the equilibrium price and quantity, the technology parameter, the input prices and the production elasticity of substitution.

\hypertarget{sec:econsurplus}{%
\section{Analysis of economic surplus}\label{sec:econsurplus}}

\hypertarget{introduction-2}{%
\subsection{Introduction}\label{introduction-2}}

Now that we have completed the analysis of the choice behaviour of consumers and consumer value in markets (section 2), and the choice behaviour of producers and producer value in markets (section 3), we continue our analysis on the second pillar of economics: the analysis of equilibrium. The assumption of equilibrium ensures that the interaction of consumers and producers in the market is properly accounted for and gives new information for the researcher. Not all consumers will buy the market good and not all potential firms will participate in selling goods on the market. Some might be interested but will not receive sufficient gains from trade.

\hypertarget{equilibrium}{%
\subsection{Equilibrium}\label{equilibrium}}

Demand and supply interact which may lead to equilibrium in markets. The equilibrium intuition is governed by the following two equalities:

\begin{equation}
p(Q^\ast)= p^\ast=s(Q^\ast).
\end{equation}

The first equality \(p(Q^\ast)= p^\ast\) states that the marginal benefits of the equilibrium consumer at \(Q^\ast\) are equal to the equilibrium price. Customers at the left of \(Q^\ast\) will buy the good because their marginal WTP is higher than the equilibrium price. Customers at the right side of \(Q^\ast\) will not participate in the market, because their marginal WTP is lower than the equilibrium price.

The second equality \(p^\ast=s(Q^\ast)\) states that the equilibrium price should be equal to the marginal costs of production for the equilibrium quantity produced. Firms at the left of \(Q^\ast\) have lower marginal costs than the equilibrium price and will therefore continue to sell their goods. Firms at the right of \(Q^\ast\) have higher marginal costs than the equilibrium price and will therefore choose not to produce the good.

\hypertarget{calibration-of-equilibrium}{%
\subsection{Calibration of equilibrium}\label{calibration-of-equilibrium}}

How to fit the conceptual model to a real market in order to do justice to the third pillar: empiricism? That is the challenge of calibration in this section. Using the equilibrium equations, it is possible to recover two unknown parameters of the demand and supply function. If one also has information about the equilibrium elasticities, it is possible to calibrate four model parameters. This is the reason that the estimation of equilibrium elasticities is an important task of empirical economists. In the previous section we have seen estimates for these elasticities for different kinds of market goods. The tables below can be used for calibration of the (inverse) market demand and supply curves given this knowledge on the estimated (equilibrium) elasticities.

\begin{table}

\caption{\label{tab:demand}Demand and supply elasticities and model parameters}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lllll}
\toprule
Case description & Inverse demand & Implied demand curve and equilibrium price                     elasticity of demand & Inverse supply & Implied supply curve and equilibrium                       price elasticity of supply\\
\midrule
Case I: Exponential inverse demand and exponential inverse supply & $p(Q)=Ae^{-\alpha Q}$

Parameter assumptions:
$A>0$, $\alpha>0$.

Regression (log-linear):

$\ln p = \ln A - \alpha Q + \epsilon$ & $Q^D (p)=\frac{1}{\alpha} \ln\left[\frac{A}{P}\right]$

Equilibrium elasticity:

$\epsilon^\ast=\frac{\partial Q^\alpha}{\partial p}\frac{p^\ast}{Q^\ast}=-\frac{1}{\alpha}\frac{1}{A^\ast}$ & $s(Q)=Be^{bQ}$

Parameter assumptions:
$B>0$,$B<A$.

Regression (log-linear):

$\ln MC = \ln B  +b Q + \epsilon$ & $Q^S(p)=\frac{1}{b}\ln\left[\frac{p}{B}\right]$

Equilibrium elasticity:

$\kappa^\ast = \frac{\partial Q^S}{\partial p}\frac{p^\ast}{Q^\ast} = \frac{1}{b}\frac{1}{Q^\ast}$\\
Case II: Constant elasticity inverse demand and inverse supply & $p(Q) = AQ^{-\alpha}$

Parameter assumptions:
$A>0$, $\alpha>0$.

Regression (log-log):

$\ln p = \ln A - \alpha \ln Q + \epsilon$ & $Q^D (p)=\frac{P}{A}^{-\frac{1}{\alpha}}$

Equilibrium elasticity:

$\epsilon^\ast=\frac{\partial Q^\alpha}{\partial p}\frac{p^\ast}{Q^\ast}=-\frac{1}{\alpha}$ & $s(Q)=BQ^b$

Parameter assumptions:
$B>0$

Regression (log-log):

$\ln MC = \ln B + b \ln Q + \epsilon$ & $Q^S(p)=\left(\frac{p}{B}\right)^{\frac{1}{b}}$

Equilibrium elasticity:

$\kappa^\ast = \frac{\partial Q^S}{\partial p}\frac{p^\ast}{Q^\ast} = \frac{1}{b}$\\
Case III: Logged inverse demand and inverse supply & $p(Q)=A-\alpha \ln Q$

Parameter assumptions:
$A>0$, $\alpha>0$.

Regression (linear-log):

$p=A- \alpha \ln Q + \epsilon$ & $Q^D (p)=e^{\frac{A-p}{\alpha}}$

Equilibrium elasticity:

$\epsilon^\ast=\frac{\partial Q^\alpha}{\partial p}\frac{p^\ast}{Q^\ast}=-\frac{1}{\alpha}p^\ast$ & $s(Q) = B + b \ln Q$

Parameter assumptions:
$B>0$

Regression (linear-log):

$MC = B + b \ln Q + \epsilon$ & $Q^S(p)=e^{\frac{p-B}{b}}$

Equilibrium elasticity:

$\kappa^\ast = \frac{\partial Q^S}{\partial p}\frac{p^\ast}{Q^\ast} = \frac{1}{b}p^\ast$\\
Case IV: linear inverse demand and inverse supply & $p(Q)=A- \alpha Q$

Regression (linear-linear):

$p=A-\alpha Q+ \epsilon$ & $Q^D (p)=\frac{A-p}{\alpha}$

Equilibrium elasticity:

$\epsilon^\ast=\frac{\partial Q^\alpha}{\partial p}\frac{p^\ast}{Q^\ast}=-\frac{1}{\alpha}\frac{p^\ast}{Q^\ast}$ & $s(Q)=B+bQ$

Regression (linear-linear):

$MC=B+bQ+\epsilon$ & $Q^S(p)=\frac{p-B}{b}$

Equilibrium elasticity:

$\kappa^\ast = \frac{\partial Q^S}{\partial p}\frac{p^\ast}{Q^\ast} = \frac{1}{b}\frac{p^\ast}{Q^\ast}$\\
\bottomrule
\end{tabular}
\end{table}

Table \ref{tab:calibrated} gives the calibrated parameters for the inverse demand and inverse supply functions based on information on the equilibrium elasticities and the equilibrium price and quantity. Equilibrium elasticities can be obtained from the literature or from an empirical analysis of demand and supply data. Equilibrium prices and quantities can be observed for the market that you are interested in.

\begin{table}

\caption{\label{tab:calibrated}Calibrated model parameters for commonly used inverse demand and inverse supply functions}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{llllll}
\toprule
Case description & Inverse demand & $A$ & $\alpha$ and $1/\alpha$ & $B$ & $b$ and $1/b$\\
\midrule
Case I: Exponential inverse demand and exponential inverse supply & $p(Q)=Ae^{-\alpha Q}$

Parameter assumptions:
$A>0$, $\alpha>0$.

Regression (log-linear):

$\ln p = \ln A - \alpha Q + \epsilon$ & $A = p^\ast e^{\alpha Q^\ast}$ & $\alpha = - \frac{1}{\epsilon^\ast} \frac{1}{Q^\ast}$
$\frac{1}{\alpha} = -\epsilon^\ast Q^\ast$ & $B = p^\ast e^{- b Q^\ast}$ & $b = - \frac{1}{\kappa^\ast} \frac{1}{Q^\ast}$
$\frac{1}{b} = -\kappa^\ast Q^\ast$\\
Case II: Constant elasticity inverse demand and inverse supply & $p(Q) = AQ^{-\alpha}$

Parameter assumptions:
$A>0$, $\alpha>0$.

Regression (log-log):

$\ln p = \ln A - \alpha \ln Q + \epsilon$ & $A = p^\ast Q^{\ast^\alpha}$ & $\alpha = - \frac{1}{\epsilon^\ast}$
$\frac{1}{\alpha} = -\epsilon^\ast$ & $B = p^\ast Q^{\ast -b}$ & $b = - \frac{1}{\kappa^\ast}$
$\frac{1}{\alpha} = -\kappa^\ast$\\
Case III: Logged inverse demand and inverse supply & $p(Q)=A-\alpha \ln Q$

Parameter assumptions:
$A>0$, $\alpha>0$.

Regression (linear-log):

$p=A- \alpha \ln Q + \epsilon$ & $A = p^\ast + \alpha \ln Q^\ast$ & $\alpha = - \frac{1}{\epsilon^\ast}p^\ast$ & $B = p^\ast - b \ln Q^\ast$ & $b = - \frac{1}{\kappa^\ast}p^\ast$\\
Case IV: linear inverse demand and inverse supply & $p(Q)=A- \alpha Q$

Regression (linear-linear):

$p=A-\alpha Q+ \epsilon$ & $A = p^\ast + \alpha Q^\ast$ & $\alpha = - \frac{1}{\epsilon^\ast}\frac{p^\ast}{Q^\ast}$ & $B = p^\ast - b Q^\ast$ & $b = - \frac{1}{\kappa^\ast}\frac{p^\ast}{Q^\ast}$\\
\bottomrule
\end{tabular}
\end{table}

The parameters \(A\) and \(B\) can be calibrated using the identities for the inverse demand and inverse supply at the equilibrium. The parameters \(a\) and \(b\) can be calibrated using the equations we have for the price elasticities for specified demand and supply functions (see Table \ref{tab:demand}).
To illustrate how a market can be calibrated using elasticities from the literature we provide an example of case I: exponential inverse demand and inverse supply functions. Table \ref{tab:calibration} shows information we have about the equilibrium elasticities for vegan nuggets.

\begin{table}

\caption{\label{tab:calibration}Input for the calibration (numbers are illustrative and not from empirical studies)}
\centering
\begin{tabular}[t]{ll}
\toprule
Variable explanation & Value\\
\midrule
Price elasticity of demand $\epsilon^\ast$ & $-0.7$\\
Price elasticity of supply $\kappa^\ast$ & $1.1$\\
Equilibrium price per piece $p^\ast$ & $2.9$ euro\\
Equilibrium quantity per year $Q^\ast$ & $200,000$ pieces per year\\
\bottomrule
\end{tabular}
\end{table}

Using the first row of Table \ref{tab:demand} we then calibrate the parameters of the inverse demand and the inverse supply function. These are given in Table \ref{tab:modelparameters}:

\begin{table}

\caption{\label{tab:modelparameters}Calibrated model parameters}
\centering
\begin{tabular}[t]{ll}
\toprule
Variable explanation & Value\\
\midrule
Parameter of the inverse demand function $\alpha$ & $\alpha = \frac{1}{\epsilon^\ast}\frac{1}{Q^\ast} = \frac{1}{0.8}\frac{1}{200,000} = \frac{1}{160,000}$\\
Parameter of the inverse demand function $A$

(maximum MWTP in the sample) & $A = p^\ast e^{\alpha Q^\ast} = 2.9 e^{\frac{200,000}{160,000}} \approx 10.12 \text{ euro}$\\
Parameter of the inverse supply function $b$ & $b = \frac{1}{\kappa^\ast}\frac{1}{Q^{\ast}} = \frac{1}{1.1}\frac{1}{200,000} = \frac{1}{220,000}$\\
Parameter of the inverse supply function $B$

(minimum marginal costs in the sample) & $B = p^\ast e^{b Q^\ast} = 2.9 e^{-\frac{200,000}{220,000}} \approx 1.17 \text{ euro}$\\
\bottomrule
\end{tabular}
\end{table}

Using the calibrated inverse demand and inverse supply function one can derive the consumer benefits, the producer costs and economic surplus as we will see in the next section. The important insight here is that model parameters can be calibrated using empirical observations of equilibrium prices and quantity and empirical estimates of the equilibrium elasticities. This provides the bridge between theoretical and conceptual models and empirical observations.

\hypertarget{analysis-of-economic-surplus}{%
\subsection{Analysis of economic surplus}\label{analysis-of-economic-surplus}}

Economic surplus (ES) generated in the market is given by the sum of consumer surplus and producer surplus. It seeks to combine pillar (iii) with a particular view on normativity (pillar (iv)): for the calculation of economic surplus it is assumed that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The choices of market actors express their value attached to the market products;
\item
  Choice value can be fully counted as economic surplus because the choices of market actors are `good'.
\end{enumerate}

We come back to these assumptions in Chapters @ref(erroreconsurplus and \ref{moral}.

Mathematically, economic surplus is defined by:

\begin{align}
ES = CS + PS &= \underbrace{\int_0^{Q^\ast} p(Q)dQ - p^\ast Q^\ast}_\text{Consumer surplus} + \underbrace{p^\ast Q^\ast - \int^{Q^\ast}_0 s(Q)dQ}_\text{Producer surplus}\\
&= \underbrace{\int_0^{Q^\ast} p(Q)dQ}_\text{Total consumer benefits} - \underbrace{\int^{Q^\ast}_0 s(Q)dQ}_\text{Total consumer costs}.
\end{align}

The first line shows the first way to express economic surplus: it is the sum of consumer surplus and producer surplus. Consumer surplus is the area under the inverse demand curve minus the consumer spending on the market good. Producer surplus is the producer revenues minus the total producer costs. Producer costs are equal to the area under the inverse supply curve (because it is assumed that fixed costs are 0).

The advantage of this way of expressing economic surplus is that it is clear how the surplus is divided between consumers and producers. Sometimes national governments are only interested in the consumer surplus for a particular product for inhabitants of their own country and not in the producer surplus of firms in other countries. Therefore, it might be useful to report both \(CS\) and \(PS\) and to state where the benefits fall.

There is a second way to express economic surplus, which is given by the second line of the equation. Because payments of consumers are revenues for producers, the second and the third term cancel out resulting in an equation which states that consumer surplus is equal to the consumer benefits minus the producer costs. Interestingly, one does not have to observe the equilibrium prices in order to calculate economic surplus as long as one knows the inverse demand and inverse supply curves and the equilibrium quantity.

At this point it is also useful to investigate the point for which economic surplus is optimal. Differentiating economic surplus with respect to the equilibrium quantity gives:

\begin{equation}
\frac{\partial ES}{\partial Q^\ast} = p(Q^\ast) - s(Q^\ast) = 0
\end{equation}

This is equal to the equilibrium condition which states that marginal benefits are equal to marginal costs. The expression shows that economic surplus in the market is optimal when marginal consumer benefits are equal to marginal costs in equilibrium. In the absence of other costs and considerations, one can conclude with Adam Smith from this equation that free trade will result in the maximum economic surplus. However, one should keep in mind that together with the assumptions on rationality, equilibrium is imposed rather than empirically supported using data analysis and that particular pillar (iv) assumptions on normativity are made that might not hold in practice.

Integral calculus can be employed to derive economic surplus for particular specifications of the inverse demand and the inverse supply curve. We will illustrate this for an example. Suppose the inverse supply curve is exponentially increasing in quantity and given by:

\begin{equation}
s(Q) = B e^{bQ}, \quad B>0, \quad b >0.
\end{equation}

Furthermore, assume that the inverse demand curve is exponential and given by:

\begin{equation}
p(Q) = Ae^{-\alpha Q}, \quad A>0, \quad \alpha >0.
\end{equation}

Substitution in the expression for economic surplus gives:

\begin{align}
ES &= \underbrace{\int_0^{Q^\ast} Ae^{-\alpha Q} dQ}_\text{Total consumer benefits} - \underbrace{\int^{Q^\ast}_0  B e^{bQ}dQ}_\text{Total consumer costs}\\
&= \left[-\frac{1}{\alpha}Ae^{-\alpha Q} + z\right]^{Q^\ast}_0 -  \left[\frac{1}{b}B e^{bQ} + w\right]^{Q^\ast}_0 \\
&= -\frac{1}{\alpha}A\left(e^{-\alpha Q^\ast} - 1 \right) -\frac{1}{b}B\left(e^{b Q^\ast} - 1 \right),\\
&= \frac{1}{\alpha}A\left(1 - e^{-\alpha Q^\ast} \right) -\frac{1}{b}B\left(e^{b Q^\ast} - 1 \right).
\end{align}

This shows that economic surplus in the market can be written as a function of the observed equilibrium quantity and the structural parameters of the inverse demand and the inverse supply function. In the previous sub-section, we calibrated these structural parameters using information on equilibrium prices and equilibrium demand and the equilibrium demand and supply elasticities (see Table \ref{tab:modelparameters}). We can now use these parameters to derive the consumer benefits, the producer costs and the total economic surplus in the market for vegan nuggets. Table \ref{tab:econsurplusexample} provides an extension of Table \ref{tab:modelparameters} and shows the results. We leave it to the reader to develop examples for the other cases 2--4.

\begin{table}

\caption{\label{tab:econsurplusexample}Economic surplus (example)}
\centering
\begin{tabular}[t]{ll}
\toprule
Variable explanation & Value\\
\midrule
Consumer benefits (euros per year) & $\frac{1}{\alpha}A[1-e^{-\alpha Q^\ast}] = 160,000 \times 10.12 \times [1 - e^{\frac{200,000}{160,000}}]$

  $= 1,155,519 \text{ euros}$\\
Producer costs (euros per year) & $\frac{1}{b}B(e^{bQ^\ast} - 1) = 220,000 \times 1.17 \times (e^{\frac{200,000}{220,000}} + 1)$

  $380,956 \text{ eurors}$\\
Consumer surplus (euros per year) & $CS = CB - CC$

   $\frac{1}{\alpha}A[1-e^{-\alpha Q^\ast}] - p^\ast Q^\ast = 1,155,519 - 580,000$

   $= 575,519 \text{ euros}$\\
Producer surplus (euros per year) & $PS = PB - PC$

   $p^\ast Q^\ast - \frac{1}{b}B(e^{bQ^\ast} - 1) = 580,000 - 380,956$

   $= 199,044 \text{ euros}$\\
Economic surplus (euros per year) & $ES = CB - PC = 774,563 \text{ euros}$

   $= CS + PS= 774,563 \text{ euros}$\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{sec:extcosts}{%
\section{External costs and economic surplus}\label{sec:extcosts}}

\hypertarget{introduction-3}{%
\subsection{Introduction}\label{introduction-3}}

In the previous section we investigated mathematically the economic surplus that markets generate. This economic surplus is generated assuming free trade possibilities and equilibrium and is only related to the decisions of consumers and producers. It therefore does not account for potential other costs and benefits that fall outside the market and does not include other considerations related to rationality and normativity.

More specifically, and related to sustainability, a challenge for the model developed in the previous section can be environmental costs that fall outside of the market. Examples of negative \emph{external costs} of meat production/consumption for the environment are CO2 pollution, animal welfare, soil erosion, water pollution and air pollution (see Godfray et al. 2018). External costs provide euro values of these negative impacts in order to make the impacts comparable with the market surplus of consumers and producers.

Economists have argued that these costs should be added to economic surplus in order to give an honest account of the surplus that markets generate. This addition of \emph{external costs} entails a particular pillar (iv) assumption on responsibility which will be discussed in Chapter \ref{moral}. In order to account for external costs (\(EC\)), we add the term \(e(Q^\ast)\) to the economic surplus. Economic surplus is defined as:

\begin{align}
ES &= CS + PS - EC \\
&= \underbrace{\int_0^{Q^\ast} p(Q)dQ}_\text{Total consumer benefits} - \underbrace{\underbrace{\int^{Q^\ast}_0 s(Q)dQ}_\text{Total consumer costs} - \underbrace{e(Q^\ast)}_\text{Total external costs}}_\text{Total social costs}.
\end{align}

Besides this, we also define the marginal external costs \(MEC(Q)=\frac{\partial e(Q)}{\partial Q}\), which show---loosely speaking---with how much total external costs change for a change in the quantity. The total social costs are then defined as the producer costs and the external costs. The marginal social costs are given by the first derivative of the total social costs. This first derivative is equal to \(s(Q)+MEC(Q)\) which signifies that marginal external costs are economically speaking just another cost component above the producer costs. Again, we can investigate optimality of the extended economic surplus function by investigating the first derivative:

\begin{equation}
\frac{\partial ES}{\partial Q^\ast} = p(Q^\ast) - s(Q^\ast - MEC(Q^\ast) = 0.
\end{equation}

This condition is different from the condition in the previous section because of the added external costs. The condition shows that---given the model assumptions---optimal economic surplus cannot be reached without intervention in the market because the behaviour of consumers and producers leads to \(p(Q^\ast)-s(Q^\ast)=0\). When marginal external costs are positive, the derivative is negative, showing that improvements in economic surplus can be obtained when demand decreases. In short, this means that when positive (marginal) external costs are ignored by market actors the prices for the market goods are too low. When there are marginal external benefits, \(MEC(Q^\ast )<0\), market prices are too high.

\hypertarget{general-analysis-of-the-consumer-externality-tax}{%
\subsection{General analysis of the consumer externality tax}\label{general-analysis-of-the-consumer-externality-tax}}

One way to deal with environmental externalities is to set a consumer tax. Mathematical analysis can be employed to investigate this consumer tax. The regulator sets a tax \(\tau\) and this will result in a new equilibrium price \(p^{\ast R}\) and a new equilibrium demand \(Q^{\ast R}\). The tax will impact the equilibrium equation for consumers as now their marginal benefits should be higher or equal to the equilibrium price plus the tax. For the equilibrium consumer we have:

\begin{equation}
p(Q^{\ast R}) = p^{\ast R} + \tau.
\end{equation}

For the equilibrium producer, there is no change and the marginal costs should be equal to the equilibrium price:

\begin{equation}
s(Q^{\ast R}) = p^{\ast R}.
\end{equation}

The price of the producer can therefore be treated as given for the regulator. The regulator seeks to optimize welfare given the first constraint and keeping in mind that in the regulated equilibrium inverse supply is equal to the supply price. The Lagrangian is given by:\footnote{This methodology might look a bit extensive as one can directly obtain the result using the optimal welfare condition and plugging in the equilibrium condition. Nevertheless, it helps to move to more complicated analyses. See for example: Verhoef, Nijkamp, and Rietveld (1996).}

\begin{equation}
L = \underbrace{\int_0^{Q^\ast R} p(Q)dQ}_\text{Total consumer benefits} - \underbrace{\int^{Q^\ast R}_0 s(Q)dQ - e(Q^{\ast R})}_\text{Total social costs} + \underbrace{\lambda(p(Q^{\ast R}) - p^{\ast R} - \tau)}_\text{Equilibrium constraint}.
\end{equation}

The first-order conditions are given by:

\begin{align}
\frac{\partial L}{\partial Q^{\ast R}} &= p(Q^{\ast R}) - s(Q^{\ast R}) - MEC(Q^{\ast R}) + \lambda \frac{\partial p(Q)}{\partial Q} = 0 \\
\frac{\partial L}{\partial \lambda} &= p(Q^{\ast R}) - p^{\ast R} - \tau = 0 \\
\frac{\partial L}{\partial \tau} &= \lambda = 0
\end{align}

Substituting the second and third condition in the first condition as well as \(s(Q^{\ast R}) =p^{*R}\) gives:

\begin{equation}
p^{*R} - \tau - p^{*R} - MEC(Q^{* R}) + 0 \frac{\partial p(Q)}{\partial Q} = 0
\end{equation}

This results in the following expression for the consumer tax:

\begin{equation}
\tau = MEC(Q^{* R})
\end{equation}

The consumer tax should therefore be equal to the marginal external costs \emph{in the regulated equilibrium}. This is a well-known result in the field of public and environmental economics. The tax ensures that in equilibrium the optimality condition for maximizing economic surplus (including external costs) holds. In the absence of external costs, the tax will be \(0\).

When marginal external costs are constant per unit of consumption, the tax will be independent of the equilibrium quantity. Total external costs can then be defined as \(e(Q)=mQ\) and the consumer tax is given by:

\begin{equation}
\tau = m
\end{equation}

An example for meat production can be given based on the work of Van Drunen, Van Beukering, and Aiking (2010).

\begin{quote}
The total (editor: marginal) external costs for conventional pork are estimated to be at least \textbf{2.06 per kg} for an average consumer price of 6.69 (PVE, 2009), or 31\%. In this, animal welfare is the main factor, followed by biodiversity, animal disease and climate change.
\end{quote}

Van Drunen, Van Beukering, and Aiking (2010) estimate the marginal external costs at 2.06 euros per kilogram for conventional pork which is about 1/3 of the current equilibrium price. Interestingly, animal welfare is the largest component in the external costs. Given the analytical analysis this would result in a consumer tax of \(m=2.06\) euros per kilogram for conventional pork. From the analytical model we know this will improve economic surplus. Nevertheless, it is interesting to analyse with how much the surplus will change.

\hypertarget{consumer-tax-for-specific-inverse-demand-and-supply-functions}{%
\subsection{Consumer tax for specific inverse demand and supply functions}\label{consumer-tax-for-specific-inverse-demand-and-supply-functions}}

This general analysis can be linked to concrete inverse demand and inverse supply functions. Suppose these are given by:

\begin{equation}
p(Q) = A e^{-\alpha Q}, s(Q) = MC
\end{equation}

Assume that the marginal external costs are constant per unit and therefore:

\begin{equation}
e(Q)=mQ.
\end{equation}

From the analysis in the previous section we already know that the tax in the regulated equilibrium is equal to the marginal external costs \(m\).

We want to study what the predicted resulting equilibrium demand is in the tax-regulated optimum. In order to investigate this research question, we will look at the equilibrium conditions for consumers and producers:

\begin{equation}
A e^{-\alpha Q^{* R}} = p^{* R} + \tau = p^{* R} + m
\end{equation}

For the equilibrium producer, there is no change and the marginal costs should be equal to the equilibrium price:

\begin{equation}
MC = p^{* R}
\end{equation}

This gives us two equations and two unknowns. Substituting the second equation into the first gives:

\begin{equation}
A e^{-\alpha Q^{* R}}  = MC + m
\end{equation}

The assumption of constant marginal costs helps to derive analytical results. Without this assumption one often has to solve the equation numerically. Solving for the equilibrium quantity in the regulated equilibrium gives:

\begin{align}
e^{-\alpha Q^{* R}} &= \frac{MC + M}{A} \\
- \alpha Q^{* R} &=\ln \left[\frac{MC + M}{A} \right] \\
Q^{* R} &= -\frac{1}{\alpha}\ln \left[\frac{MC + M}{A} \right]= \frac{1}{\alpha}\ln \left[\frac{A}{MC + M}\right]
\end{align}

This requires the intuitive additional assumption that the marginal social costs \(MC+m\) should be lower than the maximum willingness to pay of the first consumer \(A\).

Because marginal costs are assumed to be constant, the equilibrium price remains at \(MC\) and therefore \(MC=p^{* R}\), in the regulated equilibrium. With these results one can estimate the change in economic surplus in the regulated and in the unregulated equilibrium by substituting the expression for the quantity in the economic surplus function. We leave this as an exercise for the reader.

Some important assumptions are made in the analysis. First, the model suggests that the only way the consumer can avoid external costs is by not choosing to buy the product. Despite potential knowledge that consumers have, external costs are not incorporated somewhere in the decision of the consumer to choose the market good. Whether this is a reasonable account of behavioural decisions requires further investigation using for example surveys and interviews. It can be that the consumer already adds the external costs to the price that is paid when making the purchase decision, even when there is no externality tax. As consumers are increasingly aware of the impacts of consumption on climate this might be realistic. If this would be the case, the equilibrium that we as economists observe is the socially optimal equilibrium and externality taxes would lead to lower welfare. Suppose consumer partly take the external costs \(eQ\) into account when buying their good. Each consumer adds additional costs \(\theta e(Q^{* R})\) to the supply price when making the purchase decision with \(0 \leq \theta \leq 1\). This will impact the equilibrium condition but not the value derived from the consumption and production of the market good:

\begin{equation}
L = \underbrace{\int_0^{Q^\ast R} p(Q)dQ}_\text{Total consumer benefits} - \underbrace{\int^{Q^\ast R}_0 s(Q)dQ - e(Q^{\ast R})}_\text{Total social costs} + \underbrace{\lambda(p(Q^{\ast R}) - p^{\ast R} - \theta e(Q^{* R}) - \tau)}_\text{Equilibrium constraint}.
\end{equation}

Using this Lagrangian one can derive the optimal tax:

\begin{equation}
\tau = (1 - \theta) MEC(Q^{* R})
\end{equation}

This expression confirms the intuition: when individuals fully consider marginal external costs when purchasing the good, \(\theta=1\), and the tax should be \(0\). When individuals ignore marginal external costs in their purchase decision, \(\theta=0\), and the tax should be equal to the marginal external costs. Surveys can be used to assess which share of external costs is already accounted for when individuals make their decisions.

Second, sometimes there are other ways in which consumers take responsibility for external costs, such as real monetary compensation of climate damage. For these cases it is expected that the consumers trade the compensation costs with a direct utility benefit related to taking responsibility. When the compensation costs that are paid really reduce damage, they can be counted as benefits in the economic surplus function. The externality tax is then lower compared to the one proposed above. A way to deal with this is to include only \emph{uncompensated} external costs in the economic surplus function. When \(\theta\) is the share of compensated external costs this leads to the same expression for the tax as for the previous example.

Third, it is useful to interpret \(e(Q^*)\) in more detail. Because our analysis is a partial equilibrium analysis, there can be two types of goods that generate externalities: the market good and the outside good. Strictly speaking total external costs are then a function of the consumption level of \emph{both} goods and can be defined as \(e(Q^*,G^* )=e(Q^*,Y-(p^*+\tau)Q^* )\). In practice, the introduction of a tax implies substitution of the market good to the outside good due to higher price levels, leading to higher consumption levels of the outside good. Furthermore, the tax itself has a direct impact on the consumption level of the outside good. Such a rise in consumption of the outside good can lead to additional external costs. Usually it is assumed that external costs of goods on all other markets are correctly priced. However, such an assumption is likely to be unrealistic.

When unregulated external costs are present for the outside good these can be properly accounted for in partial equilibrium analysis. When we assume that the externality costs for the market good and the outside good are additive the external costs can be defined as \(e(Q^* )=e_Q (Q^* )+e_G (Y-(p^*+\tau)Q^* )\). Here, the first term is increasing in the market good consumption and the second term is decreasing when there are positive external costs for consumption of the outside good. The second term gives the external costs of rising income levels. For sustainability analysis, one can find estimates for the external costs of the outside good by relating spendable income levels to emissions of CO2 equivalents multiplied with the price of these CO2 equivalents. The analytical analysis is more complicated, but suggests that the recommended tax should be lower when positive marginal external costs of the outside good are present.

One of the implications of this discussion is that policy makers should target their regulations to markets which have higher marginal external costs than the marginal external costs of the average consumption basket of consumers. Otherwise there is the risk that a tax will lead to substitution of the market good to the outside goods which has higher marginal external costs.

Another lesson to be learned from this third point is that it is best to start with taxation of the most polluting market good. For example, the external costs for particular meat types that are close substitutes are higher than for others. For example, starting taxation with the most polluting meat type will therefore lead to lowest risks of regulatory failure when an externality tax is introduced.

On the other hand, one could also argue that externality taxes in other markets should also be introduced. However, it is questionable whether such a `Pigouvian state' with all markets regulated with optimal taxes is politically feasible and desirable.

\hypertarget{stylised-solutions-for-the-case-when-marginal-external-costs-are-proportional-to-the-equilibrium-price.}{%
\subsection{Stylised solutions for the case when marginal external costs are proportional to the equilibrium price.}\label{stylised-solutions-for-the-case-when-marginal-external-costs-are-proportional-to-the-equilibrium-price.}}

It is possible to obtain stylised analytical solutions when it is assumed that the marginal external costs are a proportion of the equilibrium price in the regulated equilibrium. This practical assumption is sometimes made for a quick approximation of welfare effects for applied analysis. We can then write \(MEC=c_e p^{*R}=\tau\). The equilibrium conditions can then be written as:

\begin{equation}
p(Q^{* R}) = p^{* R} + \tau = p^{* R} (1 + c_e)
\end{equation}

For the equilibrium producer, marginal costs should be equal to the equilibrium price:

\begin{equation}
s(Q^{* R} )=p^{* R}.
\end{equation}

The challenge is the to find a solution for \(Q^{* R}\) for the following equation:

\begin{equation}
p(Q^{* R}) = s^{* R} (1 + c_e)
\end{equation}

Analytical expressions can then be obtained for common combinations of inverse demand and inverse supply functions. Table \ref{tab:priceregulation} gives the specifications of inverse demand and inverse supply, the equilibrium values for the unregulated equilibrium, and the equilibrium values for the regulated equilibrium. The parameter values can be obtained using the calibration procedure discussed earlier.

\begin{table}

\caption{\label{tab:priceregulation}Equilibrium prices and quantities with and without regulation assuming a tax that is proportional to $p^{\ast R}$}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lllllll}
\toprule
Case description & Inverse demand & Inverse supply & $p^\ast$ & $Q^\ast$ & $p^{\ast R}$ & $Q^\ast R$\\
\midrule
Case I: Exponential inverse demand and exponential inverse supply & $p(Q)=Ae^{-\alpha Q}$

Parameter assumptions:
$A>0$, $\alpha>0$

Regression (log-linear):

$\ln p = \ln A - \alpha Q + \epsilon$ & $MC(Q) = s(Q)=Be^{bQ}$

Parameter assumptions:
$B>0$,$B<A$.

Regression (log-linear):

$\ln MC = \ln B  +b Q + \epsilon$ & $A^{\frac{b}{\alpha + b}}B^{\frac{\alpha}{\alpha + b}}$ & $\frac{1}{\alpha + b} \ln \left[\frac{A}{B}\right]$ & $A^{\frac{b}{\alpha + b}}B^{\frac{\alpha}{\alpha + b}} \left(1 + c_e\right)^{\frac{\alpha}{\alpha + b}}$

  or:

  $p^\ast \left(1 + c_e\right)^{\frac{\alpha}{\alpha + b}}$ & $\frac{1}{\alpha + b} \ln \left[\frac{A}{B(1 + c_e)} \right]$

  or:

  $Q^\ast - \frac{1}{\alpha + b} \ln [1 + c_e]$\\
Case II: Constant elasticity inverse demand and inverse supply & $p(Q) = AQ^{-\alpha}$

Parameter assumptions:
$A>0$, $\alpha>0$

Regression (log-log):

$\ln p = \ln A - \alpha \ln Q + \epsilon$ & $s(Q)=BQ^b$

Parameter assumptions:
$B>0$

Regression (log-log):

$\ln MC = \ln B + b \ln Q + \epsilon$ & $A^{\frac{b}{\alpha + b}}B^{\frac{\alpha}{\alpha + b}}$ & $\left(\frac{A}{B}\right)^{\frac{1}{\alpha + b}}$ & $A^{\frac{b}{\alpha + b}}B^{\frac{\alpha}{\alpha + b}} \left(1 + c_e\right)^{\frac{\alpha}{\alpha + b}}$

  or:

  $p^\ast \left(1 + c_e\right)^{\frac{\alpha}{\alpha + b}}$ & $\left(\frac{A}{B(1 + c_e)} \right)^\frac{1}{\alpha + b}$

  or:

  $Q^\ast (1 + c_e)^{\frac{-1}{\alpha + b}}$\\
Case III: Logged inverse demand and inverse supply & $p(Q)=A-\alpha \ln Q$

Parameter assumptions:
$A>0$, $\alpha>0$

Regression (linear-log):

$p=A- \alpha \ln Q + \epsilon$ & $s(Q) = B + b \ln Q$

Parameter assumptions:
$B>0$

Regression (linear-log):

$MC = B + b \ln Q + \epsilon$ & $A \frac{b}{\alpha + b} B \frac{\alpha}{\alpha + b}$ & $e^{\frac{A-B}{\alpha + b}}$ & $A \frac{b}{\alpha + b} B \frac{\alpha}{\alpha + b} + c_e \frac{\alpha}{\alpha + b}$

  or:

  $p^\ast + c_e \frac{\alpha}{\alpha + b}$ & $e^{\frac{A - B(1+c_e)}{\alpha +b}}$

  or:

  $Q^\ast e^{\frac{-B(1+c_e)}{\alpha + b}}$\\
Case IV: linear inverse demand and inverse supply & $p(Q)=A- \alpha Q$

Regression (linear-linear):

$p=A-\alpha Q+ \epsilon$ & $s(Q)=B+bQ$

Regression (linear-linear):

$MC=B+bQ+\epsilon$ & $A \frac{b}{\alpha + b} B \frac{\alpha}{\alpha + b}$ & $\frac{A - B}{\alpha + b}$ & $A \frac{b}{\alpha + b} B \frac{\alpha}{\alpha + b} + c_e \frac{\alpha}{\alpha + b}$

  or:

  $p^\ast + c_e \frac{\alpha}{\alpha + b}$ & $\frac{1}{\alpha + b} \ln \left[\frac{A}{B(1 + c_e)} \right]$

  or:

  $Q^\ast - B c_e \frac{\alpha}{\alpha + b}$\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{analysis-of-changes-in-economic-surplus-due-to-externality-taxation}{%
\subsection{Analysis of changes in economic surplus due to externality taxation}\label{analysis-of-changes-in-economic-surplus-due-to-externality-taxation}}

An externality tax results in changes in economic surplus. These changes can be summarized in three parts: changes in consumer surplus, changes in producer surplus and changes in external costs. Because taxes are assumed to be transfers and have no impact on the inverse demand curve, these drop out of the economic surplus function. Mathematically the change in economic surplus can be expressed as:

\begin{align}
\Delta ES &= ES (Q^{**}) - ES(Q^*) = \Delta CS + \Delta PS + \Delta EC \\
 &= -\int_{Q^{**}}^{Q^*} p(Q)dQ + \int_{Q^{**}}^{Q^*} s(Q)dQ +e(Q^*) - e(Q^{**})
\end{align}

Because of lower equilibrium demand, consumer surplus decreases, producer costs decrease and external costs also decrease.

An analytical example can be provided using a Cobb-Douglas based inverse demand curve and constant marginal costs of production. Assume external costs are constant per unit and equal to \(e(Q)=mQ\). The change in economic surplus can then be written as:

\begin{align}
\Delta ES &= -\int_{Q^{**}}^{Q^*} \frac{A}{A+B} \frac{1}{Q} +\int_{Q^{**}}^{Q^*} MCdQ + m(Q^* - Q^{**}) \\
&=-\frac{A}{A+B} \ln\left[\frac{Q^*}{Q^{**}} \right] +MC (Q^* - Q^{**}) + m(Q^* - Q^{**})
\end{align}

This requires the calculation of the new equilibrium quantity after the tax has been implemented. The equilibrium tax will be equal to the marginal external costs \(m\). The supply price in the regulated equilibrium will not change and will be equal to \(MC\). The equilibrium quantity after the tax is implemented is then given by:

\begin{align}
p^{**} + \tau &= \frac{A}{A+B} \frac{1}{Q^{**}} \\
MC + m &= \frac{A}{A+B} \frac{1}{Q^{**}} \\
Q^{**} &= \frac{A}{A+B} \frac{1}{MC + m}
\end{align}

The equilibrium quantity without regulation is given by the demand function:

\begin{equation}
Q^* = \frac{A}{A+B} \frac{1}{p} = \frac{A}{A+B} \frac{1} {MC}
\end{equation}

Substituting these expressions in the equation for the change in welfare gives:

\begin{align}
 \Delta ES &= \frac{A}{A+B} \ln \left[\frac{MC + m}{MC} \right] + (MC + m) \frac{A}{A+B} \left(\frac{1}{MC} - \frac{1}{MC + m} \right) \\
 &= \frac{A}{A+B} \ln \left[ \frac{MC + m}{MC} \right] +  \frac{A}{A+B} \ln \left[\frac{MC + m}{MC} -1 \right] \\
 &= \frac{A}{A+B} \ln \left[ \frac{MC + m}{MC} \right] +  \frac{A}{A+B} m \\
 &= \frac{A}{A+B}\left( \ln \left[ \frac{MC + m}{MC} \right] +m \right)
\end{align}

This can be illustrated using an example. Suppose that the marginal costs of production of a particular type of meat are equal to 6 euro per kilogram. Consumption is equal to 2 million kilograms per year. Furthermore, assume that the external costs are estimated at 2 euros per kilogram. From the equilibrium condition we have \(\frac{A}{A+B}=Q* MC=4,000,000\). Substituting these values gives the change in economic surplus when an optimal tax is implemented:

\begin{equation}
\Delta ES=4,000,000\left(\ln \left[\frac{6}{6+2} \right]+\right)=6,849,272 \text{ euros per year}.
\end{equation}
This example has showed how---given the model assumptions---one can estimate a quantitative value for the change in economic surplus resulting from a consumer tax.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

This chapter shows what the behavioural underpinnings of supply and demand in markets are and how supply and demand functions can be calibrated using elasticity estimates and observations of market prices and quantities. We have also showed how economic surplus in markets can be estimated based on the inverse demand and inverse supply functions. Furthermore, an application to consumer externality taxation is given. The models that were developed are stylised and can be used as a natural starting point for further analytical and empirical development.

\hypertarget{erroreconsurplus}{%
\chapter{Behavioural Error and Economic Surplus}\label{erroreconsurplus}}

\hypertarget{introduction-4}{%
\section{Introduction}\label{introduction-4}}

This chapter investigates the implications of behavioural errors for the calculation of economic surplus. The sub-field of behavioural economics started around 1950 with the pioneering works of Herbert Simon and Ward Edwards. The aim was to develop a novel perspective on rationality and choice behaviour. Laibson and List (2015) define behavioural economics as follows:

\begin{quote}
Behavioural economics uses variants of traditional economic assumptions (often with a psychological motivation) to explain and predict behaviour, and to provide policy prescriptions (p.385).
\end{quote}

From their perspective, modern behavioural economics therefore can be viewed as an extension of the neo-classical perspective. Laibson and List (2015) suggest that behavioural economics refines three of the key core principles of classical economics (p.386). We add to this list how the fourth pillar `normativity' changes when behavioural errors are present.

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  \emph{Optimization}: market actors try to choose their best (feasible) option. They employ heuristics or a decision goal function for choosing amounts (utility, costs etc.).
\item
  \emph{Equilibrium}: market actors try to choose their best feasible option when interacting with others. This results in a behavioural equilibrium.
\item
  \emph{Empiricism}: behavioural models need to be tested with data.
\item
  \emph{Experienced based normativity}: experiences (not choices) of market actors generate value and experienced value of market actors is equal to value for society.
\end{enumerate}

Compared to the neo-classical paradigm, behavioural economics changes the first core pillar `optimization' by adding the word `try'. In classical economic models it is usually assumed that market actors behave rationally: choices of market actors are the result of a rational optimization procedure in the sense that choices track utility value. Behavioural economics has operationalized this idea by replacing the direct utility function by other heuristics or by other kinds of direct utility functions. An example of a consumer heuristic is: buy \(Q\) of the market good when it rains and \(Q+2\) of the market good when it does not rain. Such a heuristic is independent of the valuation or utility considerations and is therefore difficult to connect to analysis of economic surplus without further assumptions. Examples of different conceptualization of utility functions are utility functions that account for biased processing of probabilities and loss aversion (Tversky and Kahneman 1992).

The second pillar states that equilibrium in markets might be affected by behavioural errors as market actors interact with each other and is a logical result of adding behavioural error at the first pillar. According to behavioural economists, consumers and producers seek to optimize, but make behavioural errors. This in turn has impacts on how market actors interact with each other. When consumers and producers make systematic mistakes, this can impact equilibrium prices and quantities, but more in-depth analysis is needed to show what the impacts of behavourial errors is on equilibrium outcomes.

The third pillar states that particular assumptions on the rationality of choices made by neo-classical economists should be backed up by empirical analysis. It has been argued by neo-classical economists that choices are made as if market actors are rational. Friedman (1953) wrote about this in his famous essay `The methodology of Positive Economics':

\begin{quote}
Consider the problem of predicting the shots made by an expert billiard player. It seems not at all unreasonable that excellent predictions would be yielded by the hypothesis that the billiard player made his shots \textbf{as if} he knew the complicated mathematical formulas that would give the optimum directions of travel, could estimate accurately by eye the angles, etc., describing the location of the balls, could make lightning calculations from the formulas, and could then make the balls travel in the direction indicated by the formulas. Our confidence in this hypothesis is not based on the belief that billiard players, even expert ones, can or do go through the process described; it derives rather from the belief that, unless in some way or other they were capable of reaching essentially the same result, they would not in fact be expert billiard players.

It is only a short step from these examples to the economic hypothesis that under a wide range of circumstances individual firms behave as if they were seeking rationally to maximize their expected returns (generally if misleadingly called ``profits'') and had full knowledge of the data needed to succeed in this attempt; as if, that is, they knew the relevant cost and demand functions, calculated marginal cost and marginal revenue from all actions open to them, and pushed each line of action to the point at which the relevant marginal cost and marginal revenues were equal (Friedman, 1953).
\end{quote}

Using the analogy with the expert billiard player, Friedman (1953) argues that there can be a complicated mathematical description of market choices that correctly \emph{describes} behaviour even when market actors themselves are not aware of the complexities of their own behaviour. Why then, is additional behavioural economic analysis useful?

An important reason for behavioural economists to delve into the peculiarities of individual choices is that this might lead to a better \emph{understanding} of the underlying mechanisms of choice behaviour. After all, scientists are allowed to be curious and are free to formulate hypotheses about the behaviour of individuals. A key purpose of science is to seek understanding and behavioural economists contribute to this with their empirical research.

For \emph{prediction} of choices, understanding of the mechanisms of choice is not needed. `As if' rationality can be a sufficient assumption as more complicated and realistic behavioural models might not always lead to better predictions. Prediction of choice can be tackled using econometric techniques (regression, machine learning) and does not require any underlying behavioural model of choice that describes the mechanisms. For example, one can investigate the change in the consumption of solar panels resulting from subsidies using regression techniques with demand as a dependent variable and subsidies as an independent variable. Utility maximisation and cost minimisation are not needed for these types of impact analyses but might give suggestions about what kind of control variables one can include in the regression.

The scope of the essays of Friedman is only positive or descriptive economics.\footnote{Without normative recommendations, the field of economics reduces to the field of econometrics.} What Friedman (1953) did not discuss in his essay is that for normative policy analysis as if rationality is not a sufficient assumption. This is because these analyses require a model about how market actors determine value. For example, the question whether the subsidy for solar panels resulted in added value for society cannot be answered using models of prediction alone. Suppose subsidies result in a significant impact on the demand for solar panels. When effectiveness would be the only purpose the recommendation would be to have very high subsidies in order to increase the demand for solar panels.

To conclude our discussion about pillar (\emph{iii}): the goal of the analysis (understanding, prediction, normative policy analysis) therefore determines whether \emph{as if} rationality is an appropriate assumption. It also determines which research strategy is appropriate. When estimating economic value---and relate market outcomes to appropriate policy interventions---there is a normative component in the analysis, implying that the assumption of \emph{as if} rationality is not sufficient.

Pillar (\emph{iv}) shows that there is also a change in normative perspective when using behavioural economic models for policy analysis. The reason is that there can be a divergence between the value `calculation' at choice and the experienced value of market actors. This has potential impacts on the calculation of economic value and policy recommendations. Customers can be more (less) satisfied with a product than they expected at the moment of choice. Firms might overestimate (underestimate) the marginal costs of production. These biases can have impacts on equilibrium outcomes and also on the estimation of economic surplus when one assumes that it is experienced value which counts.

To illustrate the importance of behavioural errors for the economics of sustainability, it is interesting to look at an example from the literature. Busse et al. (2015) find that:

\begin{quote}
The choice to purchase a convertible or a four-wheel-drive is highly dependent on the weather at the time of purchase in a way that is inconsistent with classical utility theory. We consider a range of rational explanations for the empirical effects we find, but none can explain fully the effects we estimate.
\end{quote}

Interestingly, Busse et al. (2015) show that even for expensive purchases in real markets---such as a new car---particular valuation biases stemming from the choice context can occur. In other words, the inverse demand curve that we have learned to estimate with statistical techniques potentially captures contextual valuations which might be irrelevant for the experienced value of the product. This leads to a divergence between choice value and experienced value.

The remainder of these notes are organised as follows. Section 2 discusses the conceptual argument against neo-classical valuation and also discusses common responses. Section 3 uses choice behavioural modelling to arrive at an inverse demand curve that captures behavioural error. This section helps to develop mathematical narratives of choice that capture some of the essential ingredients of behavioural economics without delving to deeply into psychological details. Section 4 develops a simple behavioural economic surplus model graphically and mathematically and shows the implications of behavioural errors on the calculation of economic surplus. Section 5 and 6 continue by analysing the implications for externality taxation by investigating changes in economic surplus. Section 7 concludes.

\hypertarget{arguments}{%
\section{Arguments against neo-classical valuation and responses}\label{arguments}}

\hypertarget{introduction-5}{%
\subsection{Introduction}\label{introduction-5}}

The quantitative and methodological implications of behavioural economics for the calculation of economic surplus in markets are often not so clear. Such a calculation is needed as an input for practical cost-benefit assessments that seek to account for behavioural error. One of the methodological worries of cost-benefit practitioners who employ the micro-economic toolbox of the neo-classical economists is that behavioural errors lead to \emph{systematic} flaws in policy recommendations because surplus calculations are incorrect. Before discussing these economic implications of behavioural error, we will discuss the qualitative argument in more detail. It has the following structure:

\begin{enumerate}
\def\labelenumi{(\Alph{enumi})}
\tightlist
\item
  Market actors make behavioural errors when valuing market goods;
\item
  Therefore, the valuations derived from the choice-based neo-classical modelling approach are incorrect;
\item
  As a result, neo-classical economic analyses of policies are mistaken as economic surplus is calculated in an incorrect way.
\end{enumerate}

\hypertarget{discussion-of-premise-a}{%
\subsection{Discussion of Premise A}\label{discussion-of-premise-a}}

\hypertarget{introduction-6}{%
\subsubsection{Introduction}\label{introduction-6}}

Most researchers will grant that market actors make some behavioural errors in valuation and accept premise (A). However, five objections against the move from (A) to (B) are often raised. These objections are the `learning objection' (sub-section B.2), the `size objection' (sub-section B.3), the `generalisability objection' (sub-section B.4), the 'knowledge objection' (sub-section B.5) and the `competition objection' (sub-section B.6).

\hypertarget{the-learning-objection}{%
\subsubsection{The learning objection}\label{the-learning-objection}}

The first objection is the `learning objection' which comes in three parts. Individuals learn to behave rationally because they:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  learn from repeated choices (individual learning);
\item
  learn from the behavioural errors and choices of others (social learning);
\item
  learn from information from other sources.
\end{enumerate}

To illustrate learning with an example: A family household is likely to be able to estimate the amount of biological milk they need after some repetition. Or in the words of Friedman (1953):

\begin{quote}
``they figure it out what their optimal quantity is without knowing any of the mathematical formulas that the researcher uses to describe behavioural choice. When the household buys too much milk there will be leftovers for the next week. When there is not enough milk the household has to substitute to water or other beverages.''
\end{quote}

Such an argument is less convincing for less regular choices such as the purchase of a car, a new house or a holiday. For these choices social learning might be a relevant aspect. For example, for tourism destination choices, reviews of campings and hotels can serve as an input for valuations and can lead to more optimal behaviour of consumers under the assumption that these reviews are truthful. Customers can also learn from information provided by other sources such as science, customer protection agencies, media sources or the government.

In order to counter the learning objection, one has to show that in the presence of individual repetition, social interaction and information provision, individuals still make systematic mistakes. This is quite challenging to investigate empirically and often open for investigation for particular market goods.

\hypertarget{the-size-objection}{%
\subsubsection{The size objection}\label{the-size-objection}}

The second objection against premise (A) is the `size objection' which challenges the size of the behavioural error. When behavioural errors are present these might not be quantitatively relevant from a personal or societal perspective as the inverse demand curve which captures the WTP at choice, has not been impacted substantially. This objection has been countered by results from studies that investigated the size of behavioural error. For example, Allcott, Lockwood, and Taubinsky (2019) predict in a study on sugar-sweetened beverages in the US that---given their assumptions---consumers consume 31\% too much sugar-sweetened beverages on average.\footnote{See for example Visschers, Wickli, and Siegrist (2016).} This suggests that overconsumption in this market is substantial and quantitatively relevant. Furthermore, other studies have investigated food waste of consumers in order to estimate sub-optimal choice behaviour. These studies suggest that there is structural overconsumption of food.

\hypertarget{generalisability-of-empirical-results}{%
\subsubsection{Generalisability of empirical results}\label{generalisability-of-empirical-results}}

The third objection is more conceptual and is called the `generalisability objection'. It is targeted at the generalisability of empirical and experimental results and also applies to neo-classical empirical results. Does the result of Allcott, Lockwood, and Taubinsky (2019) only holds for the US or not? Does the result also apply to other goods? When for some particular markets in some countries behavioural errors are quantitatively relevant this does not imply that behavioural errors are relevant for all markets. According to this third objection, premise (A) has to be studied on a case-by-case basis.

Researchers have used meta-analyses to partly counter this objection. These meta-analyses investigate and summarize the results of many empirical studies and seek to explain structural differences in results using variables such as population characteristics, study design and setup, year of the study etc. An example of such a meta study is the study of Cadario and Chandon (2020) who investigate the effectiveness of healthy eating nudges. They employ the definition of Leonard (2008):

\begin{quote}
Nudges are defined by Leonard (2008) as ``any aspect of the choice architecture that alters people's behavior in a predictable way (1) without forbidding any options or (2) significantly changing their economic incentives. Putting fruit at eye level counts as a nudge; banning junk food does not.''
\end{quote}

Eating nudges therefore seek to change behavioural eating choices by changing the choice architecture without using economic incentives or traditional educational tools such as cooking workshops Cadario and Chandon (2020). When nudges show that people change their behaviour for a small change in choice architecture this signals something about potential behavioural error in their original choice. Cadario and Chandon (2020) find that the type of intervention has a substantial impact on the effectiveness of eating nudges. They consider three types of interventions: cognitive, affective and behavioural (p.468). Cognitive interventions seek to provide knowledge about food choices. These nudges have lower impacts than affective interventions which are targeted at feelings about food. Affective interventions in turn have lower impacts than behavioural interventions which are targeted at changes in behaviour (p.477). For example, Figure \ref{fig:cadario} in the paper shows that size enhancements such as smaller plates have the largest impacts Cadario and Chandon (2020).

\begin{figure}
\includegraphics[width=9.58in]{./figures/cadario} \caption{Figure from @cadario2020healthy}\label{fig:cadario}
\end{figure}

Jachimowicz et al. (2019) is another example of a meta study on the impact of changing default options. Controlling for a number of variables related to channels and study characteristics they show that changing defaults is mainly effective in the consumer domain, but less in the environmental domain (which is the domain where policy makers seek to foster pro-environmental behaviour). Their result therefore suggests that changing the choice architecture is mainly beneficial for society when performed by producers of market goods rather than the government.

These meta study examples show that the generalisability objection can be partly mitigated by providing systematic overviews of studies on the empirical impacts of behavioural interventions and error. As the field of empirical behavioural economics is growing rapidly, it is expected that more meta studies will be available in the next decade.

\hypertarget{knowledge-objection}{%
\subsubsection{Knowledge objection}\label{knowledge-objection}}

The fourth objection is the `\emph{knowledge objection}', which stems from an epistemic concern at the sight of the researcher: is it possible to know as a researcher whether a market actor makes a mistake in valuation? Is it really possible to determine the difference between experienced and decision utility? Premise (A) depends on the assumption that there is some `true' experienced utility concept: one can only determine what a valuation error is by contrasting a descriptive utility function with another `true' experienced utility function.

Consumers might have reasons for choice that are unobserved by the researcher. For example, it might be that households waste food because they are afraid to run out of food or want to keep food in the fridge for unexpected guests. Without asking about this, it is hard to infer from food choices only whether food waste is the result of behavioural errors in valuation. As Infante, Lecouteux, and Sugden (2016) write: {[}according to the behavioural economics view{]} ``the inner rational agent is trapped in a psychological shell''. The process of using the `true' preferences rather than the choice-based preference of individuals for economic surplus analysis is called `'preference purification' (Infante, Lecouteux, and Sugden 2016) The assumption that there are true preferences to be satisfied has motivated the use of nudging approaches in order to help individuals to make `better' choices.

Four methodological approaches have been offered in the literature to provide more evidence on the relationship between choice and experienced utility (Chetty 2015). The first approach is the `direct measurement approach':

\hypertarget{direct-measurement-approachmeasure-experienced-utility-using-self-reported-happiness}{%
\paragraph{\texorpdfstring{\textbf{Direct measurement approach}:measure experienced utility using self-reported happiness}{Direct measurement approach:measure experienced utility using self-reported happiness}}\label{direct-measurement-approachmeasure-experienced-utility-using-self-reported-happiness}}

This approach uses complementary survey techniques to measure whether experienced utility deviates from choice utility. The researcher has to make a choice whether the directly measured experienced utility captures the true (optimal) utility or whether the choice-based utility is the true utility. Often it is unclear how the two relate. Classical economists usually prefer preferences based on revealed choices. Other researchers trust the self-reported happiness surveys more. Direct measurement approaches also can include qualitative reasons behind choices that might give useful information about the `why' of choices (see for example Mouter, Koster, and Dekker 2021).

The second approach discussed by Chetty (2015) is the rational context approach:

\hypertarget{rational-context-approach-use-revealed-preference-measurements-in-a-context-where-agents-are-known-to-make-choices-that-maximize-their-experienced-utilities-these-are-called-sufficient-statistics}{%
\paragraph{\texorpdfstring{\textbf{Rational context approach}: use revealed preference measurements in a context where agents are known to make choices that maximize their experienced utilities; (these are called `sufficient statistics')}{Rational context approach: use revealed preference measurements in a context where agents are known to make choices that maximize their experienced utilities; (these are called `sufficient statistics')}}\label{rational-context-approach-use-revealed-preference-measurements-in-a-context-where-agents-are-known-to-make-choices-that-maximize-their-experienced-utilities-these-are-called-sufficient-statistics}}

When the utility is measured correctly in a context where the individual behaves rational, this experienced utility can be compared with measurements of decision utility in other contexts where behavioural errors are present in order to determine the size of behavioural errors. For example, when researchers know that there are contextual effects of the weather on car purchases (Busse et al. 2015), researchers might come up with a particular context, where weather does not influence preferences. Note however, that the researcher still has to make a choice what counts as a relevant context where the individual behaves rational.

The third approach is the structural modelling approach.

\hypertarget{structural-modelling-approach-build-a-structural-model-of-the-difference-between-decision-and-experienced-utility.}{%
\paragraph{\texorpdfstring{\textbf{Structural modelling approach}: build a structural model of the difference between decision and experienced utility.}{Structural modelling approach: build a structural model of the difference between decision and experienced utility.}}\label{structural-modelling-approach-build-a-structural-model-of-the-difference-between-decision-and-experienced-utility.}}

This approach uses mathematical modelling techniques with inputs from psychology and calibration based on empirical research. Examples of this approach will be given in Section II on behavioural modelling.

Allcott, Lockwood, and Taubinsky (2019) add a fourth strategy which is the rational reference group approach.

\hypertarget{rational-reference-group-approach-consumption-is-compared-to-particular-reference-groups-who-have-full-information-on-the-product-and-its-impacts.}{%
\paragraph{\texorpdfstring{\textbf{Rational reference group approach}: consumption is compared to particular reference groups who have full information on the product and its impacts.}{Rational reference group approach: consumption is compared to particular reference groups who have full information on the product and its impacts.}}\label{rational-reference-group-approach-consumption-is-compared-to-particular-reference-groups-who-have-full-information-on-the-product-and-its-impacts.}}

This approach deals with informational deficiencies of market actors. In Allcott, Lockwood, and Taubinsky (2019), the reference group are the dietarians who are assumed to have full information on the impact of sugar sweetened beverages on health. Here the researcher has to make a choice which particular group can be viewed as the group having full information. This assumes that the choices of other individuals are not based on full information and that this is the reason for a divergence in consumption between the two groups. More sophisticated analysis can add control variables to explain differences in consumption levels between the groups related to for example income, education or age. This concludes the discussion on the fourth objection whether it is possible to know as a researcher that market actors make mistakes.

\hypertarget{the-competition-objection}{%
\subsubsection{The competition objection}\label{the-competition-objection}}

The fifth objection against premise (A) is the 'competition objection. Friedman (1953) highlights that when a firm makes systematic mistakes, it would be competed out of the market by firms that behave more rationally. Systematic monetary mistakes can be costly and thereby subject to the forces of the market. This objection has been countered by offering two types of responses.

First, such an argumentative strategy cannot deal with \emph{systematic} mistakes of all producers and is also invalid for the systematic mistakes that consumers make when calculating their marginal benefits. Consumers do not always pay the price for the mistakes they make when estimating their marginal benefits and are therefore not competed out of the market. Producers might all overestimate marginal costs of production leading to less entry in the market. Second, for durable and experiential goods such as cars, houses and holidays it is harder for consumers to track quantity left-overs in order to arrive at their optimal quantity. Third, not all mistakes are publicly available and known. Therefore, evaluation mistakes can remain in the presence of competition.

\hypertarget{discussion-of-premise-b}{%
\subsection{Discussion of premise (B)}\label{discussion-of-premise-b}}

\hypertarget{introduction-7}{%
\subsubsection{Introduction}\label{introduction-7}}

The second premise (B) in the argument concludes from the presence of behavioural errors that marginal valuations are incorrect. For consumers this would imply that the maximum willingness to pay for a particular market good is measured incorrectly. This maximum marginal willingness to pay (MWTP) is the ratio of the marginal utility of consumption and the marginal utility of income.

\hypertarget{examples-of-optimization-with-behavioural-error-that-do-not-impact-mwtp}{%
\subsubsection{Examples of optimization with behavioural error that do not impact MWTP}\label{examples-of-optimization-with-behavioural-error-that-do-not-impact-mwtp}}

A strategy to counter this objection is to give examples of optimization procedures that include behavioural errors (so premise (A) is granted), but do not result in a different MWTP (so premise (B) does not result).

Suppose the direct utility with behavioural error is defined as: \(U^d(Q,G)=\epsilon \times U(Q,G)\), where \(\epsilon > 0\). The marginal utilities are then scaled with a factor \(\epsilon\) and therefore the ratio of marginal utilities is unaffected by behavioural error. When consumers make \emph{proportional} errors in the evaluation of the direct utility, these mistakes will drop out and the MWTP is still correctly measured.

A similar result can be obtained for \emph{additive} errors in direct choice utility: \(U^d (Q,G)=U^e (Q,G)+ \epsilon\). The error will drop out as the \emph{marginal} utilities govern market behaviour and choice of consumers as these show up in the first-order conditions for utility maximization. For multiplicative and additive errors in direct utility, the MWTPs are therefore correctly estimated. Premise (A) is granted, but premise (B) does not follow.

A more general third example can be given by assuming that the decision utility function is given by \(U^d (Q,G)=V(U^e (Q,G))\), where V is an increasing function of the experienced utility \(U^e (Q,G)\). This implies that when experienced utility increases (decreases), choice utility also increases (decreases). Using the budget constraint, the direct choice utility can be written as:

\begin{equation}
U^d (Q,G)=V(U^e (Q, y - pQ)).
\end{equation}

The first derivative for optimal choice of the market good is given by:

\begin{equation}
\frac{\partial U^d}{\partial V}(U^e_Q - pU^e_Q) = 0.
\end{equation}

As \(\frac{\partial U^d}{\partial V} > 0\) this results in:

\begin{equation}
p_d = \frac{U_Q^e}{U_G^e}.
\label{eq:focutility}
\end{equation}

This shows that for choice utility functions that are increasing in the experienced utility, the experienced MWTP coincides with the rational MWTP. Although this mathematical result is quite general, it is not entirely clear what it means from a behavioural perspective and whether it makes psychological sense. Usually the choice to buy the good is made first and the experience comes afterwards.
What this discussion shows is that premise (B) does not necessarily follow from premise (A). Rational MWTPs can in principle be correctly measured in the presence of behavioural error.

\hypertarget{discussion-of-premise-c}{%
\subsection{Discussion of premise C}\label{discussion-of-premise-c}}

\begin{enumerate}
\def\labelenumi{\Alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Discussion of premise C.
  Premise (C) can follow from premise (B) but requires additional analysis which will be provided in sections III and IV. Errors in evaluation of value can potentially lead to a discrepancy in the value used for decision making and the experienced value of consuming a good or service. Consumer and producer surplus can therefore be affected by behavioural errors.
\end{enumerate}

When both consumers and producers make behavioural errors or when externalities are present the impacts on economic surplus are less obvious. Without further assumptions it is unclear whether behavioural errors impact economic surplus negatively or positively.

\hypertarget{choicemodelserrors}{%
\section{Behavioural choice models with errors}\label{choicemodelserrors}}

\hypertarget{introduction-8}{%
\subsection{Introduction}\label{introduction-8}}

This section discusses parsimonious models of consumer choice that include behavioural biases. The focus is not on the underlying cognitive, neurological or physiological processes of why biases might arise. This is because the aim is to study the impact of behavioural error on economic surplus calculations rather than to discuss psychological aspects of behaviour that are relevant for the prediction of market choices. In line with the previous section, two kinds of direct utility functions are applied to illustrate the implications of behavioural error for economic evaluation and policy decision:

\begin{itemize}
\item
  \textbf{Direct decision utility function}: the direct decision utility for consuming a quantity \(Q\) of a market good and a quantity \(G\) of the outside good is given by \(U^d (Q,G)\). It includes all potential behavioural biases and errors an individual can make when calculating the value for decision making. The function can also be a descriptive device for a particular heuristic decision approach of an individual.*
\item
  \textbf{Direct experienced utility function}: the direct experienced utility function for consuming a quantity \(Q\) of a market good and a quantity \(G\) of an outside good is given by \(U^e (Q,G)\). It includes the numerical evaluation for this consumption for the case of perfect information and knowledge resulting in no biases in the evaluation. It therefore represents the `true' value of consumption.*
\end{itemize}

Classical economic theories usually assume that the decision utility function is equal to the experienced utility function: \(U^d (Q,G)=U^e (Q,G)\). The recent interest in behavioural economics has led to attempts to provide extensions for this model for public economics (see Bernheim and Taubinsky 2018).

This section discusses the implication of such extensions for the inverse demand curve that we estimate using market data. Section \ref{approach1} provides an example for a model where decision utility and experienced utility are fully separated. Section \ref{approach2} develops a model where individuals satisfice rather than optimize. Section \ref{approach3} develops a model where individuals calculate utilities on the basis of the weighted average of a decision utility function and an experienced utility function. Section \ref{approach4} develops a model that employs direct utility weights.

\hypertarget{approach1}{%
\subsection{Approach 1: separated decision and experienced utility}\label{approach1}}

We first investigate the implications of behavioural error for optimal utility and the inverse demand function when decision and experienced utility are fully separated. This implies that experienced utility has no relation at all to the choice utility. For this sub-section, the following direct linear decision utility function is assumed:

\begin{equation}
U^d (Q,G)= U^d (Q, y - pQ)
\end{equation}
where \(G=Y-pQ\), stems from the budget constraint. Behavioural errors in valuation can therefore occur for both the market good (the decision value of consumption) and the composite good (the decision value of remaining money). Taking the first derivative of the decision utility function gives the first order-condition for decision utility maximisation:

\begin{equation}
U^d_Q - U^d_G p = 0.
\end{equation}

Here, \(U_Q^d \equiv \frac{\partial U^d (Q)}{\partial Q}>0\) is the marginal decision utility for consuming the market good and \(U_G^d \equiv \frac{\partial U^d (Q)}{\partial G}>0\) the marginal utility of the outside good which is equal to the marginal utility of income in equilibrium. Using the first-order condition one can derive the inverse demand curve:

\begin{equation}
p_d = \frac{U_Q^d}{U_G^d},
\label{eq:focdu}
\end{equation}
which is the well-known ratio of marginal decision utilities.

The question is: does it matter? Do consumers loose utility because of the behavioural errors in evaluation? And will this impact economic valuation estimates? Or more formally: are behavioural errors a sufficient condition for biased policy evaluation? In order to answer this question, we have to analyse the counterfactual case of perfect rationality. Suppose that the consumer would be perfectly rational. For that case the optimal the inverse demand is given by:
\begin{equation}
p_e = \frac{U_Q^e}{U_G^e},
\label{eq:focexpu}
\end{equation}
This looks very much like the result of Eq. \eqref{eq:focdu}. The only thing that changes in the expression for the inverse demand are that we now have a ratio of the marginal \emph{experienced} utilities.

The marginal choice utilities result in demand choice \(Q\) which is in turn evaluated by the experienced utility function. For policy analysis, the only thing that matters in the end is whether the demand Q that is chosen is optimal. This demand determines the final consumer benefits based on the experienced willingness to pay in Eq. \eqref{eq:focexpu}.

Consider the following case. John has a very complicated and biased decision utility function and makes all kinds of errors in the marginal utility of income and in the marginal utility derived from buying solar panels. However, this results in a consumption level of the good which is exactly equal to the case of the decision based on experienced inverse demand (Eq. \eqref{eq:focdu}. Therefore \(Q^\ast_e = Q^\ast_d\). The implication for John is that in terms of experienced utility---and in terms of economic value---there is no impact of the behavioural errors of John on the optimal experienced utility.

Only when there is a divergence between \(Q^\ast_e\) and \(Q^\ast_d\). there is an impact on economic value and for these cases the question is: how much? The answer to that question of course depends on the divergence between the marginal decision utility and the marginal experienced utility as captured by the respective inverse demand curves. When the experienced utility function is very flat around the optimal demand, sub-optimal choices of demand due to behavioural errors do not impact valuations a lot.\footnote{You can make a graph with a concave experienced utility function, optimal demand and sub-optimal demand to visualize this.} This leads us to an intermediate conclusion:

\emph{Behavioural errors are not a sufficient condition for errors in the calculation of economic value. This implies that the assumption of perfect rationality of consumers is not a necessary condition for a correct calculation of economic value. The quantitative impact of sub-optimal choice of demand depends on the steepness of the experienced utility function around the optimum. This is captured by the second-order derivative of the experienced utility function.}

Or in other words: it depends. Practically speaking the flatness of the experienced utility curve can be investigated by asking question about changes in experienced value such as happiness when a particular good or service is consumed in higher or lower levels.

\hypertarget{approach2}{%
\subsection{Approach 2: satisficing and the inverse demand curve}\label{approach2}}

One of the founding fathers of behavioural economics, Herbert Simon suggested that persons are not optimizing, but satisficing (Simon 1957). In the context of market analysis satisficing means that market actors consume and produce quantities that give `good enough' utility or costs rather than optimal utility or costs. In mathematical terms this implies that persons do not set the first-order condition for utility maximisation or cost minimisation exactly to \(0\). The point of satisficing is not that decision utility and experienced utility are different, but rather that the first pillar `optimisation' is incorrect. This gives satisficing models of behaviour a distinct flavour.

Suppose a representative individual has an income level \(Y\) and spends money on the market good \(Q\) and some composite or outside good \(G\). The price of the composite good is assumed to be exogenous and is normalised to \(1\). The price of the market good is given by \(p\). The consumer can choose \(Q\) and \(G\) but has to account for the budget constraint.

For the rational case the first-order condition for utility maximisation results in Eq. \eqref{eq:focutility} . This is the well-known result that the inverse demand curve is defined by the ratio of two marginal experienced utilities. For specified direct experienced utility functions, the first-order condition can also be solved to obtain the optimal quantity \(Q^*\).

Now suppose the representative consumer is satisficing rather than optimizing. This potentially leads to sub-optimal demand choices. Mathematically speaking, satisficing can be modelled by adding an error term in the first-order condition. This error term \(e^{-aE}\epsilon\) is related to the effort provision of a consumer \(E\). The parameter \(a>0\) governs the strength of the impact of effort on the error. When the individual provides infinite effort, \(E\rightarrow \infty\) this error term will vanish to \(0\). When the consumer provides no effort, \(e^{-aE}\epsilon=\epsilon\). The parameter \(\epsilon\) is assumed to be bounded from below and above. It can be interpreted as the size of the error for consumers who do not provide any effort. The satisficing condition can then be written by:

\begin{equation}
- U^e_G p + U^e_Q + e^{-aE}\epsilon = 0.
\end{equation}

The inverse choice demand function is then given by:

\begin{equation}
p_d = \frac{U_Q^e +  e^{-aE}\epsilon}{U_G^e} = \frac{U_Q^e}{U_G^e} + \frac{e^{-aE}\epsilon}{U_G^e}.
\end{equation}

Compared to the experienced inverse demand curve (Eq. \eqref{eq:focdu}), satisficing therefore results in an inverse choice demand curve which shifts inwards or outwards with \(\frac{U_Q^e}{U_G^e}\). The sign and size of this shift depends on the effort provision, the size and sign of the error and the marginal utility of income. The rational model is a limiting case that results for \(\epsilon =0\), and/or \(E \rightarrow \infty\).

Until now it is assumed in this subsection that the consumer is not aware of the cognitive biases implying that it is assumed that effort provision is exogenous. An open question in this model is therefore how optimal effort can be determined assuming that an individual makes a trade-off between the benefits of effort provision and the costs of sub-optimal consumption. As in the previous section, it is likely that the curvature of the utility function around the optimum plays a role here. This curvature is captured by the second derivate of the utility function. When this second derivative is strongly negative, sub-optimal consumption has a large impact on utility. For these cases, it is then more likely that the individual will provide more effort when making choices. When the utility function is very flat around the optimum, consumption mistakes resulting from satisficing are not very costly for the consumer and therefore it is likely that the effort that is provided by the individual to reduce error is low.

\hypertarget{approach3}{%
\subsection{Approach 3: System I and system II thinking}\label{approach3}}

A third approach to model behavioural error is to define decision utility as the weighted average of System I and System II utility. This terminology was made popular by Kahneman (2011), and is based in the work of Stanovich and West (2000). Loosely speaking, System I can be interpreted as the brain system which is heuristic and makes fast decisions. System II is the slow thinking system which makes decisions upon deliberation and reflection. Instead of choosing to model behaviour with one system or the other, it makes sense to account for both systems when writing down a direct utility function. Therefore, we define direct utility as the weighted average of system I and system II perceived utility:

\begin{equation}
U^d (Q,G)= \varphi U^I (Q, G) + (1 - \varphi) U^{II} (Q, G). 
\end{equation}

This specification of utility assumes the individual has at least some partial knowledge about the experienced utility when making the decision, but is also partly governed by the `heuristic' System I decision utility which is assumed to be irrelevant for evaluation purposes. The parameter \(0 \leq \varphi \leq 1\), governs how system I utility and system II utility are weighted. When \(\varphi=1\), system I utility is completely governing the decision of the consumer. When \(\varphi=0\), we obtain the rational or neo-classical case where decision utility is equal to experienced utility.

When we assume that experienced utility is equal to system II utility, \(U^{II} (Q,G)=U^e (Q,G)\), and we obtain:

\begin{equation}
U^d (Q,G)= \varphi U^I (Q, G) + (1 - \varphi) U^e (Q, G).
\end{equation}

Substituting \(G=Y-pQ\), and taking the total derivative gives the first-order condition for utility maximisation:

\begin{equation}
-p \varphi U_G^I + \varphi U_Q^I - p (1 - \varphi) U^e_G +  (1 - \varphi) U^e_Q = 0 \end{equation}

Collecting the terms around the price gives:

\begin{equation}
-p (\varphi U_G^I + (1 - \varphi) U^e_G) +  \varphi U_Q^I + (1 - \varphi) U^e_Q = 0 \end{equation}

Solving for the price gives the implicit solution for the inverse demand curve:

\begin{equation}
p_d = \frac{ \varphi U_Q^I + (1 - \varphi) U^e_Q }{\varphi U_G^I + (1 - \varphi) U^e_G}
\end{equation}

The inverse demand curve can therefore be expressed as a ratio of weighted averaged system I and system II marginal utilities. When \(\varphi =1\), the inverse demand curve is completely independent of the decision utility and the model of section II.A arises. When \(\varphi=0\), we obtain the rational or neo-classical case where inverse demand is the ratio of marginal experienced utilities.

Some progress can be made by making additional assumptions on how experienced marginal utility and system I marginal utilities relate. Suppose that these marginal utilities are proportionally related so that we can write:

\begin{equation}
U^I_Q = c_Q U^e_Q,
\label{eq:uiq}
\end{equation}
and:
\begin{equation}
U^I_G = c_G U^e_G.
\label{eq:uig}
\end{equation}

When consumers are unaware of the potential benefits of additional consumption of the good, \(c_Q<1\). When they are too optimistic about the consumption benefits of the good, \(c_Q>1\). Similarly, consumers can be unaware of the potential of using the money for other purposes leading to \(c_G<1\). When consumers are too optimistic about the potential of spending their money otherwise, \(c_G>1\).

Substituting the expressions for the decision utility functions into the expression for the inverse demand curve gives:
\begin{equation}
p_d = \left(\frac{ \varphi c_Q + (1 - \varphi) }{\varphi c_G + (1 - \varphi) } \right) \frac{U^e_Q }{U^e_G}\equiv b_c p_e 
\end{equation}
Given the assumptions in Eq. \eqref{eq:uiq} and \eqref{eq:uig}, the experienced demand curve \(p_e\) is shifted with a \emph{proportional constant} factor \(b_c = \frac{ \varphi c_Q + (1 - \varphi) }{\varphi c_G + (1 - \varphi) }\). This proportional shift is very useful for the analysis of economic value and gives a structural relationship between the experienced and the decision inverse demand curve. When \(c_Q<c_G\), there will be an \emph{underestimation} of marginal benefits as \(b_c<1\). When \(c_Q>c_G\), there will be an \emph{overestimation} of marginal benefits, and \(b_c>1\). The case \(c_Q=c_G\) is the special case where behavioural errors are present, but without impacts for the marginal benefits of consumers as the marginal utilities are shifted with the same proportion.

This again highlights the important fact that the MWTP is a ratio of marginal utilities. Behavioural errors can occur when consumers are not rational about the marginal utility of consumption (`benefits of additional consumption'), or the marginal utility of remaining money (`benefits of additional income'). Note that it is assumed that the parameter \(0 \leq \varphi \leq 1\) is exogenous. For more complicated models of behaviour, this parameter can be made dependent on the degree of information and the effort provision of the individual.

\hypertarget{approach4}{%
\subsection{Approach 4: direct utility weights.}\label{approach4}}

The fourth approach uses structural modelling and incorporates direct utility weights that capture misperceptions in the benefits in consumption of the market good and the outside good. The advantage of this approach is that the conceptualization of behavioural error is very transparent and that adjustments of existing specifications of direct utility can be used. We will illustrate this approach using an exponential-linear direct utility function defined as:

\begin{equation}
U^d (Q,G) = w_1 \frac{A}{a}\left(1 - e^{-aQ}\right) + w_2 BG
\label{eq:explin}
\end{equation}
In Eq. \eqref{eq:explin} \(w_1\) is a perception weight related to the direct utility of consumption of the market good. Benefits of the market good are overestimated when \(w_1>1\), and underestimated when \(w_1<1\). In a similar way, \(w_2>1\) describes the case of overestimation of the benefits of remaining money and \(w_2<1\) the case of underestimation of the benefits of remaining money.

Substituting the amount for the outside good \(G=Y-pQ\) and taking the total derivative gives:
\begin{equation}
\frac{dU^d}{dQ} = w_1 A e^{-aQ} - p w_2 B = 0.
\end{equation}
The inverse choice demand function including behavioural errors is then given by:
\begin{equation}
p_d = \frac{w_1}{w_2}\frac{A}{B}e^{-aQ}.
\end{equation}
The experienced inverse demand results for \(w_1=w_2=1\). The experienced inverse demand curve lies above the decision inverse demand curve when proportional errors in the evaluation of the market good utility are larger than proportional errors in the evaluation of the benefits of remaining money (\(w_1<w_2\)).

Again, this model shows a plausible behavioural underpinning for proportional shifts in the inverse demand function as we can define \(b_c=\frac{w_1}{w_2}\). When there are no income effects this approach also works for more complicated specifications of the direct utility function. In the presence of income effects things become more complicated. The interested reader might try this using a Cobb-Douglas direct utility function.

\hypertarget{conclusion-1}{%
\subsection{Conclusion}\label{conclusion-1}}

This section has showed that for plausible behavioural utility functions one can obtain proportional shifts in the inverse demand function due to behavioural error. There are several ways to obtain more information about the size of behavioural error. First, error can be operationalised by asking customers' MWTP before and after consuming the good. When the two diverge this can be a sign that behavioural error is present.

Second, other methods can be employed to obtain the weights in the direct utility function. For example, one can ask customers to grade the market good before and after consumption using satisfaction surveys. When a customer grades the good in utility due to experience. The ratio between decision and experienced utility is then given by \(1/1.25\) and this provides an argument to set \(w_1=0.8\). Whether it is possible to disentangle satisfaction related to price payments and actual use of the good is an interesting open question.

Third, one can investigate whether consumers think they have chosen the optimal amount of the good after experiencing the good.

\hypertarget{beherrorsurplus}{%
\section{Behavioural errors and economic social surplus}\label{beherrorsurplus}}

\hypertarget{implications-of-behavioural-errors-for-consumer-surplus}{%
\subsection{Implications of behavioural errors for consumer surplus}\label{implications-of-behavioural-errors-for-consumer-surplus}}

This section shows what the implications of behavioural error are for the calculation of economic social surplus. For simplicity it is assumed in this section that consumers pay a price \(p^*\) for the good and that this price will not change as a result of behavioural error. Define \(p_d (Q)\) as the decision inverse demand curve and \(p_e (Q)\) as the experienced inverse demand curve. Figure \ref{fig:beherrors} shows the implication of behavioural errors for experienced consumer surplus.

\begin{figure}
\includegraphics[width=15.25in]{./figures/beh_errors} \caption{implications of behavioural errors on consumer surplus}\label{fig:beherrors}
\end{figure}

The left panel of Figure \ref{fig:beherrors} shows the case of \emph{underestimation} of the marginal value of an additional unit consumption: the decision marginal willingness to pay is lower than the experienced marginal willingness to pay. From the previous section we know that there can multiple sources for this underestimation.

As expected, this underestimation of value results in an equilibrium quantity level which is lower than the optimal level as \(Q^*_e > Q^*_d\). This will lead to a loss in consumer surplus. But how large is this loss compared to the case of full rationality?

The consumer surplus for the left panel is equal to the area under the experienced inverse demand curve up to the equilibrium demand \(Q^*_d\). When behavioural errors are present, the consumer benefits are therefore equal to \(A+C+D\). The consumer costs are equal to the area \(D\). Consumer surplus in the presence of behavioural error is then equal to consumer benefits minus consumer costs: \(A+C\).

When consumers are perfectly rational, the consumer surplus is again equal to the area under the experienced inverse demand curve. As the marginal willingness to pay for the good is higher for perfect rationality, the equilibrium consumption is higher and equal to \(Q^*_e\). This results in consumer benefits equal to \(A+B+C+D+E+F\), consumer costs equal to \(D+E+F\) and consumer surplus equal to \(A+B+C\). Behavioural errors therefore lead to a loss in consumer surplus equal to \(B\). This area is equal to the net benefits of those who do not participate in the market because they underestimate the marginal value of consumption.

Next, consider the second case of overestimation of the marginal value of consumption as depicted by the right panel of Figure \ref{fig:beherrors}. Again, the optimal amount is not consumed, but now the market demand is higher than optimal. Consumer surplus is equal to the area under the experienced inverse demand curve \((A+C+E)\), minus the consumer costs \((C+E+D)\), resulting in the area \(A-D\).

In the presence of perfect rationality, the consumption will be equal to \(Q^*_e\). This leads to consumer surplus equal to \(A\). When there is an overestimation of the marginal benefits of consumption of the market good, there is a \emph{loss} in consumer surplus equal to the area \(D\). This area is equal to the net loss in surplus because of the additional entry of consumers due to misperception of marginal benefits. The experienced marginal willingness to pay for these consumers is lower than the price and therefore their net contribution to total consumer surplus is negative. From this discussion on Figure \ref{fig:beherrors} we arrive at the following observation related to consumer surplus:

\emph{When optimal market demand is not equal to decision market demand, consumer surplus is negatively affected by behavioural errors. The bias goes only via the channel of higher or lower consumption of the market good.}

What is true at the individual level is therefore also true at the market level. This is because for the consumers who do not change behaviour in the market when they become more rational there is no change in their consumption decision. These consumers buy the good anyways, regardless of whether their marginal decision willingness to pay is biased or perfectly rational. Therefore, their experienced marginal benefits will not change when they become more rational. Even when the decision marginal willingness to pay is much more complicated graphically, it is only the equilibrium demand together with the experienced inverse demand curve that determines what the size of consumer surplus is and what the size of the losses are related to behavioural error. Behavioural errors therefore mainly impact the decisions of the consumers who are close to the equilibrium level: the so-called marginal consumers.\footnote{Note that marginal does not mean unimportant here, but rather consumers who are at the margin of changing their choice.} Providing information to those who are in doubt is therefore most relevant.

Figure \ref{fig:beherrors2} shows the particular case of a market where the equilibrium consumption demand is `coincidentally' equal to the optimal demand. This case is important as it shows that behavioural error is not a sufficient condition for losses in calculations of consumer surplus. The consumer surplus for rational and boundedly rational choice is in both cases equal to the area \(A\).

\begin{figure}
\includegraphics[width=10in]{./figures/beh_errors2} \caption{Behavioural error without losses in consumer surplus}\label{fig:beherrors2}
\end{figure}

A mathematical definition of economic social surplus in the presence of behavioural error is given by:
\begin{equation}
CS = \int_0^{Q_d^*} p_e(Q)dQ - p^*Q_d^*
\label{eq:cserror}
\end{equation}
The first part of Eq. \eqref{eq:cserror} are the consumer benefits which are given by the integral under the experienced inverse demand curve up to the equilibrium decision demand. The second part are the consumer costs which are equal to the equilibrium decision demand multiplied by the price. In line with Figure \ref{fig:beherrors2}, this expression is equal to the case of perfectly rational behaviour when the equilibrium demand with behavioural error is equal to the optimal demand with rationality: \(Q_d^* = Q^*_e\).

A survey question for consumers to investigate their behavioural errors is therefore: Do you consume too much or too little of a particular good upon reflection? The answers might give an indication of whether market choice demand is above or below rational demand based on the experienced marginal benefits.

The results depend on the assumption that equilibrium prices do not adjust due to behavioural errors of consumers. The next sub-sections will analyse this assumption in more detail.

\hypertarget{the-impact-of-behaviour-error-on-supply-decisions-and-producer-surplus}{%
\subsection{The impact of behaviour error on supply decisions and producer surplus}\label{the-impact-of-behaviour-error-on-supply-decisions-and-producer-surplus}}

The next step is to gain insights on the impact of behavioural errors of firms on supply decisions. These firms are assumed to minimize costs given a production target resulting in a total cost function that is potentially a function of the production level. It is assumed that fixed costs are zero. Total costs in the market are given by the area under the inverse experienced supply curve. Figure \ref{fig:firmerror} shows the implications of behavioural errors of firms on market equilibrium.

\begin{figure}
\includegraphics[width=16.61in]{./figures/firmerror} \caption{The implications of behavioural errors of firms on market equilibrium. Note that the figures schematically summarize the argument and potentially amplify the difference between $s_d (Q)$ and $s_e (Q)$ for clarity reasons. In reality the errors might be smaller or larger}\label{fig:firmerror}
\end{figure}

The left panel of Figure \ref{fig:firmerror} shows the decision inverse supply function \(s_d (Q)\) which can be interpreted as the perceived marginal production costs for a group of firms. Because consumers can make behavioural errors, equilibrium arises from the intersection of this decision inverse supply curve with the inverse decision demand curve \(p_d (Q)\). This results in an equilibrium price equal to \(p^*_d\). The equilibrium quantity and the equilibrium price are lower compared to the case of perfect rationality of firms. The right panel shows the opposite situation, where firms make behavioural errors leading to a lower equilibrium price and higher equilibrium demand compared to the case of perfectly rational firms.

What are the implications of behavioural errors of firms for producer surplus? The left panel of Figure \ref{fig:firmerror} shows the situation where the marginal costs of production are overestimated. The total producer revenues in the market are then equal to the area \(A+B+C+D+E\). The total producer costs are equal to the area under the experienced supply function which is equal to \(E\). Producer surplus in the presence of behavioural error, is then equal to \(A+B+C+D\).

For the case of perfectly rational firm decisions, demand will increase resulting in producer benefits equal to \(C+D+E+G+H\). Producer costs are equal to \(E+H\) and therefore producer surplus is equal to \(C+D+G\). The difference with the bounded rationality case is then given by \(G-A-B\), which is smaller or larger than \(0\) depending on the change in demand. Overestimation of marginal costs can therefore lead to an increase in producer surplus in the market (but at the expense of consumer surplus as prices increase).

The right panel shows the case where the marginal costs are underestimated by firms. This leads to a lower price and higher equilibrium demand. Producer surplus is equal to the producer revenues \((C+D+E+I+J)\) minus the producer costs \((B+D+E+F+G+H+I+J)\) and is therefore equal to \(C-B-F-G-H\).

Producer surplus can also be written in mathematical terms:
\begin{equation}
PS = p_d^* Q_d^* - \int_0^{Q_d^*} s_e(Q)dQ.
\label{eq:pserror}
\end{equation}

The first part of Eq. \eqref{eq:pserror} are the producer revenues, whereas the second part are the total experienced or real costs in the market (as fixed costs are assumed to be equal to 0). When \(Q_d^* = Q_e^*\) producer surplus coincides with the rational case.

Before moving on it is useful to reflect a bit more on the nature of behavioural error for producers. A difference with consumers is that for particular sectors, producer surplus is really monetary: both the benefits and the costs are of a monetary nature. Consider, for example the case of solar panels. It is reasonable to assume that firms that make solar panels are well aware of the costs of production and their benefits of selling the solar panels. This is not the case for consumers: consumer spending is monetary, but the inverse demand is a monetized preference summarized in the maximum marginal willingness to pay. A preference is less tangible and might lack the direct `feedback' of monetary spending and revenues.

Many firms in the right panel of Figure \ref{fig:firmerror} will make a structural loss as the experienced supply curve lies above the equilibrium price. At some point these firms literally pay the price for their behavioural errors. Only the firms with \(s_e (Q)< p_d^*\) will make positive profits. For an equilibrium in the market, it is therefore more plausible that the situation of the left panel occurs: firms overestimate marginal costs, for example because they are not minimizing expected costs, but do something else.

This argumentation might be less convincing for sectors where supply decisions use inputs which are non-monetary, less tangible and/or more difficult to process by producers.

Consider for example supply decisions on sustainability research. When scientists have to estimate the amount of labour they invest in research proposals they can make mistakes and underestimate the marginal costs of research. When we observe structural extra work by scientists this might be related to behavioural biases related to labour supply and time: the decision supply curve is then below the experienced supply curve.

\hypertarget{totalsurplus}{%
\subsection{Behavioural error and total economic surplus}\label{totalsurplus}}

This sub-section brings together the insights of the previous sections and shows the total economic surplus when behavioural errors are present at the demand and the supply side. It thereby investigates how equilibrium economic value is affected by behavioural errors. We can distinguish 4 conceptual cases:

\begin{enumerate}
\def\labelenumi{\Roman{enumi}.}
\tightlist
\item
  Producers overestimate marginal costs and consumers overestimate marginal benefits;
\item
  Producers overestimate marginal costs and consumers underestimate marginal benefits;
\item
  Producers underestimate marginal costs and consumers overestimate marginal benefits;
\item
  Producers underestimate marginal costs and consumers underestimate marginal benefits
\end{enumerate}

Only case I will be discussed graphically and the other cases are left to the reader. But before doing this, we add up consumer surplus and producer surplus to obtain a mathematical expression for total economic surplus in the presence of behavioural error:

\begin{align}
ES = CS + PS =& \int_0^{Q_d^*} p_e(Q)dQ - p^*Q_d^* + p_d^* Q_d^* - \int_0^{Q_d^*} s_e(Q)dQ \\
=& \int_0^{Q_d^*} p_e(Q)dQ - \int_0^{Q_d^*} s_e(Q)dQ
\end{align}

This mathematical expression holds for all conceptual cases. The well-known insight that remains here is that payments of consumers are revenues of producers. When regulators are not interested where the economic surplus ends up this `dampens' the impact of behavioural errors of consumers (producers) as these are partly mitigated by benefits for producers (consumers).\footnote{Following classical economic analysis, we ignore the potential normative relevant issue of the source/reason of payments and benefits. Consumers can benefit from behavioural errors of producers and producers can benefit from behavioural errors of consumers.} Compared to the rational case, the only thing that changes is \(Q_d^*\) which is in line with the earlier observation that behavioural errors materialize in sub-optimal choice of demand.

From this economic surplus equation, we can also learn that there is exactly one point for which economic surplus is optimal. Differentiating \(ES\) with respect to \(Q_d^*\) gives \(p_e (Q_d^*)-s_e (Q_d^*)=0\), which is satisfied when \(Q_d^* = Q_e^*\) as it then shows the equilibrium condition for perfect rationality.\footnote{The second derivative of \(ES\) is equal to the slope of the inverse demand curve (negative) minus the slope of the supply curve (positive) which implies that economic surplus is strictly concave in \(Q\). Therefore, there is only one solution for optimal economic surplus which economic surplus is optimal when experienced inverse demand is downward and the experienced supply curve is not downward sloping.}

Figure \ref{fig:losssurplus} shows the impact of behavioural errors for two possible constellations of case I. Equilibrium arises at the point where the decision inverse supply curve is equal to the decision inverse demand curve.

\begin{figure}
\includegraphics[width=17.03in]{./figures/losssurplus} \caption{Loss of economic social surplus because of behavioral errors for Case I}\label{fig:losssurplus}
\end{figure}

The left panel shows the case where equilibrium demand is higher than optimal. The right panel shows the case where equilibrium demand \(Q_d^*\) is lower than optimal. The red area indicates the economic surplus loss because of behavioural errors. This area occurs in the left panel because for the consumers between \(Q_e^*\) and \(Q_d^*\) the experienced marginal costs of production are higher than their experienced marginal benefits. In the right panel it occurs because for the consumers between \(Q_d^*\) and \(Q_e^*\) the marginal costs of production are lower than the marginal experienced willingness to pay. More economic surplus could be gained when demand moves closer to the optimal demand. In the absence of externalities, the \emph{economic surplus} loss resulting from behavioral errors can be written as the difference between the two experienced curves:
\begin{equation}
\Delta ES = \int_{Q_e^*}^{Q_d^*} p_e(Q) dQ -  \int_{Q_e^*}^{Q_d^*} s_e(Q) dQ 
\end{equation}
This formula exactly represents the red `triangles' in Figure \ref{fig:losssurplus}. The economic surplus loss formula is the same for all cases I--IV introduced above. It can be operationalised by assuming particular forms for the experienced inverse demand and inverse supply curves. The figure and the formula show that the economic surplus loss is governed by the difference between \(Q_d^*\) and \(Q_e^*\).

Figure \ref{fig:nolosssurplus} shows the special case where behavioural errors are present without losses in economic surplus. When choice demand is equal to optimal demand, no economic surplus losses occur despite the fact that consumers and producers make behavioural errors. Nevertheless, this figure shows that behavioural errors lead to higher equilibrium prices compared to the optimal case.

\begin{figure}
\includegraphics[width=8.72in]{./figures/nolosssurplus} \caption{Behavioural error without economic surplus losses}\label{fig:nolosssurplus}
\end{figure}

The increase in prices results in an increase in producer surplus for those firms producing on the market. The higher prices for consumers do not impact economic surplus as these are transfers from consumers to the producers. Despite the fact that behavioural errors are made at both sides of the market and that prices are higher compared to the rational case; economic surplus is at the optimal level. Higher prices resulting from behavioural error at the consumer level are therefore not sufficient to conclude that economic surplus goes down.

\hypertarget{behavioural-error-and-policy-recommendations}{%
\section{Behavioural error and policy recommendations}\label{behavioural-error-and-policy-recommendations}}

\hypertarget{behavioural-errors-and-information-provision}{%
\subsection{Behavioural errors and information provision}\label{behavioural-errors-and-information-provision}}

Figure \ref{fig:nolosssurplus} suggests a paradoxical issue when behavioural errors are made and information is provided to consumers or producers in order to help them to make more rational choices: economic surplus can go down. The popular view of behavioural policy intervention interprets errors as a particular form of externalities as they lead to a sub-optimal demand choice in markets (Chetty (2015), figure 9).

As the impacts of behavioural error do not fall upon other actors in and outside the market, but can be viewed as internal costs for the market actors themselves, these are usually referred to as \emph{internalities}. Examples of internalities can be health effects related to cycling, smoking, drinking alcohol or eating meat. When individuals underestimate the impacts of their consumption on their own health, this is a negative internality. One of the policy interventions to combat internalities is the provision of information (cognitive interventions), the change of feelings about the product (affective interventions) and programs to change behaviour (behavioural interventions).

Suppose information is provided to firms in Figure \ref{fig:nolosssurplus} and the decision inverse supply curve becomes equal to the experienced inverse supply curve. The result of this is that equilibrium decision demand \(Q_d^*\) will move upwards. Because equilibrium demand then moves further away from the socially optimal demand, an economic surplus loss will occur.

A similar result is obtained for providing information to consumers: when consumers in Figure \ref{fig:nolosssurplus} become perfectly rational due to information, equilibrium decision demand becomes lower than the optimal demand. For the case shown in Figure \ref{fig:nolosssurplus}, this results in an economic surplus \emph{loss} compared to the situation without a policy. This somewhat paradoxical result only occurs when there are behavioral errors at the demand \emph{and} the supply side. When errors are made only by producers or only by consumers, providing (costless) information is beneficial because equilibrium decision demand moves into the direction of the optimal demand. We can summarize this analysis in the following observation about behavioural error internalities:

\emph{Costless information increases economic surplus when it moves equilibrium demand closer to the optimal demand. This is always the case when only one side of the market makes behavioral errors (either consumers or producers).}

There is of course a key normative discussion point here that requires attention. There might be an intrinsic value of providing information for more rational choices. This intrinsic value is now excluded from the analysis. However, when one accepts that this intrinsic value is utilitarian, it can be included as a benefit that adds positively to economic surplus when information is provided. This might result in provision of information even when this does not enhance economic surplus.

\hypertarget{pricing-of-internalities}{%
\subsection{Pricing of internalities}\label{pricing-of-internalities}}

Besides information provision, one can also use a price instrument to regulate internalities. This price instrument requires input on the relationship between the decision and the experienced demand and supply curves. Suppose we can define the decision inverse demand curve as a proportion of the experienced inverse demand curve: \(p_d (Q)= b_c p_e (Q)\). Section \ref{choicemodelserrors} gave a mathematical behavioural underpinning for such a proportional relationship. The parameter \(b_c\) has a useful interpretation: when behavioral errors result in an inverse demand curve with (on average) 10\% higher maximum willingness to pay for consumers, \(b_c=1.1\). Similarly, we can write \(s_d (Q)= b_p s_e (Q)\) for producers, where \(b_p\) indicates how strong the shift in the experienced marginal costs is due to behavioral errors. For example, when \(b_p=1.05\), producers overestimate the marginal costs of production by 5\%.

Now suppose a regulator introduces a consumer tax \(\tau\) in order to regulate error internalities. The regulator seeks to optimize economic surplus subject to the constraint that marginal decision benefits are equal to the supply price plus the consumer tax.

Errors will impact the equilibrium demand via the equilibrium conditions. Optimizing economic surplus using Lagrangian techniques results in a tax equal to (see Appendix \ref{apperror}):
\begin{equation}
\tau = p^{*R} \frac{b_c - b_p}{b_p}.
\label{eq:opttau}
\end{equation}
In this expression, \(p^{*R}\) is the \emph{equilibrium supply price} in the regulated equilibrium. This supply price does not include the tax. For the case of constant marginal costs, firms do not adjust their price after the tax is introduced and \(p^*=p^{*R}=MC\). The consumer price paid is equalto \(p^{*R} + \tau\). The tax is therefore proportional to the equilibrium supply price and can deal with all four conceptual cases discussed in section \ref{totalsurplus}. Three numerical examples are provided below.

Suppose consumers overestimate benefits with 10\% and producers are perfectly rational. Marginal costs are independent of the quantity and equal to \(p^*=p^{*R}=MC= 10\) euros. Then the tax will be equal to \(\tau=p^{*R} \frac{1.1-1.0}{1.0}=10 \frac{1}{10}=1\) euro. This tax is equal to 10\% of the equilibrium supply price in the regulated equilibrium.

When producers overestimate marginal costs with 5\% and consumers overestimate marginal benefits with 10\%, the tax will be equal to \(\tau=p^{*R} \frac{1.1-1.05}{1.05}=10 \frac{0.05}{1.05}=0.47\) euro. This tax is equal to 4.7\% of the equilibrium supply price in the regulated equilibrium.

Suppose, producers underestimate the marginal costs of doing research with 10\% and consumers overestimate the benefits with 5\%. This results in a tax equal to: \(\tau=p^{*R} \frac{1.05-0.90}{0.90}=10 \frac{1}{6}\), which is about 17\% of the equilibrium supply price in the regulated equilibrium. These three examples show how to do \emph{what-if} analysis when the exact size of behavioural error is unknown.

Table \ref{tab:beherror} gives an overview of the cases discussed in section \ref{totalsurplus}, the parameter assumptions and the implications for the sign of the tax. When the sign is negative a subsidy is justified to increase demand. For Cases I and IV, both a tax or a subsidy can apply depending on the sign of the difference \(b_c-b_p\). This is because equilibrium demand can be lower, higher or equal to the optimal demand. For Case II there has to be a subsidy in order to move the equilibrium demand closer to the optimum. Case III shows the opposite result and always results in a tax. As discussed before, cases I and II are more likely to occur than cases III and IV.

\begin{table}

\caption{\label{tab:beherror}Behavioral error and the sign of an error internality tax}
\centering
\begin{tabular}[t]{llll}
\toprule
Case & Case description & Parameter assumptions & Sign of the tax\\
\midrule
I & Producers overestimate marginal costs and consumers overestimate marginal benefits; & $b_c>1; b_p>1$ & Positive when $b_c>b_p$
 
Negative when  $b_c<b_p$

Zero when $b_c=b_p$\\
II & Producers overestimate marginal costs and consumers underestimate marginal benefits; & $b_c<1; b_p>1$ & Negative\\
III & Producers underestimate marginal costs and consumers overestimate marginal benefits; & $b_c>1; b_p<1$ & Positive\\
IV & Producers underestimate costs and consumers underestimate marginal benefits; & $b_c<1; b_p<1$ & Positive when $b_c>b_p$
 
Negative when  $b_c<b_p$

Zero when $b_c=b_p$\\
\bottomrule
\end{tabular}
\end{table}

For the analysis the assumption is made that the parameters \(b_c\) and \(b_p\) do not change when a particular policy is implemented. A behavioral interpretation could be that consumers and producers are not aware of the behavioral decision errors they make. When the parameters \(b_c\) and \(b_p\) are behavioral choice parameters things become more complicated as consumers and producers then optimally gather information themselves to reduce their behavioural errors. Nudges, information provision and taxes might have an impact on this behavioral process leading to more complicated tax expressions.

\hypertarget{behavioral-errors-and-pricing-of-consumption-externalities}{%
\section{Behavioral errors and pricing of consumption externalities}\label{behavioral-errors-and-pricing-of-consumption-externalities}}

\hypertarget{introduction-9}{%
\subsection{Introduction}\label{introduction-9}}

Consumption externalities occur in markets when the experienced marginal social costs are higher than the experienced marginal costs. There can be so-called negative or positive side effects of consumption that are not captured in the market behavior of producers and consumers. The negative side effects usually gain most attention by policy makers.

Suppose external costs are given by \(e(Q)\) and marginal external costs by \(MEC \equiv \frac{\partial e(Q)}{\partial Q}\). Then marginal experienced social costs are given by \(s_e (Q)+MEC\). A tax is then justified in order to move to the socially optimal equilibrium, where marginal experienced consumer benefits are equal to marginal experienced social costs. But before analyzing this we ask ourselves the question: how do behavioral errors impact economic surplus losses due to external costs?

\begin{figure}
\includegraphics[width=12.92in]{./figures/econsurplusexterrors} \caption{economic surplus losses, externalities and behavioral errors}\label{fig:econsurplusexterrors}
\end{figure}

Figure \ref{fig:econsurplusexterrors} indicates the situation where marginal costs are overestimated by firms as \(s_d (Q)>s_e (Q)\). Consumers are assumed to be fully rational. Equilibrium demand in the unregulated equilibrium is below the socially optimal demand \(Q_s^*\). Therefore, the red area indicates the potential economic surplus gain from a taxation policy that results in real marginal social costs equal to marginal private experienced benefits.

A policy that leads to better information for suppliers will bring the decision supply curve \(s_d (Q)\) closer to the experienced supply curve \(s_e (Q)\). This leads to \emph{higher} equilibrium consumption. Because of the external costs this will lead to an additional economic surplus \emph{loss} equal to the orange area. An internality and an externality interact with each other: behavioral errors and the marginal external costs. This might lead to suboptimal policies when one of these effects is ignored when formulating a policy.

Figure \ref{fig:figure7} shows another conceptual case where equilibrium demand is higher compared to the fully rational case. In this figure it is assumed that producers behave fully rational and consumers overestimate the marginal benefits of consumption. Information provision to consumers can lead to an economic surplus gain equal to the orange area. An \emph{additional} policy that taxes the externality will lead to an economic surplus gain equal to the red area. Internalities and externalities can also be regulated with a single tax. Such a policy would lead to an economic surplus gain equal to the red plus the orange area.

\begin{figure}
\includegraphics[width=11.86in]{./figures/figure7} \caption{economic surplus losses, externalities and behavioral errors}\label{fig:figure7}
\end{figure}

Ignoring behavioural errors in decision making leads to potential loss of economic surplus. There are three possible conceptual situations each having particular implications for the assessment of policy success.

First, there is the potential to obtain a higher gain in economic surplus as the regulated equilibrium quantity ends up to be lower than the socially optimal quantity \(Q_s^{**} >Q_s^*\). The tax could have been set higher, but because of the regulator did not account for the behavioural errors it is lower than optimal. Such a result is in general acceptable as the policy moves demand closer to the social optimum.

Second, it could be that the regulated equilibrium quantity is higher than the socially optimal equilibrium quantity: \(Q_s^{**} <Q_s^*\). The tax is too high and this will likely result in opposition.

Third, there can be cases where a tax is not beneficial at all because behavioral errors lead to the socially optimal demand: \(Q_s^{**} - Q_s^*\) Behavioral errors than coincidentally result in equilibrium demand equal to socially optimal demand.

This discussion suggests that there are strong qualitative similarities with the discussion in section \ref{totalsurplus} (around Figure \ref{fig:losssurplus}). In Figure \ref{fig:losssurplus} it was found that policies that move equilibrium demand closer to the demand under perfect rationality increase economic surplus. This qualitative conclusion remains true in the presence of consumption externalities, but now optimal (social) demand \(Q_s^*\) is the relevant benchmark. When externality taxes move the equilibrium demand closer to \(Q_s^*\) these taxes are beneficial for society (according to the model).

\hypertarget{a-combined-externality-internality-tax}{%
\subsection{A combined externality-internality tax}\label{a-combined-externality-internality-tax}}

We can approach the analysis in an analytical way by optimizing the economic surplus function (including external costs \(e(Q)\)) with respect to the tax. Let \(MEC=\frac{\partial e(Q)}{\partial Q}\) be the external costs for example related to pollution. The tax then seeks to correct simultaneously for the internality and the externality. Equilibrium is determined by the intersection of the decision inverse demand and the decisions inverse supply curve. The optimal tax is then given by:

\begin{equation}
\tau = p^{*R} \frac{b_c - b_p}{b_p} +b_c MEC.
\end{equation}

The first part is isomorphic (`equal of form') to the earlier derived internality tax expression of Eq. \eqref{eq:opttau}. In the presence of external costs an additional term is added to this tax expression which multiplies \(b_c\) with the marginal external costs.

When consumers overestimate marginal benefits, \(b_c>1\), the recommended tax is higher compared to the case of perfectly rational consumers. The reason is that the observed equilibrium quantity is below the quantity under perfect rationality.
When consumers underestimate marginal benefits, \(b_c<1\), the externality tax should be lower compared to the perfectly rational case. Behavioural errors of suppliers do not have a direct impact on the second part of the tax expression. When the tax is targeted at suppliers instead of consumers this would not be the case.

\hypertarget{discussion-and-conclusion}{%
\section{Discussion and conclusion}\label{discussion-and-conclusion}}

There are enough examples from the recent literature that market actors can make valuation errors or use heuristics when they are making choices. The lessons of behavioural economists are therefore important inputs for those who work on applied economic policy analysis. Economists who seek to make assessments have the task to investigate whether their assessment and policy recommendations are robust for behavioural errors. Asking consumers whether they consume the right amount of a particular good upon reflection in surveys might help to gain information about the size of behavioural error biases.

The discussion in the previous sections has showed under fairly general assumptions what the implications of behavioural errors in partial equilibrium could be. However, still input is needed on the experienced supply and demand curves. If it is not possible to measure these curves, one still cannot derive the economic surplus effects of policies without making additional assumptions about the approximate proportional relationship between the decision and experienced demand and supply curves.

In order to develop stylized cases, this chapter assumes proportional behavioural errors which are helpful for interpretation and rough quantitative estimates of how behavioural errors impact recommendations. In the absence of empirical information, such a \emph{what-if} analysis is often the best option available for practical economists doing cost-benefits analysis for public policy.

For externality tax recommendations the sign of the impact of behavioural errors on externality taxes is not clear beforehand. This implies that economists should be cautious in their recommendations when knowledge and evidence about the size and sign of behavioural errors is lacking for particular markets, unless one is willing to assume that the neo-classical model is the correct model for a particular market by default.

As section \ref{arguments} shows, from a normative perspective the regulation of internalities is still controversial as there can be many objections to the idea of regulating and observing structural behavioural error that are counterintuitive at first sight. For example, should a meat tax include the potential health effects or not when individuals deliberately make their choices? Should cycling be subsidized when there are also accident externalities present? Again, the use of surveys to investigate perceptions of what is internal and external for consumers can help here. Furthermore, analysis of the acceptance of particular policies targeted at internalities might be useful. The case of smoking might provide a good benchmark example of regulating internalities where freedom of choice is limited by addiction to the market good. Whether such an argument is also applicable for other market goods is open for investigation.

\hypertarget{moral}{%
\chapter[Moral Considerations and Economic Surplus ]{\texorpdfstring{Moral Considerations and Economic Surplus\footnote{These lecture notes are based on the paper `Moral Considerations and Economic Surplus' (working paper). Please do not distribute these lecture notes without permission.} \footnote{The author gratefully acknowledges the contribution of the Dutch Organization for Scientific Research (NWO Responsible Innovation grant---313-99-333). Furthermore, the content of these lecture notes benefited from the authors' participation in a seed money project from the \href{https://www.agci.vu.nl/en/}{Amsterdam Sustainability Institute}. I am indebted to many people inside and outside academia for their invaluable thoughts and responses.}}{Moral Considerations and Economic Surplus }}\label{moral}}

\hypertarget{introduction-10}{%
\section{Introduction}\label{introduction-10}}

These lecture notes introduce another definition of economic surplus (`the economic pie') that allows for moral considerations. It specifically investigates moral considerations related to the normative validity of counting costs and benefits of consumers and producers and external costs as economic social surplus. Why is this important? In the first week we have seen that the classical perspective of economic value rests on four pillars: (\emph{i}) optimisation, (\emph{ii}) equilibrium, (\emph{iii}) empiricism and (\emph{iv}) choice-based normativity. The implication of this perspective is that choices of market actors perfectly track value for society.

The second week provided an extended perspective using insights from psychology. According to this perspective there can be a difference between choice-based value and experienced based value. Given particular assumptions it can be justified that experienced value is the right metric to measure instead of choice-based value. As we have seen, this has implications for policy interventions related to information provision and externality taxation.

This chapter questions the results of chapter \ref{surplus} from a different angle: there can be moral considerations related to the choices of market actors that lead to the question whether it is justified to include the value of consumers, producers and the external costs completely in the economic surplus calculation.

Before moving on, it is useful to illustrate why moral considerations are important. Consider a pork producer who is active in the market for pork. The government has set specific rules for pork production for those who want to become active in this market. With the aim of lowering marginal costs, the pork producer ignores some of these rules related to animal welfare without providing good reasons and without changing external costs. The producer rationally trades off lower marginal costs with the probability of getting a fine when being caught. According to the economic model of value developed in chapter \ref{surplus} this leads to an \emph{increase} in producer surplus in the market of pork and thereby into an increase is economic surplus. The example suggests that there are normative limits for behaviour by consumers and producers in markets that might not be captured in behavioural choice.

Or consider a philosopher of animal welfare who considers the eating of pork to be morally problematic because pigs are sentient beings `like us'. If the reasoning of this philosopher is correct, counting consumer benefits fully as social surplus is likely to be questionable.

These two examples at least illustrate that the choice-based normativity perspective (chapter \ref{surplus}) and experienced-based normativity (chapter \ref{erroreconsurplus}) might not be shared by all persons and groups in society. In these lecture notes we therefore change the 4th pillar `normativity' from choice-based to morally-based evaluation.

How to deal with this in cost-benefit analysis?

The key question to investigate here is: ``What should count as `better or worse off'?'' (Atkinson 2009; Sandel 2013). This normative question can be investigated from different viewpoints: the individual level, the community level and the societal level.

At the \emph{individual level}, consumers and producers make moral commitments to their own perspective on the good life. They might want to eat less meat, fly less and might want to lower their carbon emissions by making more sustainable choices. However, these moral commitments and targets are not always reached leading to a discrepancy between the individuals' perspective on the good life and their actual choices. Most persons are aware of this gap between their goals and actual behaviour in their personal lives. For example, sometimes individuals make promises to particular sustainability targets to others but do not keep these promises. Another example is the observation of flight shame which suggests that individuals have an independent reflective moral attitude (conscience) towards their own behaviour. Even when choices (Chapter \ref{surplus}) and experiences (Chapter \ref{erroreconsurplus}) are valuable and decisions are made to buy the product, this suggests that these choices might not be evaluated as truly `good' by the individual.

At the \emph{community level}, individuals participate in communities who determine (partly or fully) what counts as valuable. Community membership can be voluntary or non-voluntary and membership of communities results in particular duties and customs. Communities entail particular minimum standards of behaviour for consumers and producers based on the community values. These standards can be viewed as partly independent of the preferences that are expressed by consumers and producers in their day-to-day market choices. For example, an activist participating in a `climate change community' is likely to be held accountable for behavioural choices: only `telling the truth' is likely not sufficient according the community standards.

At the \emph{societal level}, individuals participate in the common project of living together. Governments can formulate particular minimal sustainability standards for market participants which in turn can lead to particular duties and customs for market actors. This was illustrated with the example of the pork producer above. Furthermore, governments can ask market participants to make concrete commitments in terms of sustainability. Goals can act as targets for moral excellence and result in obligations.

In order to count something as valuable for society these different viewpoints can play a role and can result in a discrepancy between choice-based value and morally weighted social value. In line with the results of Chapter \ref{erroreconsurplus} this can imply that the social optimum is not reached even when there are no external costs. Note that the different viewpoints do not restrict choice of market actors but seek to foster a plural perspective on what should count as valuable.

The remainder of this chapter is structured as follows. First, Section II gives an overview of several strategies that are employed to deal with moral considerations when valuing market outcomes. This section motivates why an adjustment of economic surplus is needed when moral considerations are present. Section III develops a novel analytical specification of economic surplus and sections IV and V discuss the implications for pricing. Examples are provided along the way. Section VI concludes.

\hypertarget{dealing-with-moral-considerations-at-the-valuation-stage}{%
\section{Dealing with moral considerations at the valuation stage}\label{dealing-with-moral-considerations-at-the-valuation-stage}}

\hypertarget{introduction-11}{%
\subsection{Introduction}\label{introduction-11}}

This section discusses four general approaches to deal with moral considerations when analysing (changes) in economic social surplus. The first approach is the ethical checkbox approach (section II.B), the second approach is the economizing ethics approach (section II.C), the third approach is the ethicizing economics approach (section II.D). The fourth approach is the qualitative valuation approach (section II.E). These approaches are not necessarily perfect substitutes but can be employed in a complementary way.

\hypertarget{ethical-checkbox-approach}{%
\subsection{Ethical checkbox approach}\label{ethical-checkbox-approach}}

The first approach to deal with moral considerations is the `ethical checkbox' approach (ECH). This approach analyses moral considerations independently of economic surplus assessments using a list of \emph{necessary} ethical requirements or rules for the behaviour of market actors. It is closely related to the `independent doctrine' which views (micro-)economics as separate from ethics (see High 1985, 4--6, for a brief history). The advantage of the ECH approach is that it gives room to the realm of moral and ethical reasoning for guiding market practices. This circumvents that ethical considerations are fully overtaken by utilitarian and game-theoretic arguments of economists when policy recommendations are given. When moral considerations are of a different kind than (monetized) utilitarian consumer value or producer profits and transcend consumer and producer choice behaviour such a separation seems useful and justified. When the ethical checkboxes are ticked, micro-economists can employ the standard definition of economic social surplus which is---in the absence of externalities---assumed to be equal to the sum of consumer and producer surplus.

A disadvantage of the ECH approach is that it ignores the loss of economic social surplus resulting from consumers and producers in the market who do not act according to the ethical checkbox. The example of the pork producer who violated the rules illustrates this. Furthermore, the ECH approach also ignores other moral considerations that are not part of the checkbox but are nevertheless important for individual market actors, the communities they participate in or for society. This is because the standard specification of economic social surplus is based on choice-based normativity (pillar (\emph{iv})). The ECH approach assumes that costs and benefits should be counted without further normative consideration by the individual, the community or society. Consumer and producer moral responsibility do not enter the equations insofar this responsibility is not fully expressed in the particular market under consideration. The normative evaluation of public policies for markets is of course affected by this implicit assumption of moral \emph{excellence} of market actors. As a result, it is questionable whether the assumed economic social surplus function correctly measures economic social value in the presence of (remaining) moral considerations.

\hypertarget{economizing-ethics-approach}{%
\subsection{Economizing ethics approach}\label{economizing-ethics-approach}}

The second approach to deal with moral considerations when calculating economic social surplus is the `economizing ethics' approach (ECE). This approach views moral considerations as (monetized) utilitarian internalities or externalities and has been applied to analyse repugnant markets, morality and identity, public policy decisions with an ethical component and regulations in the social domain (Lazear 2000; Bnabou and Tirole 2011; Roth 2018; Chorus et al. 2018; Mouter, Koster, and Dekker 2021).

A good aspect of ECE is that it might enhance understanding of individuals choice behaviour when moral considerations for private or public decisions are present (Chorus 2015). Furthermore, it frankly acknowledges that ignoring moral considerations in markets can lead to a loss of utilitarian value for people inside and outside these markets. Posner and Sunstein (2017) (p.~1812) write:

\begin{quote}
Our simple answer, put too briefly, is that on welfarist grounds, moral commitments can matter, and that when people would suffer a welfare loss when their moral commitments are violated, regulators should ask: how much are people willing to pay to honor those commitments?
\end{quote}

And in another section of this paper Posner and Sunstein (2017) (p.~1840) write: ``\emph{Our major goal is to acknowledge rather than to resolve the measurement problem and to insist on the basic principle: people \textbf{experience} welfare losses from social outcomes that offend their moral commitments, even if those outcomes do not involve their own wealth or health. Private willingness to pay is the best way to measure those losses.}'' The ECE approach therefore assumes that all experiences should count including experiences of moral losses or gains by third parties due to the market choices of consumers and producers.

ECE might also explain descriptively why there are societal laws that put restrictions on market trade. This is because the loss of monetized moral value by third parties can go beyond the value of market transactions, but is related to the actual market behaviour of consumers, producers and the regulator. According to the ECE approach, including the monetized utilitarian externalities stemming from moral considerations thereby can lead to better estimates of (changes in) economic surplus. Furthermore, an advantage of the ECE approach for economists is that it can be operationalized using empirical choice data in combination with the micro-econometric toolbox of applied economists. It is possible to ask persons what their willingness to accept is for violating a particular moral consideration in order to measure the external effects.

A disadvantage of the ECE approach is that it often fails to make normative sense for assessments of decisions outside markets. Without acknowledging distinct social domains with their corresponding distinct core values, rules and customs, it feels like bringing the hockey-stick of economic efficiency to join a field where teams play basketball.

\hypertarget{ethicizing-economics-approach}{%
\subsection{Ethicizing economics approach}\label{ethicizing-economics-approach}}

The third approach is the `ethicizing economics' (ETE) approach which is explored in this paper. There are two different ways the ETE approach has been implemented in the literature. First, in the climate change literature, this approach has been pursued to deal with distributional effects using a novel specification of the social welfare function that employs a single ethical perspective such as prioritarianism instead of ECE (Spash 1997).

The second ETE approach which is pursued in these lecture notes, seeks to operationalise diverse normative ideas about what is socially valuable by re-specifying economic social surplus using ethical parameters. Such a normative \emph{diversity} might even be present at the level of the individual as a mixture of deontological, virtue and consequential arguments play a role in the reflection on behavioural choices of market actors (see Van Staveren 2007; White 2009). Economists are well equipped to study consequences, but other moral considerations related to duties and virtues have received less attention. What kinds of considerations can we think of?

Deontology is the study of duties:

\begin{quote}
The word deontology derives from the Greek words for duty (deon) and science (or study) of (logos). In contemporary moral philosophy, deontology is one of those kinds of normative theories regarding which choices are morally required, forbidden, or permitted. In other words, deontology falls within the domain of moral theories that guide and assess our choices of what we ought to do (deontic theories).\href{https://plato.stanford.edu/entries/ethics-deontological/}{Stanford Encyclopedia of Philosophy}
\end{quote}

Deontological moral considerations therefore are therefore related to what market actors ought (not) to do and not what they are actually doing. This type of moral considerations results in particular duties. For now, it is sufficient to notice that such moral considerations can exist for particular markets of interest.

Moral considerations related to virtue focus on moral character:

\begin{quote}
It may, initially, be identified as the one that emphasizes the virtues, or moral character, in contrast to the approach that emphasizes duties or rules (deontology) or that emphasizes the consequences of actions (consequentialism).\href{https://plato.stanford.edu/entries/ethics-deontological/}{Stanford Encyclopedia of Philosophy}
\end{quote}

Besides moral character, virtue ethics also focusses on motivations and intentions behind the choices of market actors. According to this perspective, choosing polluting goods for utilitarian reasons is not sufficient when there are no appropriate motivations or reasons given.

The framework for cost-benefit analysis developed in Chapter \ref{surplus} does not deal with these moral considerations and leaves out normative reflection at the individual, community and societal level in the calculation stage. For example, a virtue ethicist focussing on sustainability practices might want to count the benefits of flying to Barcelona for the funeral of a family member differently than the marginal benefits of a person who goes 5 times per year on holidays by air travel `just for fun'. Motivations can matter for counting benefits and costs according to this view.

The ETE approach acknowledges that the mathematical specification of economic social surplus is a moral enterprise itself and that it can impact economists' normative recommendations for optimal economic policies. Furthermore, the plural version of it acknowledges that there are multiple reasonable normative positions each resulting in a particular specification of the economic `pie'. Counting producer costs and benefits fully---as we did in week Chapter \ref{surplus}---is just one of the normative perspectives.

Working directly on the economic social surplus function seems to be more in line with the arguments developed by moral philosophers from schools and traditions that focus on virtues and duties. The intuition of the difference between the ECE approach and the ETE approach is illustrated by structuring the argument. The ECE approach proceeds as follows:

\begin{enumerate}
\def\labelenumi{(\Alph{enumi})}
\tightlist
\item
  Consumer choices of market good \(Q\) are morally questionable for sound moral considerations \(R\).
\item
  Consumer benefits for \(Q\) are not adjusted in the economic surplus.
\item
  Additional external costs are counted for violation of moral considerations \(R\) by consumers.
\end{enumerate}

This follows the ideas of Posner and Sunstein (2017) who propose to monetize external costs and results in a model that is equivalent to the model in Chapter \ref{surplus} with additional external cost terms.

The ETE approach instead, deals directly with the moral consideration when specifying the economic surplus:

\begin{enumerate}
\def\labelenumi{(\Alph{enumi})}
\tightlist
\item
  Consumer choices of market good \(Q\) are morally questionable for sound moral considerations \(R\).
\item
  Then in order to account for moral consideration \(R\) in studies of the market in \(Q\), it is reasonable to adjust the consumer benefits for \(Q\) downwards using an ethical parameter.
\end{enumerate}

Both approaches account for moral considerations but do this in a different manner depending on whether researchers find ECE or ETE more appropriate.

\hypertarget{qualitative-valuation-approach}{%
\subsection{Qualitative valuation approach}\label{qualitative-valuation-approach}}

The fourth approach to deal with moral considerations is the qualitative valuation (QUV) approach. For this approach the quantitative calculation of exact changes in economic surplus is less relevant. The QUV approach can highlight which values are relevant for normative policy decisions on markets or decisions in other spheres of society (Klamer 2017; Wempe and Frooman 2018).

It is valuable for at least six reasons. First, it can enhance reciprocal understanding of those involved in valuation by fostering the conversation about the values underlying regulatory and market choices. Second, it acknowledges that normative preferences for public policies can partly be socially constructed for example by reading, role switching or by a deliberative process. Third, it is helpful for articulating which moral considerations are relevant for market valuation and why. Fourth, QUV can be helpful for the selection of competent persons for policy decisions. Fifth, an advantage of the QUV approach can be that the conversation is open and not dictated by a pre-defined analytical valuation method. Sixth, qualitative motivations developed by participants in the QUV approach can be helpful for policy makers articulating their motivations for their public policies (see for example Mouter, Koster, and Dekker 2021, sec. 5).

A disadvantage of the QUV approach is that no \emph{quantitative} estimation of impacts and value are provided that allow for a comparison of different economic policy interventions in markets including the status quo of doing nothing. Therefore, quantitative questions such as: `how high should an externality tax on good \(X\) be?' or `what is the loss of economic social value of changes in the market for \(X\)?' cannot be answered using the QUV approach only. A second disadvantage is that it is harder to reach consensus, as heterogeneous opinions are sometimes hard to weigh when they are based on qualitative statements only. A third disadvantage is that the outcomes of the QUV approach depend on the quality of expression of the people who are involved. This might disadvantage those with less rhetorical and verbal qualities.

\hypertarget{contributions-of-the-remainder-of-this-chapter}{%
\subsection{Contributions of the remainder of this chapter}\label{contributions-of-the-remainder-of-this-chapter}}

There is no good reason to assume \emph{a priori} that the assumption of moral excellence of consumers, producers and regulators in markets is valid. Furthermore, different groups in society or different experts might define moral excellence in different ways, and in some markets the assumption of moral excellence might be more valid than in others. According to Sen (2000) (p.~952):

\begin{quote}
sensible cost-benefit analysis demands something beyond the mainstream method, in particular, the invoking of explicit social choice judgments that take us beyond market-centered valuation.
\end{quote}

The aim of the remainder of this chapter is to offer a novel specification for the economic pie that allows for diverse normative considerations related to economic social surplus. The methodological approach allows for the inclusion of three kinds of moral considerations. Moral considerations related to:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  marginal consumer benefits;
\item
  producer marginal costs;
\item
  external costs.
\end{enumerate}

In order to show the impacts for economic policy recommendations, the new conceptualisation of economic social surplus is applied to the optimal taxation of a consumption externality. Such an exercise is useful to gain intuition on how moral considerations and ethical parameters can impact standard economic policy recommendations. The externality taxes seek to efficiently price positive or negative side effects resulting from market transactions while still allowing for trade in the market. But what is defined as `efficient' of course depends on the definition of the economic pie by the researcher. Something, we already discovered in Chapter \ref{erroreconsurplus}.

\hypertarget{scope}{%
\subsection{Scope}\label{scope}}

Before moving on, it is good practice to think about the scope of the model: where can it be applied, and for which decisions can it not be applied? The model of this week is applicable for the analysis of the taxation of a consumption externality in existing markets with remaining moral considerations. It is not applicable for the analysis of whether a market should be opened or closed by a government. This class of policy decisions leads to other kinds of moral considerations related to offering choice opportunities. This raises fundamental freedom questions of which the analysis is beyond the scope of this academic course.

Furthermore, distributional concerns, price and tax discrimination, bounded rationality of market actors (Chapter \ref{erroreconsurplus}.), strategic firm interactions, limited information, general equilibrium and bargaining between consumers and producers are ignored. This is done for clarity of exposition and argumentation and to stay close to how practical cost-benefit is often operationalized.

\hypertarget{redefining-the-economic-pie}{%
\section{Redefining the economic pie}\label{redefining-the-economic-pie}}

\hypertarget{the-advisory-committee}{%
\subsection{The advisory committee}\label{the-advisory-committee}}

This section redefines the economic `pie' for a stylised partial equilibrium model of a single good or service mathematically and graphically. Suppose that an advisory committee is summoned that has \(N\) members. Assume that each member \(n\) of this advisory committee receives a weight \(\omega_n \geq 0\) and that these weights sum up to \(1\). Depending on the kind of market, the committee could consist of the full population of a region, city, country or the world, a group of representative elected officials, representatives of particular communities, representatives of responsible advisory institutions, a group of (scientific) experts or a weighted combination of all these groups. Membership of the committee can be based on experience, knowledge and expertise and/or on the degree that the expert represents a community or population of interest or simply on the status of being a citizen.

We will not discuss how this advisory committee \emph{should} look like, what the requirements of membership should be and what the procedural process of deliberation should be. The mathematical description also abstracts away from potential strategic considerations within the advisory committee.

It is assumed that all normative positions of the committee members are in principle correct or internally consistent and are based on correct information about the consequences of behaviour. This pragmatic assumption is inconsistent with the observation that in the presence of objective moral considerations there can be mutual exclusive positions where given the underlying (meta-)ethical assumptions the moral reasoning of only one or several of the committee members is correct.\footnote{See Singh et al. (2020) for a recent discussion on expert agreement, disagreement and the (non-)existence of objective moral facts.}

\hypertarget{adjusted-consumer-surplus}{%
\subsection{Adjusted consumer surplus}\label{adjusted-consumer-surplus}}

Total monetized consumer benefits for the market are denoted by \(\int_0^{Q^*}p(Q)dQ\) where \(Q\) is the demand for the good or service. The marginal consumer benefits are given by \(p(Q)\). This market inverse demand function reflects the maximum marginal willingness to pay for a group of anonymous consumers and is assumed to be independent of consumers' income levels. In equilibrium, marginal consumer benefits are equal to the private costs, resulting in an equilibrium demand of \(Q^*\). It is assumed that all consumers all have the same private access costs and pay the same equilibrium price \(p^*\) for the good or service. From a normative perspective the assessment of marginal consumption benefits of the consumer as reflected by the consumers' marginal willingness to pay deserves more investigation.

When there are moral considerations related to consumers' marginal benefits, member \(n\) of the committee can adjust the consumer benefits with an ethical weight \(0 \leq \alpha_n \leq 1\). This adjustment can be based on moral considerations and reasons related to consumer responsibility. Committees can use survey data and interviews to learn more about the reasons and intentions of consumers or investigate whether the consumption choice has been made under free conditions or is the result of addiction. The adjustment of marginal consumer benefits does not entail that choice \emph{should} be restricted or forbidden.

Moral considerations relating to consumer behaviour can lead to scaling down consumer benefits in particular markets. Eq. \eqref{eq:acs} adjusts the consumer surplus for moral considerations related to consumer marginal benefits:

\begin{equation}
ACS = \sum_{n=1}^N \omega_n \left(\int_0^{Q^*} \alpha_n p(Q)dQ - p^* Q^*\right) = \bar{\alpha} \int_0^{Q^*} p(Q)dQ - p^* Q^*,
\label{eq:acs}
\end{equation}
where \(\bar{\alpha} = \sum_{n=1}^N \omega_n \alpha_n\) is the weighted average sum over all committee members for the ethical parameters for consumer benefits.\footnote{An additional assumption is required for a valid model: at least one committee member needs to have \(\alpha_n>0\). It is not necessary to have a weighted average: a majority voting decision on the weights is also possible, although this fully leaves out the normative perspectives of particular minorities in the advisory committee.} In the presence of moral considerations, consumer benefits are therefore adjusted downwards. Adjusted consumer surplus estimates in markets with moral considerations will therefore decrease in comparison to the case where moral excellence of consumers is assumed (\(\bar{\alpha} = 1\)).

At this point it is helpful to provide an illustration how the model can be operationalized for practical policy analysis. Suppose an advisory committee on the economic value of the pork market is summoned. There are 10 representatives invited for this committee. Each of them has received an equal weight \(\omega_n=0.1\).

Table \ref{tab:representatives} below provides an overview of the ethical weights the representatives propose for the marginal benefits of consumers and their reasons for choosing these weights. It includes a wide range of voices and reasons from society, institutes and advocacy groups.

\begin{table}

\caption{\label{tab:representatives}Representatives and weights. Numbers are chosen for illustration.}
\centering
\begin{tabular}[t]{llll}
\toprule
Committee member & Decision weight & Recommended weight for consumer marginal benefits & Reasons\\
\midrule
Citizen representative. & $\omega_1 = 0.1$ & $\alpha_1 = 0.6$ & Using a survey, we estimated what the average citizen weight for consumer marginal benefits in the pork market is for our country. It is estimated at $\alpha_1=0.6$.\\
Next generation representative. & $\omega_2 = 0.1$ & $\alpha_2 = 0.5$ & Consumption of pork meat leads indirectly to sustainability impacts for future generations. Consumers have a duty of care for these generations and have the opportunity to choose otherwise. However, changing food habits can be difficult as we know from our own experience. Therefore, we set $\alpha_2=0.5$.\\
Consumer representative. & $\omega_3 = 0.1$ & $\alpha_3 = 1$ & Consumers have the possibility to choose healthy sustainable meat substitutes but they do not want them and do not value them high enough at the moment. There is no need to scale down their benefits as their choice is made under free conditions and there is no pork meat addiction.\\
Producer representative. & $\omega_4 = 0.1$ & $\alpha_4 = 1$ & We produce for the market and respond to consumers needs and wants. Consumers are free to change these needs and wants if they want to live up to their own moral standards. There is no need to adjust marginal consumer benefits.\\
Governmental representative (chair of the committee). & $\omega_5 = 0.1$ & $\alpha_5 = 0.6$ & As a government we want to be neutral among normative perspectives when evaluating policies. We therefore will take the average weights of the other committee members as they represent the different voices in society.\\
\addlinespace
Economist (specialization: environmental economics) & $\omega_6 = 0.1$ & $\alpha_6 = 1$ & No scaling of consumer benefits is needed because full information about sustainability impacts is available for quite some time. Therefore, it seems that the current direct utility function of consumers gives the most appropriate evolutionary social fit for consumers.\\
Philosopher (specialization: animal ethics) & $\omega_7 = 0.1$ & $\alpha_7 = 0.7$ & For those consumers who commit themselves to the no-harm principle in ethics and see no distinct difference between animals and humans, scaling down consumer benefits is reasonable. Research for our country shows that 30\% of the people commit themselves to the combination of these principles but still consume meat.\\
Ecologist (specialization: biodiversity) & $\omega_8 = 0.1$ & $\alpha_8 = 0.6$ & Consumers have a responsibility for harm against nature. The systemic and well-known impacts of climate change on biodiversity require a reconsideration of estimated consumer benefits in this market.\\
Climate activist representative. & $\omega_0 = 0.1$ & $\alpha_9 = 0$ & The time for action is NOW as human extinction is coming near. No consumer benefits should be counted in the social surplus as consumption of meat substantially contributes to climate change.\\
Animal representative. & $\omega_{10} = 0.1$ & $\alpha_{10} = 0$ & It is time for animal empathy. Imagine that you would be born in this world as a pig instead of a human? Would you find it appropriate to be eaten by another species for money?\\
\bottomrule
\end{tabular}
\end{table}

The average ethical weight for consumer marginal benefits is given by the following weighted sum:

\begin{equation}
 \bar{\alpha} = \sum_{n=1}^N \omega_n \alpha_n = 0.1 \sum_{n=1}^{10} \alpha_n = 0.6
\end{equation}

This shows that the average consumer weight results in counting 60\% of the consumer marginal benefits. According to this committee, the counted consumer benefits will therefore be 40\% lower compared to the model of Chapter \ref{surplus}. (Note that numbers and reasons are chosen for illustration).

It might be helpful for you to reflect on the reasons and their relationship to the ethical weights. Do you think the reasons behind the weights are convincing? Which expert is closest to your own opinion on the topic? Do you think there are other reasons that should be included? What do you think about assigning the experts an equal weight \(0.1\)?

Figure \ref{fig:moralconsiderations} summarizes this section with a graphical illustration of the impacts of averaged ethical weights for consumer surplus in markets. It shows that market equilibrium is established at the point where the inverse demand is equal to the supply price \(p^*\). In the presence of moral considerations related to consumer benefits, the inverse demand applied for evaluation shifts inwards with a factor \(\bar{\alpha} < 1\). As equilibrium demand remains at \(Q^*\), the counted total consumer benefits are equal to \(A+B\) and are \(C+D\) lower compared to the case \(\bar{\alpha} = 1\) which is the classical case of Chapter \ref{surplus}. Consumer surplus is given by \((A+B)-(B+D)=A-D\). This is lower compared to the consumer surplus calculation that ignores moral considerations (\(A+C\)).

\begin{figure}
\includegraphics[width=8.17in]{./figures/moralconsiderations} \caption{Moral considerations and adjusted consumer surplus}\label{fig:moralconsiderations}
\end{figure}

\hypertarget{adjusted-producer-surplus}{%
\subsection{Adjusted producer surplus}\label{adjusted-producer-surplus}}

Producer behaviour is described by a market inverse supply function \(s(Q)\) that shows the minimum price for which a total of Q units are supplied by a group of anonymous firms. The area under this inverse supply function up to the equilibrium demand gives the total costs for all firms in the market. It is assumed that fixed costs are zero. Given the assumptions made, the area above this inverse supply curve up to the equilibrium price then measures the total net profits in the market. As in the basic micro-economic model, firms are assumed to be price-takers and general equilibrium and potential market power considerations are ignored. Furthermore, it is assumed that costs for production inputs can be counted as real societal costs instead of transfers to other firms or workers.

Moral considerations related to the marginal costs can be related to the investments of firms to produce goods or services. When there are moral considerations related to capital costs, labour, natural resources, (raw) materials and information/data used for production, these producer costs can be adjusted upwards by committee members applying an ethical weight \(\delta > 1\).

Moral considerations related to capital costs can be present when it is unclear where the money comes from that is used to finance the business. When money comes from illegal or doubtful criminal sources or when there is evidence of financial fraudulent behaviour this gives a reason to adjust the marginal cost curve upwards (see Reurink (2018) for an overview of such moral considerations).

There are many examples of moral considerations to correct producer costs upwards related to labour. Firms might have lower production costs because they use forced labour or child labour of children who cannot go to school (probably indirectly via subcontracting). Suppose that the counterfactual option of fair production would be to produce at a marginal cost curve that is 5\% higher. When committee member n considers subcontracted forced and/or child labour to be morally problematic (s)he can adjust the total costs using \(\delta_n=1.05\). In line with the ETE approach, this correction does not include the external costs of forced labour but seeks to account for the intrinsic moral quality of the production process of the firms in the market that make use of this labour.

Moral considerations related to resources and raw materials can be important as well. For example, when materials of products cannot be re-used in the production process this might be viewed as intrinsically less valuable for ecological oriented committee members. Taking the moral considerations related to (iii) and (iv) into account, the adjusted (equilibrium) producer surplus can be written as:

\begin{equation}
APS = \sum_{n=1}^N \omega_n \left(p^*Q^* - \int_0^{Q^*}\delta_N s(Q) dQ\right)= p^*Q^* - \bar{\delta} \int_0^{Q^*} s(Q) dQ,
\end{equation}
where \(\bar{\delta} = \sum^N_{n=1}\omega_n \delta_n\) are the weighted average sums of the producers' related ethical weights over all representatives. Moral considerations lead to higher producer costs compared to the standard model of Chapter \ref{surplus}. In highly controversial markets there is no reason why this adjusted producer surplus cannot be negative.

At this point it is useful to illustrate the model in this subsection using an example. Table \ref{tab:scalingmarginal} summarizes the weights and the reasons of the representatives. All numbers are chosen for illustrative purposes.

\begin{table}

\caption{\label{tab:scalingmarginal}Scaling of marginal costs of production. All numbers are chosen for illustration.}
\centering
\begin{tabular}[t]{llll}
\toprule
Committee member & Decision weight & Recommended weight for consumer marginal benefits & Reasons\\
\midrule
Citizen representative. & $\omega_1 = 0.1$ & $\delta_1 = 1.1$ & Using a survey, we found out what the average citizen weight for producer marginal costs in the pork market is for our country. It is estimated at $\delta_1=1.1$.\\
Next generation representative. & $\omega_2 = 0.1$ & $\delta_2 = 1.1$ & Production of pork meat leads to sustainability impacts for future generations. Producers have a duty of care for these generations but have limited opportunities to switch jobs. Therefore, we set $\delta_2=1.1$.\\
Consumer representative. & $\omega_3 = 0.1$ & $\delta_3 = 1$ & Consumers have the possibility to choose healthy sustainable meat substitutes and production will follow demand. There is no need to scale up marginal producer costs.\\
Producer representative. & $\omega_4 = 0.1$ & $\delta_4 = 1$ & There are some issues with production and animal welfare, but that is normal in many markets. There is no need to adjust marginal costs.\\
Governmental representative (chair of the committee). & $\omega_5 = 0.1$ & $\delta_5 = 2.2$ & As a government we want to be neutral among normative perspectives. We therefore will take the average weights of the other committee members as they represent different voices in society.\\
\addlinespace
Economist (specialization: environmental economics) & $\omega_6 = 0.1$ & $\delta_6 = 1$ & No scaling is needed because full information about impacts is available. Therefore, it seems that the current marginal cost function gives the most appropriate evolutionary social fit for consumers. Producers already make a rational trade-off between following the rules and the probability to get caught.\\
Philosopher (specialization: animal ethics) & $\omega_7 = 0.1$ & $\delta_7 = 1.25$ & For those producers who commit themselves to the no-harm principle in ethics and see no distinct difference between animals and humans, scaling up marginal costs is reasonable. Survey research shows that no producers in the market commit themselves to the combination of these principles. However, 10\% of the producers produce at 20\% lower marginal costs because they do not follow the governmental rules of production. The marginal costs should therefore be scaled up with $1/0.8=1.25$.\\
Ecologist (specialization: biodiversity) & $\omega_8 = 0.1$ & $\delta_8 = 1.8$ & Producers have a responsibility for harm against nature. The systemic impacts of climate change on biodiversity require a reconsideration of marginal costs in this market. Producing meat in a biological sustainable way would imply marginal costs which are 80\% higher. Therefore, I recommend 
$\delta_8=1.8$.\\
Climate activist representative. & $\omega_9 = 0.1$ & $\delta_9 = 1.8$ & The time for action is NOW as extinction is coming near! Producing meat in a biological sustainable way would imply marginal costs which are twice as high. Therefore, I recommend $\delta_9=1.8$.\\
Animal representative. & $\omega_{10} = 0.1$ & $\delta_{10} = 10$ & It is time for animal empathy. Imagine that you would be born in this world as a pig instead of a human? Would you find it appropriate to be killed by your boss for money? The true benchmark here is the sustainable production of animal friendly LAB-meat which has marginal costs which are a factor 10 higher. Therefore, I recommend $\delta_10=10$.\\
\bottomrule
\end{tabular}
\end{table}

The average weight for the marginal costs is given by:

\begin{equation}
 \bar{\delta} = \sum_{n=1}^N \omega_n \delta_n = 0.1 \sum_{n=1}^{10} \delta_n = 2.228.
\end{equation}

This implies that counted marginal costs are about 223\% higher than the real marginal costs. It is useful for you to reflect on the different reasons behind the chosen ethical weights. Do you consider their reasoning to be valid? Are there other reasons that should have been brought to the table? Which representative most closely matches your own perspective?

\begin{figure}
\includegraphics[width=7.61in]{./figures/psmoral} \caption{Moral considerations and producer surplus.}\label{fig:psmoral}
\end{figure}

Figure \ref{fig:psmoral} summarizes this sub-section with a graphical illustration of the impacts of moral weights on the determination of producer surplus in markets. The figure shows that equilibrium is established at the point where the market inverse supply curve \(s(Q)\) is equal to the equilibrium price as firms with higher marginal production costs will not produce for a lower price. In the presence of moral considerations, the market inverse supply curve applied for evaluation shifts upwards with a factor \(\bar{\delta} > 1\). As equilibrium demand remains at \(Q^*\) regardless of how costs are counted, the counted producer costs in the economic social surplus are then \(E+G+H\). This is \(E+G\) higher compared to the case where \(\bar{\delta}= 1\). Total counted producer surplus is equal to \(F-E\). This is \(G+E\) lower compared to the neo-classical case.

A key difference with the model developed in Chapter \ref{erroreconsurplus} is that the producers do not adjust prices according to the normative standard that is set at the evaluation stage. Therefore \(s(Q)\) is used to determine the equilibrium price (and not \(\bar{\delta}s(Q)\)).

\hypertarget{adjusted-economic-surplus}{%
\subsection{Adjusted economic surplus}\label{adjusted-economic-surplus}}

In the absence of external costs, the adjusted equilibrium social surplus is given by the sum of the adjusted consumer and producer surplus:

\begin{equation}
  AES = ACS + APS = \bar{\alpha} \int_0^{Q^*} p(Q)dQ - \bar{\delta} \int_0^{Q^*} s(Q) dQ.
  \label{eq:adjsocsur}
\end{equation}

Eq. \eqref{eq:adjsocsur} shows that moral considerations bring down the consumer benefits and raise the producer costs. Figure \ref{fig:moralloss} shows the welfare loss that follows from the assumption of moral excellence of consumers and producers. Equilibrium demand \(Q^*\) results from the intersection of the inverse demand curve \(p(Q)\) and the inverse supply curve \(s(Q)\). \(Q^{**}\) would be the socially optimal quantity as it is determined by the intersection of weighted marginal consumer benefits and weighted marginal producer costs.

\begin{figure}
\includegraphics[width=7.5in]{./figures/moralloss} \caption{Welfare loss following from the moral excellence assumption}\label{fig:moralloss}
\end{figure}

The red `triangle' then indicates the loss of welfare due to the moral excellence assumption. As the demand in equilibrium is higher than optimal, there is a loss of surplus for the consumers between \(Q^*\) and \(Q^{**}\), as for these consumers counted consumer costs are higher than counted consumer benefits. The key insight can be summarized as:

\begin{quote}
According to the model, the assumption of moral excellence of market actors leads to structural overconsumption in markets as the socially optimal demand is always lower than the equilibrium demand in the unregulated situation.''
\end{quote}

The result in Figure \ref{fig:moralloss} is qualitatively different compared to behavioral welfare economic models with behavioral errors made by consumers and producers. These behavioral errors can lead to higher or lower `experienced' inverse demand curves and to higher orlower `experienced' inverse supply curves. As we have seen in Chapter \ref{erroreconsurplus}, this can result both in equal, lower or higher equilibrium demand than the socially optimal demand under perfect rationality. This is not the case for the conceptual model developed for this chapter: because of the moral excellence assumption optimal demand will always be lower than observed demand.

\hypertarget{taxation-and-moral-consideration}{%
\section{Taxation and moral consideration}\label{taxation-and-moral-consideration}}

Because market surplus is not equal to the socially optimal surplus a regulator can correct for this using a price instrument. Such a price instrument does not interfere with the freedom to consume or produce and does not have to be a substitute for using policy measures to combat remaining injustice in the market. The assumption that the ethical parameters are independent of the equilibrium price and quantity helps to obtain some stylized results.

Appendix \ref{apptax} shows that the equilibrium tax expression that optimizes the adjusted social surplus Eq. \eqref{eq:adjsocsur} is given by:

\begin{equation}
 \tau = p^{*R} \frac{\bar{\delta} - \bar{\alpha}}{\bar{\alpha}}.
   \label{eq:taxmoral}
\end{equation}

The first observation is that this tax is non-negative as the equilibrium price in the regulated equilibrium (\(p^{*R}\)) is positive and \(\bar{\delta} \geq 1 \geq \bar{\alpha} \geq 0\). In the presence of moral considerations, it is therefore economically justified to impose a positive tax that corrects for the fact that in the unregulated equilibrium, consumers and producers in the market ignore moral considerations. This observation is consistent with the discussion around Figure \ref{fig:moralloss}: socially optimal demand is below equilibrium demand in the unregulated equilibrium.

Second, because the ethical weights enter proportionally, the tax in Eq. \eqref{eq:taxmoral} is proportional to the equilibrium price \(p^{*R}\) in the regulated equilibrium. This result does not hold anymore when ethical weights are functions of the demand.

Third, moral considerations related to consumer benefits raise the tax as decreasing \(\bar{\alpha}\)\} is equivalent with shifting the adjusted market inverse demand curve inwards. The level of the tax increases in \(\bar{\delta}\)\}---as intuitively---moral considerations related to producer costs are equivalent with shifting the market inverse supply curve upwards, leading to a lower socially optimal equilibrium demand.

Fourth, a stylized example is useful to gain some intuition about the potential quantitative importance of moral considerations. Using the recommended ethical weights in Tables \ref{tab:representatives} and \ref{tab:scalingmarginal} results in a tax recommendation of:

\begin{equation}
\tau=p^{*R}  \frac{2.228-0.6}{0.6}=2.71 p^{*R)}.
\end{equation}

This is a substantial tax equal to 271\% of the equilibrium price, but based on the numbers chosen for illustrative purposes in Tables \ref{tab:representatives} and \ref{tab:scalingmarginal}. Moral considerations therefore potentially play a large role in policy decisions.

Fifth, the tax has a similar analytical structure as the tax developed in Chapter @ref\{erroreconsurplus\}. Assuming \(\bar{\alpha} = \frac{1}{b_c}\) and \(\bar{\delta} =\frac{1}{b_p}\), gives the result of week @ref\{erroreconsurplus\}.. The reason is that moral considerations scale the inverse demand and the inverse supply curve in as shown in Figure \ref{fig:moralloss} and behavioural errors scale these curves in a similar proportional way. Note however, that the reasons for doing so are entirely different. Furthermore, prices do not adjust to the ethical weights. This is different in Chapter @ref\{erroreconsurplus\} where prices adjust with the behavioural errors of consumers and producers.

\hypertarget{externality-taxation-and-moral-considerations}{%
\section{Externality taxation and moral considerations}\label{externality-taxation-and-moral-considerations}}

\hypertarget{moral-considerations-related-to-the-kind-of-externality}{%
\subsection{Moral considerations related to the kind of externality}\label{moral-considerations-related-to-the-kind-of-externality}}

The last category of moral considerations that this chapter deals with stems from the application to the taxation of externalities by a regulator: the kind of external effect determines whether it should be regulated using a tax.

This category of considerations does justice to the intuition and to discussions on good governance in political science: for some external effects imposing a tax is more reasonable than for others. Intrinsic considerations, social customs, governance goals, value and externality tolerance and moral reasons might therefore `overrule' (part of the) utilitarian external costs. As Medema (2007) (p.~339) discussing Mill writes:

\begin{quote}
Indeed, even where government intervention may be warranted, the appropriate form of such action will vary, depending on the \textbf{nature} of the market failure.'' (PK: bold emphasis added).
\end{quote}

Members of the advisory committee therefore have to decide on their weight for the external costs in the economic social surplus to do justice to these moral considerations. Define \(0 \leq \eta_n \leq 1\) as the weight attached to the externality. Then the weighted external costs over all committee members are given by \(\bar{\eta}e(Q)\), where \(\bar{\eta} = \sum_{n=1}^N \omega_n \eta_n\) is the average weight over all committee members. Again, it is assumed for simplicity that the ethical weights \(\eta_n\) of committee members do not depend on the equilibrium quantity. An example for the motivated choice of ethical weights by the representatives is given in Table \ref{tab:scalingexternal}.

\begin{table}

\caption{\label{tab:scalingexternal}Scaling of the external costs. All numbers are chosen for illustration.}
\centering
\begin{tabular}[t]{llll}
\toprule
Committee member & Decision weight & Recommended weight external costs & Reasons\\
\midrule
Citizen representative. & $\omega_1 = 0.1$ & $\eta_1 = 0.8$ & Using a survey, we found out what the average citizen weight for external costs in the pork market is for our country. It is estimated at $\eta_1=0.8$.\\
Next generation representative. & $\omega_2 = 0.1$ & $\eta_2 = 1$ & Production of pork meat leads to sustainability impacts for future generations. All external costs should be counted $\eta_2=1$.\\
Consumer representative. & $\omega_3 = 0.1$ & $\eta_3 = 0.5$ & External costs should be diminished by consumer and producer choices. However, it is fair when part of the external costs are incorporated in the price.\\
Producer representative. & $\omega_4 = 0.1$ & $\eta_4 = 0$ & In other markets external costs are allowed. Why is the pork market so special?\\
Governmental representative (chair of the committee). & $\omega_5 = 0.1$ & $\eta_5 = 0.81$ & As a government we want to be neutral among normative perspectives. We therefore will take the average weights of the other committee members as they represent different voices in society.\\
\addlinespace
Economist (specialization: environmental economics) & $\omega_6 = 0.1$ & $\eta_6 = 1$ & External costs should be dealt with as this is consistent with a utilitarian view on society.\\
Philosopher (specialization: animal ethics) & $\omega_7 = 0.1$ & $\eta_7 = 1$ & The utility of animal choices is not incorporated in the economic surplus. Therefore, external costs should be fully counted.\\
Ecologist (specialization: biodiversity) & $\omega_8 = 0.1$ & $\eta_8 = 1$ & Producers have a direct responsibility for harm against nature and consumers an indirect responsibility. External costs should be counted.\\
Climate activist representative. & $\omega_9 = 0.1$ & $\eta_9 = 1$ & The time for action is NOW as extinction is coming near! External costs should be counted fully to mitigate the negative side effects.\\
Animal representative. & $\omega_{10} = 0.1$ & $\eta_{10} = 1$ & From an animal perspective it is best to count external costs fully\\
\bottomrule
\end{tabular}
\end{table}

The average weight for the external costs is given by:
\begin{equation}
 \bar{\eta} = \sum_{n=1}^N \omega_n \eta_n = 0.1 \sum_{n=1}^{10} \eta_n = 0.81.
\end{equation}

This implies that counted external costs are about 19\% lower than the real marginal costs. Note that the numbers in Table \ref{tab:scalingexternal} are chosen for illustration.

\hypertarget{application-pricing-consumer-externalities-that-are-external-to-the-market}{%
\subsection{Application: pricing consumer externalities that are external to the market}\label{application-pricing-consumer-externalities-that-are-external-to-the-market}}

The insights of Sections III.A and III.B can be used to adjust the economic social surplus and to investigate the implications for the optimal taxation of consumption externalities. For simplicity it is assumed that these externalities do not affect the generalized prices of consumers in the market. Adding adjusted external costs leads to the following adjusted economic social surplus:\footnote{Eq. \eqref{eq:adjsocsur2} excludes monetized moral utilitarian external costs to remain consistent with the ethicizing economics approach as discussed in Section II. In principle these moral externalities could be added, but only for those members in the committee who consider the ECE approach to be more appropriate. This would result in a blended approach of the ECE and ETE approaches. For reasons of clarity and the argument given in Section II, this blended approach is not pursued in this paper.}

\begin{equation}
  AES = \bar{\alpha} \int_0^{Q^*} p(Q)dQ - \bar{\delta} \int_0^{Q^*} s(Q) dQ - \bar{\eta} e(Q^*).
  \label{eq:adjsocsur2}
\end{equation}

The implicit equilibrium expression for the first-best externality tax is given by (see Appendix \ref{apppricing} for the interested reader):

\begin{equation}
 \tau = p^{*R} \frac{\bar{\delta} - \bar{\alpha}}{\bar{\alpha}} + \frac{\bar{\eta}}{\bar{\alpha}}MEC,
   \label{eq:taxmoral2}
\end{equation}
where \(p^{*R}\) is the price in the regulated equilibrium. This tax expression has two analytical terms. The first component is equal to the tax Eq. \eqref{eq:taxmoral} derived in Section II.

The second term in Eq. \eqref{eq:taxmoral2} is related to the external costs. It multiplies the marginal external costs in the regulated equilibrium MEC with a factor \(\frac{\bar{\eta}}{\bar{\alpha}}\), which is equal to \(\bar{\eta}\) in the absence of moral considerations related to consumer benefits and governmental taxation. Suppose the external costs are fully counted implying \(\bar{\eta} = 1\). This results in the following simplified tax expression:

\begin{equation}
 \tau = p^{*R} \frac{\bar{\delta} - \bar{\alpha}}{\bar{\alpha}} + \frac{1}{\bar{\alpha}}MEC,
   \label{eq:taxmoral3}
\end{equation}

Counting consumer benefits for 50\% then implies a recommended externality tax which is 100\% higher compared to the recommended standard Pigouvian tax that internalizes marginal external costs. When giving recommendations for pricing consumption externalities, moral considerations can therefore interact in a non-linear way and potentially have substantial quantitative implications.

\hypertarget{conclusion-and-discussion}{%
\section{Conclusion and discussion}\label{conclusion-and-discussion}}

This chapter shows that moral considerations can have implications for economic policy valuation as these considerations go to the root of the normative matter: the specification of economic social surplus or the so-called economic `pie'. This specification is at the heart of the concept of economic efficiency operationalized in micro-economic policy analysis. When externalities are absent, Section III shows that---from an economic perspective---taxation is justified because `free' markets do not lead to the optimization of economic social surplus adjusted for moral considerations. This contrasts with the idea:

\begin{quote}
\(\ldots\) that competitive equilibrium is efficient since the time of Marshall (Lazear 2000, 102).
\end{quote}

Section IV extends the model to account for moral considerations related to external costs. This section also shows how the expressions for first-best Pigouvian consumption taxes are affected by moral considerations. When there are only moral considerations related to consumer benefits, this can have substantial impact on recommended externality taxes: half the consumer slice, twice the externality price. Moral considerations therefore interact with externality pricing and are not simply additive monetized external costs.

Ignoring moral considerations when calculating economic social surplus implicitly assumes moral excellence of consumers and producers. This leads to the ignorance of other normative perspectives in the valuation stage of a policy intervention in a market. This insight is especially relevant for the current use of practical cost-benefit analysis for public policies, because for these decisions moral considerations that transcend market valuations of consumers and producers are often relevant. Whether a proposed policy can be labeled as `efficient' or `optimal' therefore depends on the specification of the economic pie, which in turn depends on the underlying ethical assumptions that are made.

The proposed adjustment of economic social surplus in this paper is pragmatic in that it allows for a wide variety of normative views related to consumer, producer and governmental behavior. For the purpose of public policy evaluation such an impartial calculation of economic social surplus using a weighted average of normative perspectives seems reasonable, but of course this pragmatism is a normative perspective as well.

One could argue that in the proposed model non-utilitarian moral considerations related to duties of consumers and producers are implicitly monetized via the taxation rules. This is a valid concern, but comparatively speaking---and as stated in the introduction---the plural ethicizing economics approach is preferred over the approach that ignores these moral considerations altogether when calculating economic surplus (`ethical checkbox') or the approach that explicitly monetize the ignorance of these moral considerations (`economizing ethics') as it \emph{directly} deals with the moral considerations when specifying the economic `pie' without taking an external costs `detour'. Furthermore, the proposed calculation of economic surplus still allows for an ethical checkbox or other social and law considerations that can overrule or complement tax recommendations based on economic social surplus calculations. The use of moral externality taxes is therefore not a substitute for action to reduce grave and tragic injustice and external costs in markets.

A benefit of the approach is that room is given for considerations related to voice and exit (Hirschman 1970). `Exit' considerations are captured by market choices of consumers and producers. If they do not want to produce or consume the good they can choose something else. `Exit' is properly accounted for in the behavioural choices of consumers and producers. `Voice' considerations in our context are related to the different normative perspectives in society related to what should be counted as valuable. The ethical weights are helpful instruments to give different voices a place in the normative stage of the analysis.

\hypertarget{univariateregression}{%
\chapter{Regression Analysis in the Social Sciences}\label{univariateregression}}

\hypertarget{introduction-12}{%
\section{Introduction}\label{introduction-12}}

Why should we have something as applied econometrics in the social sciences? That is because we have theories as in Chapters (\ref{surplus})--(\ref{moral}) and those theories contain variables such as in the direct utility function of equation \eqref{eq:directutility}:

\begin{equation}
U(Q,G) = \frac{A}{\alpha}\left( 1- e^{-\alpha Q} \right ) + BG.
\end{equation}

Here, the quantities for the market good \(Q\) and the outside good \(G\) are considered to be \emph{known}---also often referred to as data. In theoretical work they are fictional or sometimes simulated. A certain amount of consumption of the market good and the outside good lead then some level of utility which economists want to maximise.

This second part of the course is therefore about using data to quantify (socio-economic) parameters. Moreover, we focus on measuring \textbf{causal} effects, instead of mere correlations. Note, that in an ideal world, we would like conducting experiments as to measure a causal relation of a phenomenon \(X\) on \(Y\). However, we almost always only have observational data on, for example demand and prices. Therefore, the second part of this course and syllabus deals with difficulties arising from using observational data to estimate these causal effects and to rewrite models as \eqref{eq:directutility} such that we can actually \emph{use} data to tease out values for---in this case---\(A\) and \(\alpha\).

This chapter is organised as follows. The next section addresses the problem of finding a \emph{relation} between some \(X\) and some \(Y\). Here, we follow an example from the well-known textbook of @stock2003introduction where we look at the relation between school class size and school class performance. At the same time, we also introduce some \texttt{STATA} commands. To do so, this section deals as well with the statistical framework that is needed for applied econometrics. Note that we assume that the reader already had a course in introductionary statistics and provide only the basic concepts most important for this course in Appendix \ref{appreviewstat}.

\hypertarget{secproblem}{%
\section{So, what is the problem?}\label{secproblem}}

As explained in the introduction above, applied econometrics aims to give the policy maker well-informed, and evidence based, values for variables she needs. She needs these variables basically for two separate things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Causal inference}: The policy maker wants to assess the effect of a change in one variable (typically called \(X\)) on another variables (often called \(Y\)).
\item
  \textbf{Prediction}: If you know what variable \(X\) is, what should \(Y\) then be?
\end{enumerate}

Nowadays, most applied econometric techniques are concerned with causal inference, not so much with prediction. Even more, the techniques often applied are beneficial for correct causal inference, but might harm prediction. However, see that without correct causal inference (so knowing the \textbf{true} causal effect) prediction is always cumbersome. That is why current methods such as machine learning methods first focus on finding the correct causal mechanism (even without sometimes specifying what they may be) and then optimize prediction.

Thus, finding (causal) mechanism helps the scientist or policy maker in assessing the outcomes of a particular (policy) intervention. In the economic realm one could think about trying to assess the following quantities:

\begin{itemize}
\tightlist
\item
  To what extent do people eat less meat if we increase the prices with 1\% (using a meat-tax)?
\item
  If we increase Dutch dikes with one meter, how much less flood risk will there be?
\item
  How much do classes perform better is we reduce class-sizes with one student?
\end{itemize}

\hypertarget{a-first-encounter-with-stata}{%
\subsection{\texorpdfstring{A first encounter with \texttt{STATA}}{A first encounter with STATA}}\label{a-first-encounter-with-stata}}

For this section, we will focus on the last question. And this is an important question for policy as teachers are costly, but parents value school performance very highly. To start answering this question we \emph{use} data. In \texttt{STATA} data is in a specific format, named \texttt{.dta}. Note that this format is not in a text format and cannot be read with the human eye. To start using \texttt{STATA} we first need to set the working directory to the appropriate directory\footnote{I suggest that for this---and every other course---you have a specific directory}. You can do this by using the dropdown menu \texttt{File} in \texttt{STATA} and click on \texttt{Change\ working\ directly} but you can also do this with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cd  }\StringTok{"/Users/tomba/Dropbox/Thomas/Colleges/M\&T\_AED\_2022/"}
\end{Highlighting}
\end{Shaded}

Note here that this file path is used on an Apple or Linux system. On a Windows system you need forward slashes. It is also good to have subdirectories in your project/course folder for e.g.~slides, data and tutorials. Now suppose you have the subdirectory \texttt{data} in your course folder\footnote{Often it is as well advisable to make a distinction between \emph{derived} and \emph{raw} data. Raw data is original data and derived data is data you have transformed or worked on. In principle, you do not want to change the original data!} and in that data directory you have a file called \texttt{caschool.dta}.\footnote{This dataset can be downloaded from Canvas.}

To import the data in STATA you make use of the \texttt{use} command (again, you can make use of the dropdown menus), as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{use} \StringTok{"./data/caschool.dta"}\NormalTok{, }\KeywordTok{clear}
\end{Highlighting}
\end{Shaded}

Note that an option is added using the syntax \texttt{,\ clear}. The comma indicates that an option is expected and the verb \texttt{clear} indicates that memory of \texttt{STATA} should be cleared from data. That is because, \texttt{STATA} can only have one dataset in its memory.

In addition, we would like to know how the dataset looks like, for example what kind of variables it contains. We do this by invoking the command \texttt{describe}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{describe}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Contains data from ./data/caschool.dta
 Observations:           420                  
    Variables:            18                  20 Feb 2017 13:10
-------------------------------------------------------------------------------------------------------------------
Variable      Storage   Display    Value
    name         type    format    label      Variable label
-------------------------------------------------------------------------------------------------------------------
observat        float   %9.0g                 
dist_cod        float   %9.0g                 
county          str18   %18s                  
district        str53   %53s                  
gr_span         str8    %8s                   
enrl_tot        float   %9.0g                 
teachers        float   %9.0g                 
calw_pct        float   %9.0g                 
meal_pct        float   %9.0g                 
computer        float   %9.0g                 
testscr         float   %9.0g                 
comp_stu        float   %9.0g                 
expn_stu        float   %9.0g                 
str             float   %9.0g                 
avginc          float   %9.0g                 
el_pct          float   %9.0g                 
read_scr        float   %9.0g                 
math_scr        float   %9.0g                 
-------------------------------------------------------------------------------------------------------------------
Sorted by: 
\end{verbatim}

This provides information about the amount of observations and variables and the names and types of variables. In this case variables are either a float (a real number) or a string (text). Note as well, that this kind of output is cumbersome and ugly and not fit directly for reporting. Later, we will try to make this look better in an automatic way.

The command \texttt{summarize} gives descriptive statistics. Suppose that in this case we are only interested in the variables \texttt{testscr} (average testscore by district) and \texttt{str} (the student-teacher ratio by district). Then we invoke this by:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summarize}\NormalTok{ testscr str}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    Variable |        Obs        Mean    Std. dev.       Min        Max
-------------+---------------------------------------------------------
     testscr |        420    654.1565    19.05335     605.55     706.75
         str |        420    19.64043    1.891812         14       25.8
\end{verbatim}

Now we see descriptive statistics for two variables, containing number of observations, the mean, the standard deviation and the minimum and maximum of each variable. For first insight in the relation between class size and class performance we might want to draw a so-called scatterplot. These type of plots relate the values of two variables in a two-dimensional way by giving the values as coordinates. The following syntax will do so.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{scatter}\NormalTok{ testscr str}
\end{Highlighting}
\end{Shaded}

And this provides the following \texttt{STATA} output.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/scatter} 

}

\caption{A scatterplot with tests-cores on the $y$-axis and student-teacher ratio on the $x$-axis}\label{fig:scattercaschool}
\end{figure}

This ``cloud'' of dots do not yield a clearly visible relation between class performance and class size. However, this can be deceptive. Often it is difficult to discern clear relations from raw data only. Therefore we need to resort to numerical evidence.

\hypertarget{sec:numevidence}{%
\subsection{Numerical evidence}\label{sec:numevidence}}

To assess whether there is a relation between class performance and class size as diplayed in Figure \ref{fig:scattercaschool} we need numerical or statistical evidence. Before we start to engage in regression analysis, we first perform a rather simple analysis, but the underlying mechanisms are identical to that of regression analysis. We first create two groups: namely, districts with ``small'' (STR \(<\) 20) and ``large'' (STR \(\geq\) 20) class sizes. Then we can adopt three relatively straightforward strategies here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimation

  \begin{itemize}
  \tightlist
  \item
    Here, we compare the average test scores in districts with low student-teacher ratios to those with high student-teacher ratios. So, we basically try to assess whether \textbf{average} behaviour is different.
  \end{itemize}
\item
  Hypothesis testing

  \begin{itemize}
  \tightlist
  \item
    Now, we aim to \textbf{test} the ``null'' hypothesis that the mean test scores in the two types of districts are the same, against the ``alternative'' hypothesis that they differ.
  \end{itemize}
\item
  Confidence intervals

  \begin{itemize}
  \tightlist
  \item
    This strategy estimates an interval for the \textbf{difference} in the mean test scores, so small versus large student-teacher ratio districts.
  \end{itemize}
\end{enumerate}

In \texttt{STATA} this data analysis would look like:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{generate}\NormalTok{ large = (str \textgreater{}= 20)}
\KeywordTok{tabulate}\NormalTok{ large, }\KeywordTok{summarize}\NormalTok{(testscr) }\KeywordTok{mean}\NormalTok{ standard }\KeywordTok{obs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            |         Summary of testscr
      large |        Mean   Std. dev.         Obs
------------+------------------------------------
          0 |   657.35126   19.358012         238
          1 |   649.97885   17.853364         182
------------+------------------------------------
      Total |   654.15655   19.053348         420
\end{verbatim}

The first command \emph{generates} a new variable called ``large'' and denotes an indicator being 0 if \(str < 20\) and 1 if \(str \geq 20\). The second command summarizes the testscore variable again, but now only gives the mean, standard deviation and the number of observation and does this by each value of the new variable \texttt{large}. Of course, this output is rather ugly and it is better to make a nice table such as Table \ref{tab:smalllarge}.

\begin{table}

\caption{\label{tab:smalllarge}Descriptive statistics of small and large classes}
\centering
\begin{tabular}[t]{llll}
\toprule
Class size & Average score & Standard deviation & $n$\\
\midrule
Small & 657.4 & 19.4 & 238\\
Large & 650.0 & 17.9 & 182\\
\bottomrule
\end{tabular}
\end{table}

Now, for all three strategies (estimation, testing, confidence intervals) we want to know something about the difference---usually denoted as \(\Delta\). Or, specifically:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For estimation: determine the \(\Delta\) = difference between group means
\item
  For hypothesis testing: can we \emph{reject} the null-hypothesis that \(\Delta = 0\)
\item
  For confidence intervals: can we construct a confidence interval for \(\Delta\)
\end{enumerate}

\hypertarget{estimation}{%
\subsubsection{Estimation}\label{estimation}}

Estimation is rather straightforward as we need to calculate the \emph{difference} between the mean test scores within each group, or:

\begin{eqnarray}
\bar{Y}_{small} - \bar{Y}_{large} &=& \frac{1}{n_{small}} \sum_{i=1}^{n_{small}}Y_i - \frac{1}{n_{large}} \sum_{i=1}^{n_{large}} Y_i \\
&=& 657.4-650.0\\
&=&7.4
\label{eq:estimationlarge}
\end{eqnarray}

This basically mean subtracting the average scores of Table \ref{tab:smalllarge} (later we see how to this automatically in \texttt{STATA}). Now \(\Delta = 7.4\) and we then have to ask ourselves whether this is a large difference in a real-world sense? Note that testscores seem to range from 600 to 800 and do not really have a direct meaning for us. A useful trick then is to look at the standard deviation (note that if things are normally distributed, 95\% of all probability mass is within the range mean, plus or minus two times the standard deviation). In this case, the difference is about \(1/3\) of the standard deviation. A different way of looking at this is looking at the percentiles of testscores. In \texttt{STATA} this looks like:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tabstat}\NormalTok{ testscr, }\KeywordTok{statistics}\NormalTok{(p10 p25 p50 p75 p90)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    Variable |       p10       p25       p50       p75       p90
-------------+--------------------------------------------------
     testscr |   630.375       640    654.45   666.675     679.1
----------------------------------------------------------------
\end{verbatim}

where the command \texttt{tabstat} asks for a tabulation of certain statistics and \texttt{px} gives the \(x\)-th percentile of that statistic. Now note that between the 50th and 65th percentile there is only 12 points. So given that, the difference of \(7.4\) is rather sizable. But whether this difference is big enough to be important for school reform discussions, for parents, or for a school committee is a question we cannot answer with this analysis.

\hypertarget{hypothesis-testing}{%
\subsubsection{Hypothesis testing}\label{hypothesis-testing}}

We also test the null-hypothesis that the difference \(\Delta = 0\). For that we need a so-called difference-in-means test and compute the corresponding \(t\)-statistic\footnote{page 94 of syllabus statistics},
\begin{equation}
t = \frac{\bar{Y}_s - \bar{Y}_l}{\sqrt{\frac{s^2_s}{n_s} +\frac{s^2_l}{n_l} }} = \frac{\bar{Y}_s - \bar{Y}_l}{SE(\bar{Y}_s - \bar{Y}_l)}
\label{eq:testinglarge}
\end{equation}
where \(SE(\bar{Y}_s - \bar{Y}_l)\) is the \emph{standard error} of
\((\bar{Y}_s - \bar{Y}_l)\), the subscripts \(s\) and \(l\) refer to ``small'' and ``large'' STR districts, and \(s_s^2 = \frac{1}{n_{small}}\sum_{i=1}^{n_s}(Y_i - \bar{Y}_s)^2\)

We can compute this difference-of-means \(t\)-statistic by filling this in with the numbers of \ref{tab:smalllarge}:
\begin{equation}
t = \frac{\bar{Y}_s - \bar{Y}_l}{\sqrt{\frac{s^2_s}{n_s} +\frac{s^2_l}{n_l} }}  = \frac{657.4 - 650.0}{\sqrt{\frac{19.4^2}{238} +\frac{17.9^2}{182} }} = \frac{7.4}{1.83} = 4.05
\end{equation}

But then what? Well, recall that we \textbf{reject} a null-hypothesis when the critical value is below a certain threshold (usually 5\%). In this case that is equivalent with stating that \(|t|>1.96\). So, we reject (at the 5\% significance level) the null hypothesis that the two means are the same. We will come back to this procedure in Section \ref{sec:unitesting}.

\hypertarget{confidence-interval}{%
\subsubsection{Confidence interval}\label{confidence-interval}}

Finally, we can construct a 95\% confidence interval for the difference between the means, which is:
\begin{equation}
(\bar{Y}_s - \bar{Y}_l)\pm 1.96 \times SE(\bar{Y}_s - \bar{Y}_l) = 7.4 \pm 1.96 \times 1.84 = (3.7, 11.0)
\label{eq:cilarge}
\end{equation}
So what does this mean again. Well, two things. First, the 95\% confidence interval for \(\Delta\) doesn't include 0 and, second, the hypothesis that \(\Delta = 0\) is rejected at the 5\% level. We will come back to confidence intervals as well, but for now a confidence interval can be seen as an interval of numbers that will not be rejected as null-hypothesis.

\hypertarget{sec:smart}{%
\subsection{Always be smart (and a bit lazy)}\label{sec:smart}}

So, why give this rather simple procedure so much attention. That is because all ``classical'' statistics are constructed by these three elements and all statistical computer will always give, at least, these three. And they are as well very much related with each other. Once you know two of them, you know the third one as well.

Now, although the procedure is rather straightforward, it is also a bit cumbersome and prone to errors. Therefore, it is much easier to let \texttt{STATA} do it:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ttest}\NormalTok{ testscr, }\KeywordTok{by}\NormalTok{(large) unequal}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Two-sample t test with unequal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]
---------+--------------------------------------------------------------------
       0 |     238    657.3513    1.254794    19.35801    654.8793    659.8232
       1 |     182    649.9788    1.323379    17.85336    647.3676    652.5901
---------+--------------------------------------------------------------------
Combined |     420    654.1565    .9297082    19.05335    652.3291     655.984
---------+--------------------------------------------------------------------
    diff |             7.37241    1.823689                3.787296    10.95752
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =   4.0426
H0: diff = 0                     Satterthwaite's degrees of freedom =  403.607

    Ha: diff < 0                 Ha: diff != 0                 Ha: diff > 0
 Pr(T < t) = 1.0000         Pr(|T| > |t|) = 0.0001          Pr(T > t) = 0.0000
\end{verbatim}

So, in this case we want to assess the difference in test score by groups (being small and large classes), where we as well add the option \texttt{unequal}, which means that variance (or standard deviations) of both groups are unequal (and they are as Table \ref{tab:smalllarge} clearly shows).

Now try to find out for yourself that this output gives you the estimation of \(\Delta\) of Eq. \eqref{eq:estimationlarge}, the \(t\)-value and corresponding test outcome of Eq. \eqref{eq:testinglarge} with the corresponding confidence intervals of Eq. \eqref{eq:cilarge}.

\hypertarget{sec:uniregress}{%
\section{Univariate regression}\label{sec:uniregress}}

The three strategies we adopted in \ref{sec:numevidence} for assessing the difference between groups directly translate to the case of regression analysis. Here we also look at estimation, hypothesis testing and confidence intervals. But before that we first look at the origin of the name regression in Subsection \ref{sec:genesis}

\hypertarget{sec:genesis}{%
\subsection{\texorpdfstring{Genesis: \emph{regression towards the mean}}{Genesis: regression towards the mean}}\label{sec:genesis}}

The name regression seems to have a negative connotation, as progress is in general seen as good and regress as bad. And actually this is true, the name regression was deliberatily given as to describe a negative process: in full \emph{regression towards the mean}. The concept of regression was actually coined by Sir Francis Galton together with other statistical terms, such as correlation and deviation (Senn 2011). Galton was a notorious statistician who measured everything and else, including the length of french bread and the size of human skulls.

in 1886, Galton started to research the height of adult children with the height of their parents (Galton 1886). The original data can be seen in the scatterplot in Figure \ref{fig:galton2}. What Galton expected was that the relation between the height of children and that of their parents was a one-to-one relation. On average children should receive the same height of their parents. So, in fact he expected a \(45^{\circ}\) line---the red line; a line with slope equal to 1.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Galton2} 

}

\caption{Relation heigh fathers and height children}\label{fig:galton2}
\end{figure}

However, he found consistently the blue line, a line with positive slope but lower than 1 (the blue line in Figure \ref{fig:galton1}). That entails that, \emph{on average} tall parents get tall children but not as tall as themselves. Of course, this goes as well the other way. Short parents get short children but not as short as themselves.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Galton1} 

}

\caption{Relation heigh fathers and height children}\label{fig:galton1}
\end{figure}

Galton coined this process \emph{regression towards the mean}.\footnote{Modern statisticians actually see this as a form of shrinkage.} In the end we would all converge towards the mean and all look the same. For the Victorian Sir Frances Galton and his contemporaries in an age where income classes were highly separated this was truly a horror. Especially, because his cousin was Charles Darwin who actually claimed that species \emph{diverged}. Of course, in Galton reasoning there is a mistake as this only models genetic influence and not \emph{accidental} differences not influenced by genetics. Note that this analysis says something the average, but not about individual differences.

This regression towards the mean is now seen as a very important characteristics of regression models, and you can easily be fooled by it. It is now stated as:

\begin{quote}
\(\ldots\) a concept that refers to the fact that if one sample of a random variable is extreme, the next sampling of the same random variable is likely to be closer to its mean
\end{quote}

For instance, suppose that everything went really well for a course and you got a 9 for an examination. That does not mean that the next time you will do equally well (you will still do, but not that well). Or, your favorite footballclub does extreme well in a particular year (Leicester City FC comes to mind who became premier league champion in 2016). That does not mean that the next year it will do equally well, and so forth and so on.

\hypertarget{regression-with-one-regressor}{%
\subsection{Regression with one regressor}\label{regression-with-one-regressor}}

So, linear regression allows us to \emph{estimate}, and make \emph{inferences} about, \emph{population} slope coefficients. Inference means drawing conclusions and population refers to the fact that we do not want to say something about our sample, but instead about our whole population. Ultimately our aim
is to estimate the \textbf{causal} effect on \(Y\) of a unit change in \(X\)---but for now, just think of the problem of fitting a straight line to data on two variables, \(Y\) and \(X\).

Similar to Subsection \ref{sec:numevidence} we have three strategies to make inferences:

\begin{itemize}
\tightlist
\item
  We estimation the relation:

  \begin{itemize}
  \tightlist
  \item
    This now boils down to the question how we should draw a line through the data to estimate the (population) slope using Ordinary Least Squares (OLS---a specific an most common type of regression analysis)
  \item
    And then we have to assess the advantages and disadvantages of OLS
  \end{itemize}
\item
  We could refer to hypothesis testing:

  \begin{itemize}
  \tightlist
  \item
    Very often this comes down to testing where the slope is zero. Namely, it the slope is zero, then the data does not show a relation between \(Y\) and \(X\).
  \end{itemize}
\item
  Using confidence intervals:

  \begin{itemize}
  \tightlist
  \item
    This is related to constructing a confidence interval for the slope
  \end{itemize}
\end{itemize}

Before we look into this we first need some clarification on notation. As mentioned above, we would like to know the population regression line:

\begin{equation}
testscr = \beta_0 + \beta_1 STR,
\end{equation}
where
\begin{eqnarray}
    \beta_1& =& \text{slope of population regression line} \\
    &=&   \frac{\Delta Testscore}{\Delta STR}\\
    &=& \text{change in test score for a unit change in STR}
\end{eqnarray}
Note the definition here of \(\beta_1\). It gives the \textbf{marginal effect} of a change in \(STR\) on \(testscr\). So the interpretation of the parameter \(\beta_1\) is very straightforward. However, we do not know the population value of \(\beta_1\) and we therefore have to estimate it using data.

In general, the population linear regression \emph{model} is different as we add element \(u_1\).
\begin{equation}
    Y_i = \beta_0 + \beta_1 X_i + u_i, \qquad i\ldots n
    \label{eq:ols}
\end{equation}
Now, \(X\) denotes the independent variable or regressor, \(Y\) the dependent variable, \(\beta_0\) the intercept, \(\beta_1\) the slope, and \(u_i\) the regression error. The regression error consists of omitted factors, or possibly measurement error in the measurement of \(Y\). In general, these omitted factors are other factors that influence \(Y\), other than the variable \(X\).

\hypertarget{estimating-with-ols}{%
\subsubsection{Estimating with OLS}\label{estimating-with-ols}}

To estimate the population linear regression model we apply the ordinary least squares estimator. Again, as Figure \ref{fig:unire} shows as well, a linear regression line is a straight line through points in a scatterplot. Actually, we want to draw that line such that the distance of all points to that line is minimized. See that in Figure \ref{fig:unire} the distances between the points and the line are given by the \(_i\)'s, the regression errors. So, if we somehow can minimize all \(u_i\)'s we are fine. But those distances could be both positive and negative and they might cancel each other out. Therefore, we first square the regression errors and then minimize (hence the name: ordinary least \emph{squares}). Also, see from Eq. \eqref{eq:ols} that:

\begin{equation}
u_i = Y_i - (\beta_0 + \beta_1 X_i) \longleftrightarrow (u_i)^2 = \left[Y_i - (\beta_0 + \beta_1 X_i) \right]^2
\end{equation}

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Lecture1_sheet8} 

}

\caption{Drawing a straight line through data in a scatterplot}\label{fig:unire}
\end{figure}

But how can we estimate \(\beta_0\) and \(\beta_1\) from data? For that we will focus on the least squares (ordinary least squares or OLS) estimator of the unknown parameters \(\beta_0\) and \(\beta_1\), which solves,
\begin{equation}
    \min_{b_0,b_1} \sum^n_{i=1} \left[Y_i - (b_0 + b_1 X_i) \right]^2
\end{equation}

In fact, the OLS estimators of the slope \(\beta\)\(_1\) and the intercept \(\beta\)\(_0\) are:\footnote{This result is given but is not all too difficult to prove. However, usually you do need these types of equations in your work.}

\begin{eqnarray}
    \hat{\beta}_1 &=& \frac{\sum^n_{i=1}(X_i - \overline{X})(Y_i - \overline{Y})}{\sum^n_{i=1}(X_i - \overline{X})^2} = \frac{s_{XY}}{s^2_X}\\
    \hat{\beta}_0 &=& \overline{Y} - \hat{\beta}_1\overline{X}
\end{eqnarray}

Although you do need to learn these formula's by heart some insightful comments can be given. First, if a parameter is estimated then it gets a on top. Second, the optimal \(\hat{\beta}_1\) is equal to \(\frac{s_{XY}}{s^2_X}\) and this is the sampling covariance between \(X\) and \(Y\) divided by the sampling variance of \(X\). This is not a correlation as the units still depend on \(X\) and \(Y\) and therefore the slope can be large than \(1\) or smaller than \(-1\), but it does say something about the relation between \(X\) and \(Y\). Third, the constant is governed by the estimated parameter \(\hat{\beta}_1\)

From here we can predict the values \(\hat{Y}_i\) and residuals \(\hat{u}_i\) as they are:
\begin{eqnarray}
    \hat{Y}_i &=& \hat{\beta}_0 + \hat{\beta}_1 X_i, \qquad i = 1, \ldots, n\\
    \hat{u}_i &=& Y_i - \hat{Y}_i, \qquad i = 1, \ldots, n
\end{eqnarray}

When we apply this to our data cloud in Figure \ref{fig:scattercaschool} then we get the following optimal population regression line:

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Lecture1_sheet13} 

}

\caption{Scatterplot and estimated regression line}\label{fig:cloud}
\end{figure}

where the estimated slope equals \(\hat{\beta}_1 = -2.28\), the estimated intercept equals \(\hat{\beta}_0 = 698.9\) and the total population regression line can be written as: \(\widehat{TestScore} = 698.9 - 2.28 \times STR\). So, how to interpret the estimated slope and intercept now? First, the slope entails that districts with one more student per teacher on average have test scores that are 2.28 points lower (that is, \(\frac{\Delta TestScore}{\Delta STR} =-2.28\)). Secondly, the intercept (taken literally) means that, according to this estimated line, districts with zero students per teacher would have a (predicted) test score of 698.9. Now, this does not make any sense---it actually extrapolates the line outside the range of the data. In this case we can say that the intercept is not economically meaningful.

Now, how to fill in predictions? One of the districts in the data set is Antelope (CA) for which \(STR = 19.33\) and \(TestScore = 657.8\) Then the predicted value for the testscore is \(\hat{Y}_{Antelope}= 698.9 - 2.28 \times 19.33 = 654.8\) and the resulting residual is \(\hat{u}_{Antelope} = 657.8 - 654.8 = 3.0\)

In \texttt{STATA} both the constant and the slope can be easily retrieved by:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{regress}\NormalTok{ testscr str, }\KeywordTok{robust}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear regression                               Number of obs     =        420
                                                F(1, 418)         =      19.26
                                                Prob > F          =     0.0000
                                                R-squared         =     0.0512
                                                Root MSE          =     18.581

------------------------------------------------------------------------------
             |               Robust
     testscr | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -2.279808   .5194892    -4.39   0.000    -3.300945   -1.258671
       _cons |    698.933   10.36436    67.44   0.000     678.5602    719.3057
------------------------------------------------------------------------------
\end{verbatim}

We will discuss the rest of this output later.

\hypertarget{sec:unitesting}{%
\subsubsection{Hypothesis testing}\label{sec:unitesting}}

We can assess the importance of the line as well with hypothesis testing. Again, recall that in in applied econometrics we will only \textbf{reject} the \textbf{null}-hypothesis, we do not accept an hypothesis based upon one statistical test only. So, we aim to test \(H_0: E(Y) = \mu_{Y,0}\) vs.~\(H_1: E(Y) \neq \mu_{Y,0}\), where \(\mu_{Y,0}\) is some specified quantity that we are interested in. Typically \(\mu_{Y,0} = 0\) as this denotes no relation, but sometimes you could be interested in, e.g., whether \(\mu_{Y,0} = 1\) when testing elasticities. Or you could be interested in other quantities.

Testing statistical hypotheses is often very confusing, because of two things. First, you actually test whether the data you have corresponds with the null-hypothesis. Or, in other words:

\begin{quote}
What is the probability that your data (\(D\)) might be right \emph{given} the null-hypothesis (\(H_0\)): \(\Pr(D|H_0)\)
\end{quote}

And that is a strange concept. You first imagine a world \(H_0\) with the data that it \emph{should} provide and then test that imaginary world.

Secondly, there is the notation that often works confusing. First, we have the \(p\)-value which equals the probability of drawing a statistic (e.g., \(\bar{Y}\)) \emph{at least as adverse} to the null (that is: your imaginary world) as the value actually computed with your data, \textbf{assuming} again that the null-hypothesis is true---again, you imaginary world. Secondly, there is the significance level of a test which is a \emph{pre-specified} probability of incorrectly rejecting the null, when the null is actually true.

Now, suppose that you want to calculate the \(p\)-value based on an estimated coefficient \(\hat{\beta}_1\), then you construct the following test:
\begin{equation}
 p\text{-value} = \Pr_{H_0}[|\hat{\beta}_1 - \beta_{1,0}| > |\hat{\beta}_1^{act} - \beta_{1,0}| 
\end{equation}
where \(\hat{\beta}_1^{act}\) is the value of \(\hat{\beta}_1\) actually observed, and \(\beta_{1,0}\) is the value of \(\beta_1\) under the null-hypothesis (e.g., \(\beta_{1,0} = 0\)). Now, this is confusing, but in words it states that if you belief the null-hypothesis (the estimated value should be then \(\hat{\beta_1}\)), (\(\hat{\beta}_1^{act}\)) or even more extreme values can be estimated.

To test the null hypothesis \(H_0\) we follow three steps. First, we need to compute the \textbf{standard error} of \(\hat{\beta_1}\), which is an estimator of \(\sigma_{\hat{\beta_1}}\). Using an ordinary least squares estimator, standard errors for coefficients are given by:

\begin{equation}
\sigma_{\hat{\beta_1}} = \sqrt{\frac{1}{n} \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \bar{X})^2 u_i^2}{\left[\frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \right]^2}},
\label{eq:olsse}
\end{equation}
which is a rather daunting expression.

Second, we need to compute the \(t\)-statistic:
\begin{equation}
t = \frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)}
\label{eq:olst}
\end{equation}

Finally, we need to calculate the \(p\)-value. To do so, we need to know the sampling distribution of \(\hat{\beta}_1\), which we know is complicated if \(n\) is small, but typically you have enough observations to invoke the \emph{Central Limit Theorem}. So, if \(n\) is large, you can use the normal approximation (CLT) as follows
\begin{eqnarray}
                        p\text{-value}& = &   \Pr_{H_0}\left[\left|\hat{\beta}_1 - \beta_{1,0}\right| > \left|\hat{\beta}_1^{act} - \beta_{1,0}\right|\right]\\
            & = &   \Pr_{H_0}\left[\left|\frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)}\right| > \left|\frac{\hat{\beta}_1 ^{act} - \beta_{1,0}}{SE(\hat{\beta}_1)}\right|\right]\\
            & = &   \Pr_{H_0}[|t| > |t^{act}|]\\
                        &\simeq& \text{probability under left + right } N(0,1) \text{ tails}
\end{eqnarray}
where \(SE(\hat{\beta}_1)\) again equals the standard error of \(\hat{\beta}_1\), denoted with \(\sigma_{\hat{\beta}_1}\)

So, if you know \(\hat{\beta}_1\) and \(\sigma_{\hat{\beta}_1}\) you can calculate this. However, computers are much faster, in doing to. For example, suppose we want to test whether \(\beta_{1,0} = 0\) using the regression output displayed above which gives \(\hat{\beta}_1 = -2.28\) and \(\sigma_{\hat{\beta}_1} = 0.52\). That is step 1. Note that \texttt{STATA} already calculated the standard error of Eq. \eqref{eq:olsse}. For the next step we need to compute the \(t\)-statistic, which is:

\begin{equation}
t = \frac{2.28 - \beta_{1,0}}{0.52} = \frac{2.28 - 0}{0.52} = -4.39.
\label{eq:olstemp}
\end{equation}

then the \(p\)-value can be seen from Figure \ref{fig:pvalues}:

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/lecture_sheet9} 

}

\caption{Calculating the $p$-value of a two-sided test when $t^{act} = -4.38$}\label{fig:pvalues}
\end{figure}

That is, for large \(n\) (and typically we have that), the \(p\)-value is the probability that a \(N(0,1)\) random variable falls outside \(|\hat{\beta}_1^{act} - \beta_{1,0})/\sigma_{\hat{\beta}_1} | = |t|\). That is the blue areas under the normal distribution and they entail a probability \emph{mass}. Now, if both surfaces on the sides are not larger than 2.5\%, then we can reject the null-hypothesis against a 5\% significance level. Now, the computer output above gives a \(p\)-value of \(0.000\), which a bit strange. The \(p\)-value is actually not zero, but a very small number and definitely smaller than \(0.05\), so we can \emph{reject} the null-hypothesis being \(\beta_{1,0} = 0\) at a 5\% significance level (and at a 1\% and 0.1\% significance level as well). Now, if the \(t\)-statistic is exactly 1.96 in absolute value, then the \(p\)-value is 0.05. So, to repeat the steps, but now using computer output for testing the hypothesis that \(\beta_1 = \beta_{1,0}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Get the standard error from computer output
\item
  Compute the \(t\)-statistics as in Eq. \eqref{eq:olst}
\item
  Get the corresponding \(p\)-value. Or, reject the null-hypothesis at the 5\% significance level if \(|t^{act}| > 1.96\).
\end{enumerate}

Now there is a link between the \(p\)-value and the significance level. The significance level is pre-specified. For example, if the pre-specified significance level is 5\%, then you reject the null hypothesis if \(|t| \geq 1.96\) or equivalently, you reject if \(p \leq 0.05\). The \(p\)-value is sometimes called the marginal significance level. Often, it is better to communicate the \(p\)-value than simply whether a test rejects or not---the \(p\)-value contains more information than the ``yes/no'\,' statement about whether the test rejects.

But recently there has been some debate about using \(p\)-values (Amrhein, Greenland, and McShane 2019). Why should you use a 5\% significance level, what is so special about that number? It is not better just to report coefficients and standard errors? Figure \ref{fig:significance} shows a figure from the journal Nature and how scientists across all fields nowadays see \(p\)-values and statistical significance. This is not to say that testing does not matter, but more reporting. First of all, \(p\) values in themselves do not contain that much information. In the regression output of above you \(p\)-values being equal to 0.000 which is not informative. Secondly, the cut-off point of 5\% is a bit harsh and could to publications being published only with \(p\)-values below 0.05, leading to what is called publication bias.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/significance} 

}

\caption{Critical review on the (mis)use of statistical significance}\label{fig:significance}
\end{figure}

\hypertarget{confidence-intervals}{%
\subsubsection{Confidence intervals}\label{confidence-intervals}}

The exact definition of confidence intervals is a bit tricky. Namely, a 95\% confidence interval for \(\hat{\beta}_1\) is an interval that contains the true value of \(\beta_1\) in 95\% of repeated samples. That means that confidence interval do not give a probability (even though we would like to interpret it that way). But you can state that every value within a confidence interval would not be rejected as null-hypothesis, while every value outside the confidence interval would be rejected. Now, if we know both \(\hat{\beta}_1\) and \(\sigma_{\hat{\beta}_1}\) (again using computer output), then a 95\% confidence interval can be very easily constructed. For our regression of output of above this entails

\begin{equation}
\hat{\beta}_1 \pm 1.96 \times \sigma_{\hat{\beta}_1} = -2.28 \pm 1.96 \times 0.52 = [-3.30, -1.26].
\label{eq:olsci}
\end{equation}
So, every value between \(-3.30\) and \(-1.26\) will \textbf{not} be rejected as null-hypothesis, while every value outside that interval will be rejected. Note that confidence interval is again automatically given by computer output. If one would like a confidence interval against another critical level, say against a 99\% critical level, one can use the \texttt{level()} option

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{regress}\NormalTok{ testscr str, }\KeywordTok{robust} \DecValTok{level}\NormalTok{(99)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear regression                               Number of obs     =        420
                                                F(1, 418)         =      19.26
                                                Prob > F          =     0.0000
                                                R-squared         =     0.0512
                                                Root MSE          =     18.581

------------------------------------------------------------------------------
             |               Robust
     testscr | Coefficient  std. err.      t    P>|t|     [99% conf. interval]
-------------+----------------------------------------------------------------
         str |  -2.279808   .5194892    -4.39   0.000    -3.624061    -.935556
       _cons |    698.933   10.36436    67.44   0.000     672.1137    725.7522
------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{sec:dummy}{%
\subsubsection{Regression with a dummy}\label{sec:dummy}}

Sometimes a regressor is binary, meaning an indicator or dichotomous (0/1) variable. Let's go back again to Subsection \ref{sec:numevidence}, where we created such a binary variable with small and large class sizes (\(X=1\) if class size is small, \(X=0\) if not). Other possible example are gender (\(X=1\) if female, \(X=0\) if male) or being treated or not (\(X=1\) if treated, \(X=0\) if not). We refer to these types of variables as being \textbf{dummy} variables---and they are very often used in the social sciences.

Now, suppose we have a population regression model that looks like:

\begin{equation}
Y_i = \beta_0 + \beta_1 X_i + u_i
\label{eq:olsdummy}
\end{equation}

Where \(Y\) denotes, e.g., test scores, and where \(X\), e.g., denotes a dummy variable for a large class (so, if \(STR \geq 20\) then \(X_i = 1\); otherwise \(X_i = 0\)), so there is then only two possibilities:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For small class size there should hold that \(X_i = 0\) yielding that \(Y_i = \beta_0 + u_i\). Namely \(\beta_1 \times X_i = \beta_1 \times 0 = 0\). That means automatically that the expectation of \(Y_i\) is the constant, being \(\beta_0\). Another way or writing is that the expectation of model \eqref{eq:olsdummy} \emph{conditional} on the fact that \(X_i = 0\) is \(\mathbb{E}(Y_i \mid X_i = 0) = \beta_0\).
\item
  For large classes there should hold that \(X_i = 1\) yielding that \(X_i = 1\), \(Y_i = \beta_0 + \beta_1 + u_i\). This means the expectation of model \eqref{eq:olsdummy} \emph{conditional} on the fact that \(X_i = 1\) is \(\mathbb{E}(Y_i \mid X_i = 1) = \beta_0 + \beta_1\)
\end{enumerate}

So a regression with a dummy as independent variable gives two different \emph{constants}, for each group (small/large classes) one. You can interpret this as a level-effect (only the level changes, not the slope as there is none here). The interpretation of \(\beta_1\) is in this case rather special and can be denoted as:
\begin{equation}
\beta_1 = \mathbb{E}(Y_i \mid X_i = 1) - \mathbb{E}(Y_i \mid X_i = 0),
\end{equation}
Which is just the population difference in group means.

If we go back to our example with \(X_i = 1\) if \(STR \geq 20\) and 0 otherwise then we get the following regression output

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{regress}\NormalTok{ testscr large, }\KeywordTok{robust}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear regression                               Number of obs     =        420
                                                F(1, 418)         =      16.34
                                                Prob > F          =     0.0001
                                                R-squared         =     0.0369
                                                Root MSE          =     18.721

------------------------------------------------------------------------------
             |               Robust
     testscr | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
       large |   -7.37241   1.823578    -4.04   0.000    -10.95694   -3.787884
       _cons |   657.3513   1.255147   523.72   0.000     654.8841    659.8184
------------------------------------------------------------------------------
\end{verbatim}

Now, note that this is the same output (\(\Delta= -7.4\), \(\sigma_\Delta = 1.82\) and \(t\)-statistic is \(-4.04\)) as when we did the difference in means test in Subsection \ref{sec:smart}. To conclude, this is just another way (and much easier) to do a difference-in-means analysis. And this directly carries over for the situation when we have additional regressors.

\hypertarget{least-squares-assumptions-for-causal-inference}{%
\section{Least squares assumptions for causal inference}\label{least-squares-assumptions-for-causal-inference}}

As stated at the start of Section \ref{secproblem} applied econometrics focuses on finding a \textbf{causal} effect. But how do you do know that the \(\hat{\beta}_1\) you estimate using the population regression model of Eq. \eqref{eq:betacausal} is indeed a causal effect. In other words, if you change \(X_i\) with one unit, will \(Y_i\) then change with \(\beta_1\) \textbf{in reality}?

\begin{equation}
Y_i = \beta_0 + \beta_1 X_i + u_i, \qquad i = 1 \dots n
\label{eq:betacausal}
\end{equation}

Fortunately, there is a small set of assumptions that indeed lead to such a causal interpretation. The so-called three least squares assumptions, being:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The conditional distribution of \(u\) given \(X\) has mean zero, that is, \(E(u \mid X = x) = 0\).

  \begin{itemize}
  \tightlist
  \item
    We also refer to this assumption as the \textbf{conditional mean independence} assumption
  \item
    This assumptions implies that \(\hat{\beta_1}\) is truly \emph{unbiased}
  \end{itemize}
\item
  \((X_i,Y_i), i =1 \ldots n\) are i.i.d.

  \begin{itemize}
  \tightlist
  \item
    This is true if \(X\), \(Y\) are collected by simple random sampling
  \item
    This delivers the sampling distribution of \(\hat{\beta_0}\) and \(\hat{\beta_1}\)---again with a relatively large number (say \(n > 50\) the sampling distribution can very well be approximated by a normal distribution
  \end{itemize}
\item
  Large outliers in \(X\) and/or \(Y\) are rare.

  \begin{itemize}
  \tightlist
  \item
    Outliers can result in meaningless values of \(\hat{\beta_1}\)
  \end{itemize}
\end{enumerate}

We will first discuss these three least squares assumptions and then give some other assumptions as well as you frequently encounter them

\hypertarget{least-squares-assumption-1-conditional-mean-independence}{%
\subsection{Least squares assumption 1: conditional mean independence}\label{least-squares-assumption-1-conditional-mean-independence}}

This first assumptions states that \(E(u \mid X = x) = 0\) and is conceptually the most difficult one to grasp. Loosely speaking, it states that the regression error \(u\) is \emph{not} related with the independent variable \(X\). They are not related with each other. Another way of looking at this is displayed in Figure \ref{fig:ass1}. Here, whatever the value of student-teacher ratio is, the expectation of the outcome variable (test scores) is always centered around the population regression line. So, on average, you always predict according to \emph{your} model, for each value of \(X\).

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Lecture1_sheet25} 

}

\caption{Condition mean independence assumption}\label{fig:ass1}
\end{figure}

Now, when is this assumption violated. Consider again the population regression model: \(TestScore_i = \beta_0 + \beta_1 STR_i + u_i\), where \(u_i\) denotes other factors. Now, these other factors can be everything and else. And you should ask yourself whether it is plausible that \(E(u|X = x) = 0\) for \textbf{all} these other factors?

This assumption lies as well at the heart of experimental settings. Namely, consider a theoretical ideal randomized controlled experiment, where:
1. \(X\) is \emph{randomly} assigned to people (students randomly assigned to different size classes or patients randomly assigned to medical treatments).
2. Because \(X\) is assigned randomly, all other individual characteristics---the things that make up \(u\)---are \emph{independently} distributed of \(X\) by definition.
3. Thus it automatically follows that: \(E(u \mid X = x) = 0\)

Now, both in actual experiments, or with \textbf{observational} data, we will need to think hard about whether \(E(u|X = x) = 0\) holds. Chapters \ref{modeling} and \ref{specification} provide various examples where this assumption is violated. However, if this assumption is violated it means that you have \textbf{biased} inference, which boils down to the fact that your estimated \(\hat{\beta_1}\) is not the one that you want and that correct inference based upon this estimated cannot be done.

\hypertarget{least-squares-assumption-2-independenty-and-identically-distributed}{%
\subsection{Least squares assumption 2: independenty and identically distributed}\label{least-squares-assumption-2-independenty-and-identically-distributed}}

The second least squares assumptions deals with actual sampling of your data, both the dependent (\(Y\)) and independent (\(X\)) variables. that entail that \((X_i,Y_i), i = 1 \dots n\) should be \emph{i.i.d.}. This assumptions arises automatically if the entity (individual, district) is sampled by simple \emph{random sampling}. There are possible violations to this assumptions. For example, you sample via your friends on social media (snowballing), or observations are not independent but are correlated, which arises very frequently in the context of temporal correlation or spatial correlation.

The consequence of violating the \emph{i.i.d.} assumption is less severe then violating the conditional mean independence assumption. It leads to wrong \emph{standard errors}, not to biased estimations.

\hypertarget{least-squares-assumption-3-large-outliers-are-rare}{%
\subsection{Least squares assumption 3: Large outliers are rare}\label{least-squares-assumption-3-large-outliers-are-rare}}

The third and final least square assumption for causal inference is that large outliers are rare. Large outliers are not well defined and depend on the size of both \(Y\) and \(X\), but in general it can be seen as an \emph{extreme} value of \(X\) or \(Y\). The problem is that such a large outlier can strongly \emph{influence} the results and in general it can be stated that OLS can be rather sensitive to an outlier. Consider the two population regression lines in \ref{fig:outlier}. The flat one (with \(\hat{\beta}_1 = 0)\) does not take the isolated observation in the upper right corner into account. The one with the positive slope does. Now, clearly the one isolated observation in the upper right corner clearly matter to a large extent and is an important driver for the results of the ordinary least squares estimator.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Lecture1_sheet29} 

}

\caption{Effect of outliers on OLS estimations}\label{fig:outlier}
\end{figure}

However, this does not automatically entail that the isolated observation should be deleted. What it does matter is that one should go back to her data and investigate whether the outlier could be a mistake---perhaps a typo made when preparing the data or something that went amiss when converting the data from an \texttt{Excel} format to a \texttt{STATA} format.

\hypertarget{other-least-squares-assumptions}{%
\section{Other least squares assumptions}\label{other-least-squares-assumptions}}

Oftentimes, two other least squares assumptions are frequently encountered. However, keep in mind that you do \emph{not} need them for causal inference. They are the assumptions of homoskedasticity and normality.

\hypertarget{homoskedasticity}{%
\subsection{Homoskedasticity}\label{homoskedasticity}}

Homoskedasticity is concerned with the standard errors. Its definition is if \(var(u \mid X=x)\) is constant---that is, if the variance of the conditional distribution of \(u\) given \(X\) does not depend on \(X\)---then \(u\) is said to be homoskedastic. Otherwise, \(u\) is heteroskedastic.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Sheet21} 

}

\caption{Homoskedastic standard errors}\label{fig:homoskedasticity}
\end{figure}

Consider Figure \ref{fig:homoskedasticity}. Clearly the variance \emph{around} the population regression line is everywhere the same, regardless the value of student-teacher ratio (\(X\)). Recall, that \(E(u \mid X=x) = 0\) so \(u\) satisfies Least Squares Assumption 1. Now, in addition we also assume that the variance of \(u\) does not depend on \(x\). This is the case of homoskedasticity

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Sheet22} 

}

\caption{Heteroskedastic standard errors}\label{fig:heteroskedasticity}
\end{figure}

Now consider Figure \ref{fig:heteroskedasticity}. Now clearly the variance around the population regression line increases in student-teacher ratio (\(X\)). So, \(E(u \mid X=x) = 0\) is still satisfied, but the variance of \(u\) does now depends on \(x\). \(u\) is now said to be heteroskedastic.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Sheet23} 

}

\caption{Wages versus years of education}\label{fig:wages}
\end{figure}

Very often data or model in the social sciences are heteroskedastic. For example, wages are usually heteroskedastic in the amount of education consumed. Figure \ref{fig:wages} shows the relation between years of education and wages, and the larger the years of education the larger hourly earnings (wages) are---as you would assume would be. But the variance also incrases in years of education. That is because you can easily predict wages when workers enjoyed very few years of schooling---usually those are just above minimum wages---but the spread becomes much wider when years of schooling goes up.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Sheet24} 

}

\caption{Heteroskedasticity in Californation schools?}\label{fig:heteroskedasticityca}
\end{figure}

Is this now the case for our Californian school dataset. If we look again at the scatterplot between test scores and student-teacher ratios in Figure \ref{fig:heteroskedasticityca}, then that is very difficult to see. But then again, does it matter whether you face heteroskedasticity or homoskedasticity.

Note that so far we have (without saying so) assumed that \(u\) might be heteroskedastic
Recall again the three least squares assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(E(u \mid X = x) = 0\)
\item
  \((X_i,Y_i), i =1,\ldots,n\), are i.i.d.
\item
  Large outliers are rare
\end{enumerate}

They do not say anything about homo- or heteroskedasticity and Because we have not explicitly assumed homoskedastic errors, we have implicitly allowed for heteroskedasticity.

But what if the errors are in fact homoskedastic? Then in fact yYou can prove that OLS has the lowest variance among estimators that are \emph{linear} in \(Y\). The formula for the variance of \(\hat{\beta_1}\) and the OLS standard error simplifies: If \(var(u_i \mid X_i=x) = \sigma_u^2\), then
\begin{equation}
    var(\hat{\beta}_1) = \frac{\sigma_u^2}{n\sigma_X^2}
    \label{eq:olssesimple}
\end{equation}
which is much simpler than Eq. \eqref{eq:olsse}. Again note that \(var(\hat{\beta}_1)\) is inversely proportional to \(var(X)\): more spread in \(X\) means more information about \(\hat{\beta}_1\)---we discussed this earlier but it is clearer from this formula.

But what does this mean for estimation. Note that \texttt{STATA} does not automatically apply Eq. \eqref{eq:olsse} for its standard errors, but uses the simpler version Eq. \eqref{eq:olssesimple} instead. But if invoke the \texttt{,\ robust} option, as we already did above, \texttt{STATA} computes heteroskedasticity-robust standard errors. So if you do not, \texttt{STATA} computes homoskedasticity-only standard errors.

The bottom line is that the errors are either homoskedastic or heteroskedastic and if you use heteroskedastic-robust standard errors, you are fine. Namely:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If the errors are heteroskedastic and you use the homoskedasticity-only formula for standard errors, your standard errors will be wrong (the homoskedasticity-only estimator of the variance of \(\hat{\beta}_1\) is inconsistent if there is heteroskedasticity).
\item
  The two formulas coincide (when \(n\) is large) in the special case of homoskedasticity
\item
  So, you should \textbf{always} use heteroskedasticity-robust standard errors.
\end{enumerate}

\hypertarget{normal-distributed-regression-term}{%
\subsection{Normal distributed regression term}\label{normal-distributed-regression-term}}

Finally, in many introductionary statistic courses, normal distributed error terms are assumed, which facilitates testing with small samples. So, \(u\) should be distributed \(N(0,\sigma^2)\). If you have a reasonable amount of observations (\(n >50\)), you do not need this assumptions, and especially not for causal inferences

\hypertarget{measures-of-fit}{%
\section{Measures of fit}\label{measures-of-fit}}

A natural question that might arise is how well the population regression line fits or explains the data. For ordinary least squares estimators there are two regression statistics that provide complementary measures of the quality of fit:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The regression \textbf{\(R^2\)}: This measures the fraction of the variance of \(Y\) that is explained by \(X\); it is unitless and ranges between zero (no fit) and one (perfect fit)
\item
  The standard error of the regression \textbf{\(SER\)}: This measures the magnitude of a typical regression residual in the units of \(Y\).
\end{enumerate}

\hypertarget{the-regression-r2}{%
\subsection{\texorpdfstring{The regression \(R^2\)}{The regression R\^{}2}}\label{the-regression-r2}}

The regression \(R^2\) is the fraction of the sample variance of \(Y_i\) ``explained'' by the
regression. To see this, first note that \(Y_i = \hat{Y}_i + \hat{u}_i\) or the observation is equal to the OLS prediction plus the predicted residual. In this notation, the \(R^2\) is the ratio between the sample variance of \(\hat{Y}\) and the sample variance of \(Y\). Here we make use of the following equity: Total sum of squares = explained ``SS'' + Residual ``SS'', where we can now define \(R^2\) as:

\begin{equation}
R^2= \frac{ESS}{TSS} = \frac{\sum^n_{i=1}\left(\hat{Y}_i - \overline{Y}\right)^2}{\sum^n_{i=1}\left(Y_i - \overline{Y}\right)^2}.
    \label{eq:r2}
\end{equation}
Now if \(R^2 = 0\) then that means \(ESS = 0\) and if \(R^2 = 1\) then that means \(ESS = TSS\). So, by definition yields \(0 \leq R^2 \leq 1\). There is one additional remark to make and that for an univariate regression model (so with single \(X\) on the right side), \(R^2\) equals the square of the correlation coefficient between \(X\) and \(Y\).

\hypertarget{the-standard-error-of-the-regression}{%
\subsection{The Standard Error of the Regression}\label{the-standard-error-of-the-regression}}

The standard error of the regression is defined as:

\begin{equation}
SER = \sqrt{\frac{1}{n-2} \sum_{i=1}^n \hat{u}_i^2}
\label{eq:ser}
\end{equation}

In comparison with the \(R^2\), the \(SER\) is measured in the units of \(u\), which are actually the units of \(Y\). It measures the average ``size'' of the OLS residual (so the average `mistake' made by the OLS regression line in absolute terms).

Often the \emph{root mean squared error} (\(RMSE\)) is used, which is very closely related to the \(SER\):
\begin{equation}
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^n \hat{u}_i^2},
\label{eq:rmse}
\end{equation}
where \(RMSE\) only differs from the \(SER\) in the \emph{degrees of freedom}.

If we again look at our regression output:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{regress}\NormalTok{ testscr str, }\KeywordTok{robust}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear regression                               Number of obs     =        420
                                                F(1, 418)         =      19.26
                                                Prob > F          =     0.0000
                                                R-squared         =     0.0512
                                                Root MSE          =     18.581

------------------------------------------------------------------------------
             |               Robust
     testscr | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -2.279808   .5194892    -4.39   0.000    -3.300945   -1.258671
       _cons |    698.933   10.36436    67.44   0.000     678.5602    719.3057
------------------------------------------------------------------------------
\end{verbatim}

then we see that the \(R^2 = .05\). So, only 5\% of all variation in test scores is explained. This of course makes sense as potential many important variables are not included in the model. However, this does not automatically mean that the impact is biased and especially that the \(STR\) is unimportant in a policy sence. Again, we focus on causal inference, not on making a good model for prediction. The \(RMSE= 18.6\) indicates that the average error made is 18.6 test score units, which can be seen as sizable. In Chapter \ref{modeling} we include other, and important, variables and what we then of course will see is that the \(R^2\) increases and the \(SER\) decreases.

\hypertarget{conclusion-and-discussion-1}{%
\section{Conclusion and discussion}\label{conclusion-and-discussion-1}}

Regression

\hypertarget{modeling}{%
\chapter{Modeling in the Social Sciences}\label{modeling}}

In Chapter \ref{univariateregression} we discussed the origins of, working of and assumptions behind univariate regression. That is, a regression model with only one independent variable \(X\) on the right hand side.\footnote{With right hand side we mean on the right side of the equal sign \(=\). It is often abbreviated with RHS.} However, and especially in the social science, you almost always see regressions many independent variables. Depending on the field, these variables can be called control variables, confounding factors or moderator variables. But why are these variables included? Is it only to improve model performance or are there other reasons? Section \ref{sec:morevar} deals with this question where after Section \ref{sec:multivariate} shows how you can include additional variables in a \emph{multivariate regression model} and especially how you should interpret them. Section \ref{sec:nonlinear} extends the multivariate regression model and shows how you can actually use this model to estimate a broad range of non-linear economic models. Section \ref{sec:fixedeffects} discusses the use of multiple dummy variables (see again Subsection \ref{sec:dummy}) in a way that economists refer to as \emph{fixed effects}. The last section concludes and provides a futher discussion of the benefits and limitations of multivariate regression models.

\hypertarget{sec:morevar}{%
\section{Why more independent variables?}\label{sec:morevar}}

So, why do we include more variables? One possible answer is because it makes a better predictive model. That is, a model that is able to explain the variation in the dependent variable \(Y\) better.\footnote{This is not entirely true. Increasing the R\(^2\) explains \textbf{in-sample} variation better, not necessarily \textbf{out-of-sample}. The latter is really what matters for prediction and this is the focus of many machine learning techniques. Note that this argument is directly related with the regression towards the mean argument made in Subsection \ref{sec:genesis}.} So, the R\(^2\) increases. But, as argued in Chapter \ref{univariateregression} we are not so much interested in prediction, but more in establishing a \textbf{causal} relation between \(X\) and \(Y\). So, if you change \(X\) (and only \(X\)) does \(Y\) changes and then with how much?

Although economists often claim that they are the only (social-)science which is focused on causality and provides a statistical framework for that, there are other approaches to causality as well. One that is often used in other sciences is the approach of the mathematican Judea Pearl (Pearl 2009). This approach focuses on the use of Directed Acyclical Graphs (DAGs), which is graphical visualisation of causality chains (or, what impacts what). We borrow this approach for the most simple setting as explained in Figure \ref{fig:unknown}. Here, we go back to our Californian school district dataset again, where we still are interested in the effect of class size on school performance. So, we suppose that that there is an effect from student teacher ratio on test scores as displayed with an directed arrow in Figure \ref{fig:unknown}. We also know that the R\(^2\) of that regression model was rather low (5\%), so by default there must be other but yet unknown factors, let us name them for now \(U\), that influence test scores as well (so a directed arrow going from \(U\) to test scores).

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/unknown} 

}

\caption{Unrelated omitted variables}\label{fig:unknown}
\end{figure}

Now we are fine with this is as long as \(U\) does \textbf{not impact} the student teacher ratio. Then, there is still an isolated effect of student teacher ratio on class size and that is exactly what we want to measure. However, if there is directed arrow going from \(U\) into \(STR\) as depicted by Figure \ref{fig:unobshet}, then the effect of student teacher ratio is not isolated anymore. Essentially, the effect of student teacher ratio on class size is composed out of two parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  The \textbf{causal} effect on student teacher ratio on class size captured by the chain \(\text{STR} \longrightarrow \text{testscore}\). The one we are after.
\item
  The impact of the unknown variables on test scores. As we have not modeled them in our regression model, the effect is captured by the chain \(U \longrightarrow \text{STR} \longrightarrow \text{testscore}\)
\end{enumerate}

Economists refer to this phenomenon as \textbf{omitted variable bias}, whilst in the statistical world, this is as often called confounding variables or the \textbf{the confounding fork} (McElreath 2020) and it, unfortunately, occurs very often.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Unobshet} 

}

\caption{Related omitted variables}\label{fig:unobshet}
\end{figure}

So, when \textbf{U} is a \emph{common} cause for both student teacher ratio and test scores there is omitted variable bias. If we go back to our population regression model as follows:
\begin{equation}
Y_i = \beta_0 + \beta_1 X_i + u_i,
\end{equation}
then we know that the error \(u\) arises because of factors that influence \(Y\) but are not included in the regression function; so, there are \emph{always} omitted variables. But do not always lead to bias. For omitted variable bias to occur, the omitted factor, let's call it \(Z\)\footnote{\(Z\) can be both known or unknown, so that is why we change from \(U\) to \(Z\)}, must be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A \textbf{determinant} of \(Y\) (i.e.~\(Z\) is part of \(u\))
\item
  A \textbf{determinant} of the regressor \(X\) (\emph{at least}, there should hold that \(corr(Z,X) \neq 0\))\footnote{In econometric textbooks, as, e.g, in Stock, Watson, et al. (2003), this condition is weakened to only being correlation (\(Z\) and \(X\) are correlated). However, if the directed arrow goes from \(STR\) into \(U\) in Figure \ref{fig:unobshet} then that would lead to something else than omitted variables, namely to a difference between a direct (\(\text{STR} \longrightarrow \text{testscore}\)) and an indirect effect (\(\text{STR} \longrightarrow U \longrightarrow \text{testscore}\)).}
\end{enumerate}

Thus, both conditions must hold for the omission of \(Z\) to result in omitted variable bias.

Now, in our Californian district school dataset we have many more variables. One of them is variable that measures the english language ability (whether the student has English as a second language). Note that in California there are many migrants, especially from Latin-America. Now, you can readily argue that not having English as first language plausibly affects standardized test scores: so, \(Z\) is a \textbf{determinant} of \(Y\). Moreover, immigrant communities tend to be less affluent and thus have smaller school budgets---and, therefore, higher \(STR\): \(Z\) is most likely as well a \textbf{determinant} of \(X\).

So, most likely, our original estimation from Chapter \ref{univariateregression}, \(\hat{\beta}_1\), is biased (so not the true causal effect). But can we say something about the direction that bias? Yes, but the argument tend to become very quickly rather complex. In this case, note that districts with more migrant communities tend to have (\emph{i}) higher class sizes and (\emph{ii}) lower test scores. So, to the original estimation they added a \emph{negative} effect. Thus, following this reasoning, the ``true'' effect must be less negative. Now, especially with negative signs this becomes rather complex, so if common sense fails you, there is the following formula:

\begin{equation}
\hat{\beta}_1 \overset{p}{\to} \beta_1 + \frac{\sigma_u}{\sigma_X}\rho_{Xu},
\end{equation}
where you should focus on the sign of the correlation between \(X\) and the regression residual \(u\) (all standard errors, \(\sigma\), are always positive by default). Now, the first least squares assumption states that \(\rho_{Xu} = 0\)---no correlation between the regressor and the regression residual. But now there is because of omitted variable bias. And because the negative relation between immigrants communities and school performance \(\rho_{Xu}\) should be negative. And because the original estimation from Chapter \ref{univariateregression} was already negative to begin with the ``true'' \(\beta_1\) should be less negative. In conclusion, districts with more English learning students (\emph{i}) do worse on standardized tests and (\emph{ii}) have bigger classes (smaller budgets), so ignoring the English learning factor results in overstating the class size effect.

You might wonder whether this is actually going on in the Californian district school data. To see this, Figure \ref{fig:omitca} offers a cross tabulation of test scores by class size and percentage English learners.

\begin{figure}

{\centering \includegraphics[width=800px]{./figures/Sheet7} 

}

\caption{Cross tabulation of test scores by class size and percentage English learners}\label{fig:omitca}
\end{figure}

Now, the table depicted in Figure \ref{fig:omitca} is complex in its various dimensions. We have our two categories of class size (small and large), together with the difference in test scores, but we now stratify this by four categories of percentage English learners. There are several important observations to make here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  districts with \emph{fewer} English Learners (so less migrants) have on average \emph{higher} test scores (what we assumed above);
\item
  districts with \emph{fewer} English Learners (so less migrants) have \emph{smaller} classes (what we assumed above);
\item
  the effect of class size with comparable percentages English learners is still (mostly negative), but not as much as we compares for all districts together (the Difference-column). This confirms our reasoning that our original estimate was too negative.
\end{enumerate}

No, as already mentioned above omitted variable bias occurs very often. So, how to correct for this such that the bias disappaers. In general, there are strategies:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  we can run a randomized controlled experiment in which treatment (\(STR\)) is randomly assigned: then percentage English learners (\(PctEL\)) is still a determinant of test score, but by construction \(PctEL\) should be uncorrelated with \(STR\). Unfortunately, is very difficult to randomize class size and often this strategy is just not attainable as being too costly or unethical (this accounts for all sciences);
\item
  we can adopt the cross tabulation approach of above, with finer gradations of \(STR\) and \(PctEL\). Then by construction, within each group all classes have the same \(PctEL\) so we control for \(PctEL\). A disadvantages is that one needs many observations, especially when one wants to stratify upon other variables as well;
\item
  finally, and perhaps the easiest approach, we can use a population regression model in which the omitted variable (\(PctEL\)) is no longer omitted. We just include \(PctEL\) as an additional regressor in a multiple regression model. This is what the next section deals with. Obviously, a disadvantage of this approach is that you need observations for the omitted variable.
\end{enumerate}

\hypertarget{sec:multivariate}{%
\section{Multivariate regression analysis}\label{sec:multivariate}}

So, if we have information about an important omitted variable, as in the case of the size of migrant communities in the example above, then we can use that information in a multivariate population regression model. In the case of two regressors, that would look like''
\begin{equation}
Y_i =\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i, i=1,\ldots,n
\end{equation}
where:

\begin{itemize}
\tightlist
\item
  \(Y\) is the dependent variable
\item
  \(X_1\), \(X_2\) are the two independent variables (regressors)
\item
  \((Y_i, X_{1i}, X_{2i})\) denote the i\(^{\mathrm{th}}\) observation on \(Y\), \(X_1\), and \(X_2\).
\item
  \(\beta_0\) is the unknown population intercept
\item
  \(\beta_1\) is the effect on \(Y\) of a change in \(X_1\), \textbf{holding} \(X_2\) constant
\item
  \(\beta_2\) is the effect on \(Y\) of a change in \(X_2\), \textbf{holding} \(X_1\) constant
\item
  \(u_i\) is the the regression error (omitted factors)
\end{itemize}

Now the only element that changes is the interpretation of a parameter, say \(\beta_1\). In this case, it can still be seen as a `slope' parameter, although now in 3-dimensional space, but it now states specifically that the other parameter(s) should be held constant. This does facilitate the interpretation of \(\beta_1\). For example, consider changing \(X_1\) by \(\Delta X_1\) while holding \(X_2\) constant. That means that the population regression line before the change looks like:
\begin{equation}
Y = \beta_0 + \beta_1 X_{1} + \beta_2 X_{2},
\end{equation}
whilst the population regression line, after the change, looks like:
\begin{equation}
Y + \Delta Y = \beta_0 + \beta_1 (X_{1} + \Delta X_1) + \beta_2 X_{2}
\end{equation}
And if we take the difference, then the interpretation of \(\beta_1\) boils down again to the marginal effect:\(\Delta Y = \beta_1 \Delta X_1\). Or, \(\beta_1 = \frac{\Delta Y}{\Delta X_1}\) when holding \(X_2\) constant and, likewise, \(\beta_2 = \frac{\Delta Y}{\Delta X_2}\) when holding \(X_1\) constant. \(\beta_0\) is now the predicted value of \(Y\) when \(X_1 = X_2 = 0\)

If we do this for the the Californian school district data, then the original population regression line was estimated as:
\begin{equation}
\widehat{TestScore} = 698.9- 2.28 STR
\end{equation}
But if we now include include percent English Learners in the district (\(PctEL\)) to the model then the population regression `line' becomes:
\begin{equation}
\widehat{TestScore} = 686.0- 1.10 STR - 0.65  PctEL
\end{equation}

Clearly, the effect of student teacher ratio becomes smaller (that is, less negative). And this is what should happen as reasoned above. The \texttt{STATA} syntax for a multivariate regression model is now rather straighforward. You basically add another to the regression equation, as below:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{reg}\NormalTok{ testscr str el\_pct, }\KeywordTok{robust}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear regression                               Number of obs     =        420
                                                F(2, 417)         =     223.82
                                                Prob > F          =     0.0000
                                                R-squared         =     0.4264
                                                Root MSE          =     14.464

------------------------------------------------------------------------------
             |               Robust
     testscr | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -1.101296   .4328472    -2.54   0.011     -1.95213   -.2504616
      el_pct |  -.6497768   .0310318   -20.94   0.000     -.710775   -.5887786
       _cons |   686.0322   8.728224    78.60   0.000     668.8754     703.189
------------------------------------------------------------------------------
\end{verbatim}

Obviously, the effect of student teacher ration reduces with 50\%! The interpretation of the rest of the statistical output, such as measures of fit and test statistics, follows in the subsections below.

\hypertarget{measures-of-fit-for-multiple-regression}{%
\subsection{Measures of fit for multiple regression}\label{measures-of-fit-for-multiple-regression}}

In multivariate regression models, there are four commonly used measures of fit.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The standard error of regression or the \(SER\) denotes the standard deviation of \(\hat{u}_i\) and includes a degrees of freedom correction (degrees of freedom in this case denotes how many variables your have used and typically is denoted with \(k\). The \(SER\) is defined as:
  \begin{equation}
  SER = s_{\hat{u}} = \sqrt{\frac{1}{n-k-1} \sum_{i=1}^n \hat{u}^2_i},
  \end{equation}
  where \(k\) is the number of variables (including the constant) use in the regression model. Note that in the univariate regression model \(k=2\)---the slope coefficient and the constant.
\item
  The root mean square error (RMSE) which denotes as well the stdandard deviation of \(\hat{u}_i\) but now without degrees of freedom. We have seen this before in Eq. \eqref{eq:rmse} and does not change.
\item
  The \(R^2\) which measures the fraction of variance of \(Y\) explained by the independent variables. Again, we have seen this one before
\item
  The adjusted ``adjusted \(R^2\)'' (or \(\bar{R}^2\)) which is equal to the \(R^2\) with a degrees-of-freedom correction that adjusts for estimation uncertainty. It can be formulated as:
  \begin{equation}
  \bar{R}^2 = 1 - \frac{n-1}{n-k-1}\frac{SSR}{TSS}.
  \end{equation}
  Note that using this formulation, in a multivariate setting, it always should hold that \(\bar{R}^2 <R^2\). Why do we care so much for the amount of variables that we use (\(k\)). That is because with each additional variable the \(R^2\) always increases. And it is essential to notice that \(k=n\), the \(R^2 = 1\), so there is no variation left anymore. But that feels like cheating. You just have a parameter for each observation that you have, but such a model must be meaningless. Therefore, you always want to correct for the number of variables that you use.
\end{enumerate}

In our Californian school district example that would amount to the following two outcomes. First for the univariate model:
\begin{eqnarray}
TestScore &= &698.9- 2.28  STR \\
&&R^2 = .05, SER = 18.6
\end{eqnarray}

And then for the multivariate model.

\begin{eqnarray}
TestScore &=& 686.0 - 1.10  STR - 0.65 PctEL \\
&&R^2=.426, \bar{R}^2=0.424, SER = 14.5
\end{eqnarray}

Note that all measures of fit increases. The \(\bar{R}^2\) now indicates that 42\% of all variation in test scores are explained. That is a \emph{huge} improvement compared to the 5\% explanatory power of the univariate case. That indicates that the \(PctEL\) strongly correlates with testscores. But again, we are not so much interested prediction, but want to find the causal impact of class size instead. Another thing to notice here is that the \(R^2\) and the \(\bar{R}^2\) are very close. That is because the number of variables is much smaller than the number of observations \(k \ll n\), so that the impact of \(k\) is not very big.

A list thing is a peculiarity of \texttt{STATA}. In the regression output of above \texttt{STATA} does not provide the \(\bar{R}^2\). That is because of the option \texttt{,\ robust}. Without that option, the regression output would look like, giving both measures of fit.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{reg}\NormalTok{ testscr str el\_pct}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Source |       SS           df       MS      Number of obs   =       420
-------------+----------------------------------   F(2, 417)       =    155.01
       Model |  64864.3011         2  32432.1506   Prob > F        =    0.0000
    Residual |  87245.2925       417  209.221325   R-squared       =    0.4264
-------------+----------------------------------   Adj R-squared   =    0.4237
       Total |  152109.594       419  363.030056   Root MSE        =    14.464

------------------------------------------------------------------------------
     testscr | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -1.101296   .3802783    -2.90   0.004    -1.848797   -.3537945
      el_pct |  -.6497768   .0393425   -16.52   0.000    -.7271112   -.5724423
       _cons |   686.0322   7.411312    92.57   0.000     671.4641    700.6004
------------------------------------------------------------------------------
\end{verbatim}

Another option is to specifically ask \texttt{STATA} to display the \(\bar{R}^2\) by invoking the command \texttt{display}, then some text (text always goes between strings), and finally the thing you want to see (\texttt{e(r2\_a)}). Something like:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{display} \StringTok{"adjusted R2 = "} \FunctionTok{e}\NormalTok{(r2\_a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
adjusted R2 = .42368043
\end{verbatim}

\hypertarget{the-least-squares-assumptions-for-multivariate-regression}{%
\subsection{The least squares assumptions for multivariate regression}\label{the-least-squares-assumptions-for-multivariate-regression}}

Now, it is easy to add other variables, so that the multivariate regression model now looks like:
\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i}+\ldots + \beta_k X_{ki}+u_i, i=1,\ldots,n
\end{equation}
Suppose we are interested in \(\beta_1\). How do we then know whether our estimation \(\hat{\beta}_1\) is unbiased? For that we again resort to our least squares assumption, some of them will change a bit and we have to add a fourth one:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The first least squares assumptions changes slightly. Now, we state that the conditional distribution of \(u\) given all \(X_i\)'s has mean zero, that is, \(E(u|X_1 = x_1,\ldots, X_k = x_k) = 0\). So, \(\beta_1\) is biased even another variable \(X_k\) is correlated with \(u\). So, only variables has to be correlated with \(u\) and then all parameters are to a certain extent biased.
\item
  The second least squares assumption is more or less as before but now in a multivariate fashion, so the whole set of (X\(_{1i}\),\ldots,X\(_{ki}\),Y\(_i\)), i =1,\ldots,n, should be independent and identical distributed (\(i.i.d\)).
\item
  The third least squares assumptions states again that large outliers are rare for all variables included, so for all \(X_1,\ldots, X_k\), and \(Y\).
\item
  The fourth assumoption is news and states that there is no perfect multicollinearity. We discuss this further below.
\end{enumerate}

\hypertarget{multicollinearity}{%
\subsubsection{Multicollinearity}\label{multicollinearity}}

Multicollinearity comes in two flavours; perfect and imperfect. The former functions as a multivariate least squares assumptions, whilst the latter oftentimes gives the largest problems. We start the discussion with perfect multicollinearity and then continue with the case of imperfect multicollinearity.

\hypertarget{perfect-multicollinearity}{%
\paragraph{Perfect multicollinearity}\label{perfect-multicollinearity}}

The official definition of perfect multicollinearity is that there is a \textbf{perfect linear combination} amongst your variables. That means that there is not one optimal solution, but instead many (actually, infinitely many) more. Let us illustrate by the following example. Suppose you include \(STR\) twice in your regression. Now, \texttt{STATA} produces then the following output:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{reg}\NormalTok{ testscr str str el\_pct, }\KeywordTok{robust}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
note: str omitted because of collinearity.

Linear regression                               Number of obs     =        420
                                                F(2, 417)         =     223.82
                                                Prob > F          =     0.0000
                                                R-squared         =     0.4264
                                                Root MSE          =     14.464

------------------------------------------------------------------------------
             |               Robust
     testscr | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -1.101296   .4328472    -2.54   0.011     -1.95213   -.2504616
         str |          0  (omitted)
      el_pct |  -.6497768   .0310318   -20.94   0.000     -.710775   -.5887786
       _cons |   686.0322   8.728224    78.60   0.000     668.8754     703.189
------------------------------------------------------------------------------
\end{verbatim}

See that \texttt{STATA} drops one of the \(STR\) variables. But why is that? See that the impact of twice this variable should be equivalent to:
\begin{equation}
\beta_1 STR = w_1 \beta_1 STR + w_2 \beta_1 STR = (w_1 + w_2) \beta_1 STR , 
\end{equation}
where \(w_1\) and \(w_2\) are weights chosen such that they satisfy the condition that \(w_1 + w_2 = 1\). But there is an infinite number of combinations that satisfy this condition! So, there is not an optimal solution and one of these variables should be dropped.

The violation of no perfect multicollearity often occurs when using dummies (see again Subsection \ref{sec:dummy}). Suppose that we regress \(TestScore\) on a constant, \(D\), and \(B\), where:\(D_i =1\) if \(STR \leq 20\), \(=0\) otherwise ; \(B_i =1\) if \(STR>20\), \(= 0\) otherwise. This example is slightly more complex as there is no perfect correlation between \(B\) and \(D\). However, the model contains as well a constant and that create a perfect linear combination, namely \(B_i + D_i = 1\) and that is the definition of a constant (\(\beta_1 \times 1\)), so there is perfect multicollinearity in the model. Now, this example is a special case of the so-called dummy variable trap. Suppose you have a set of multiple binary (dummy) variables, which are mutually exclusive and exhaustive---that is, there are multiple categories and every observation falls in one and only one category (Freshmen, Sophomores, Juniors, Seniors, Other). If you include all these dummy variables and a constant, you will have perfect multicollinearity---the dummy variable trap.

There are possible solutions to the dummy variable trap:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Omit one of the groups (e.g., the Freshmen), or
\item
  Omit the intercept
\end{enumerate}

In most cases you omit one of the groups (typically the one with the lowest value). This give the constant then the interpretation of the average value of that left-out category, where the dummy variables are then the relative differences to that left-out category.

Now, perfect multicollinearity usually reflects a mistake in the definitions of the regressors, or an oddity in the data. And, usually this is not a problem, because if you have perfect multicollinearity, your statistical software will let you know---either by crashing or giving an error message or by ``dropping'' one of the variables arbitrarily and very often the solution to perfect multicollinearity is to modify your list of regressors such that you no longer have perfect multicollinearity.

\hypertarget{imperfect-multicollinearity}{%
\paragraph{Imperfect multicollinearity}\label{imperfect-multicollinearity}}

Now imperfect and perfect multicollinearity are quite different despite the similarity of the names. Imperfect multicollinearity namely, occurs when two or more regressors are very highly correlated. And if two regressors are very highly correlated, then their scatterplot will pretty much look like a straight line---they are collinear---but unless the correlation is exactly \(\pm\) 1, that collinearity is imperfect. What this implies is that one or more of the regression coefficients will be imprecisely estimated. Why is that? That is because of the definition of the coefficient in a multivariate regression model. Namely, the coefficient on \(X_1\) is the effect of \(X_1\) \textbf{holding \(X_2\) constant}, but if \(X_1\) and \(X_2\) are highly correlated, then there is very little variation in \(X_1\) once \(X_2\) is held constant. That means that the data are pretty much uninformative about what happens when \(X_1\) changes but \(X_2\) doesn't, so the variance of the OLS estimator of the coefficient on \(X_1\) will be large. And this results in large standard errors for one or more of the OLS coefficients. But often this is very hard to detect. Are standard errors high because of imperfect multicollinearity, because the number of observations is very low, or because there is large variation in the data? The answer to this unfortunately boils down to reasoning, but before you start estimating your statistical models it always good to look at scatterplots and correlations between variables.

But what is a high correlation? With a reasonable amount of observations all correlations below \(0.9\) can be considered fine. In practice, only correlations between variables higher than say \(0.95\) start to impose problems.

\hypertarget{testing-with-multivariate-regression-models}{%
\subsection{Testing with multivariate regression models}\label{testing-with-multivariate-regression-models}}

\hypertarget{hypothesis-tests-and-confidence-intervals-for-a-single-coefficient-in-multiple-regression}{%
\subsubsection{Hypothesis tests and confidence intervals for a single coefficient in multiple regression}\label{hypothesis-tests-and-confidence-intervals-for-a-single-coefficient-in-multiple-regression}}

Recall from Subsection \ref{sec:unitesting} that for hypothesis testing in a classical statistical framework we make use of the fact that \(\frac{\hat{\beta}_1- E(\hat{\beta}_1)}{\sqrt{var(\hat{\beta}_1)}}\) is approximately distributed as \(N(0,1)\) according to the Central Limit theorem. Thus hypotheses on \(\beta_1\) can be tested using the usual \(t\)-statistic, and confidence intervals are constructed as \(\{\hat{\beta}_1 \pm 1.96 SE (\hat{\beta}_1)\}\). And this finding carries over to the multivariate setting where for \(\beta_2,\ldots, \beta_k\) we make use of the same framework. One thing to keep in mind is that \(\hat{\beta}_1\) and \(\hat{\beta}_2\) are generally not independently distributed---so neither are their \(t\)-statistics (more on this later).

Now, if we return to our Californian school district data set then we find that for the univariate case holds:

\begin{equation}
TestScore =\underbrace{698.9}_{10.4} - \underbrace{2.28}_{0.52}  STR, 
\end{equation}

And the population regression ``line'' for the multivariate case is estimated as:
\begin{equation}
TestScore = \underbrace{686.0}_{8.7} - \underbrace{1.10}_{0.43} STR - \underbrace{0.650}_{0.031} PctEL
    \label{eq:testmulti}
\end{equation}

Remember, the coefficient on \(STR\) in Eq. \eqref{eq:testmulti} is the effect on \(TestScores\) of a unit change in \(STR\), holding constant the percentage of English Learners in the district. The corresponding 95\% confidence interval for coefficient on \(STR\) in (2) is \(\{-1.10 \pm 1.96 \times 0.43\} = (-1.95,-0.26)\). And the \(t\)-statistic testing \(\beta_{STR} = 0\) is \(t = -1.10/0.43 = -2.54\), so we reject the null-hypothesis at the 5\% significance level. More evidence for the strength of the \(PctEL\) variable can be seen from the fact that, under the null-hypothesis of \(\beta_2 = 0\), the following must hold: \(t\text{-statistic} = \frac{\hat{\beta_1}}{\sigma_{\hat{\beta_1}}} = \frac{0.65}{0.03} = 21.7\), which is a very high number for a \(t\)-statistic.

\hypertarget{tests-of-joint-hypotheses}{%
\subsubsection{Tests of joint hypotheses}\label{tests-of-joint-hypotheses}}

So, testing of single coefficients is just as before. Now in the Californian school district dataset there is as well a variable called \(Expn\) denoting the expenditures per pupil. Consider the following population
regression model:
\begin{equation}
TestScore_i = \beta0 + \beta_1 STR_i + \beta_2 Expn_i + \beta_3PctEL_i + u_i
\end{equation}
The null hypothesis that ``school resources don't matter'' and the alternative that they do, corresponds to:

\begin{itemize}
\tightlist
\item
  \(H_0:\beta_1 =0\) and \(\beta_2 =0\) vs
\item
  \(H_1:\) either \(\beta_1 \neq 0\) or \(\beta_2 \neq 0\) or both
\end{itemize}

This is a joint hypothesis specifying a value for two or more coefficients, that is, it imposes a restriction on two or more coefficients. In general, a joint hypothesis will involve \(q\) restrictions. In the example above, \(q = 2\), and the two restrictions are \(\beta_1 = 0\) and \(\beta_2 = 0\). A ``common sense'' idea is to reject if either of the individual \(t\)-statistics exceeds 1.96 in absolute value. But this ``one at a time'' test isn't valid: the resulting test rejects too often under the null hypothesis (more than 5\%)! That is because the \(t\)-statistics themselved are often not independent. Instead, we need a \(F\)-statistic, which tests all parts of a joint hypothesis at once. Unfortunately, these types of formulas can become quickly rather comples. Consider the \(F\)-test for the special case of the joint hypothesis \(\beta_1 = \beta_{1,0}\) and \(\beta_2 = \beta_{2,0}\) in a regression with two regressors:

\begin{equation}
F = \frac{1}{2} \left(\frac{t_1^2 + t_2^2 - 2\hat{\rho}_{t_1,t_2}t_1 t_2}{1-\hat{\rho}^2_{t_1 t_2}}  \right)
\end{equation}

where \(\hat{\rho}_{t_1,t_2}\) estimates the correlation between \(t_1\) and \(t_2\). Reject when \(F\) is large (typically to be determined from large statistical tables). The F-statistic is large when \(t_1\) and/or \(t_2\) is large and the F-statistic corrects (in just the right way) for the correlation between \(t_1\) and \(t_2\). The formula for more than two \(\beta\)'s is nasty unless you use matrix algebra. There is a nice large-sample approximate distribution, which is the tail probability of the \(\chi^2_q /q\) distribution beyond the \(F\)-statistic actually computed.

Now, \texttt{STATA} does this in a much easier way by invoking the \texttt{test} command \textbf{right} after the regression. So, for example, we want to test the joint hypothesis that the population coefficients on \(STR\) and expenditures per pupil (\(expn\)) are both zero, against the alternative that at least one of the population coefficients is nonzero.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{reg}\NormalTok{ testscr str expn\_stu el\_pct, }\FunctionTok{r} 
\KeywordTok{test}\NormalTok{ str expn\_stu}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear regression                               Number of obs     =        420
                                                F(3, 416)         =     147.20
                                                Prob > F          =     0.0000
                                                R-squared         =     0.4366
                                                Root MSE          =     14.353

------------------------------------------------------------------------------
             |               Robust
     testscr | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         str |  -.2863992   .4820728    -0.59   0.553    -1.234002     .661203
    expn_stu |   .0038679   .0015807     2.45   0.015     .0007607    .0069751
      el_pct |  -.6560227   .0317844   -20.64   0.000    -.7185008   -.5935446
       _cons |   649.5779   15.45834    42.02   0.000     619.1917    679.9641
------------------------------------------------------------------------------


 ( 1)  str = 0
 ( 2)  expn_stu = 0

       F(  2,   416) =    5.43
            Prob > F =    0.0047
\end{verbatim}

The output shows an \(F\)-statistic with \(q=2\) restrictions with outcome 5.43. Do not directly interpret this number, but know that \(\text{Prob} > F = 0.0047\) gives the probability that under the null-hypothesis this outcome is produced. So the joint null-hypothesis that both types of expenditures are zero (at the same time), can be rejected at a 5\% (and a 1\%) significance level. Other types of joint tests can easily be constructed as well. For example, when you want to know whether both coefficient add up to 1, then you would state \texttt{test\ str\ +\ expn\_stu\ =\ 1}. The final point to make is the \(F\)-test in the regression output itself. Here, that is for example \texttt{F(3,\ 416)\ =\ 147.20}. This is joint test that all variables, except the constant, have no impact. So, \(\beta_i = 0\) for all \(i\) at the \textbf{same time}. It not often that you come across a general regression \(F\)-test that does not reject the null-hypothesis. It namely implies that your independent variable do not contain information about the dependent variable.

And with the \(F\)-test, we now have discussed all regression outcome components displayed by \texttt{STATA}. Most of this information you do not need for your report but we will come back later to this.

\hypertarget{sec:nonlinear}{%
\section{Non-linear specifications}\label{sec:nonlinear}}

The model we are using is coined the \emph{linear} regression model, and, indeed, one of the underlying assumptions is that the relations between the independent and dependent are linear. Consider the relation aain between test scores and class sizes in the Californian school district data. Using the following code (note now the \texttt{twoway} command that `binds' a scatter plot with a population regression line):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{graph} \KeywordTok{twoway}\NormalTok{ (}\KeywordTok{lfit}\NormalTok{ testscr str) (}\KeywordTok{scatter}\NormalTok{ testscr str)}
\end{Highlighting}
\end{Shaded}

Which provides the following \texttt{STATA} output.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/scatterlfit} 

}

\caption{A linear relation}\label{fig:scatterlfitcaschool}
\end{figure}

Indeed, there might be evidence that the relation depicted in Figure \ref{fig:scatterlfitcaschool}---if anything---is linear. But, clearly that is not the case for the relation between test scores and average district income. Namely, the syntax below:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{graph} \KeywordTok{twoway}\NormalTok{ (}\KeywordTok{lfit}\NormalTok{ testscr avginc) (}\KeywordTok{scatter}\NormalTok{ testscr avginc)}
\end{Highlighting}
\end{Shaded}

provides the following \texttt{STATA} output.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/scatterincome} 

}

\caption{A non-linear relation}\label{fig:scatterincome}
\end{figure}

Figure \ref{fig:scatterincome} shows a non-linear relation, where the effect of income tapers off (note the resemble with Figure \ref{fig:marginalutility})---or, there is a marginal decreasing effect of average district income on average school test scores. Thus, in affluent neighborhood test scores are higher, but increasingly less so. Of course, you can still try to estimate this with a linear population regression line as in Figure \ref{fig:scatterincome}, but this introduces a \textbf{bias}. The estimate does not capture that what you want. Namely, it now holds that \(E(u \mid X = x) \neq 0\), because for small \(X\), say \(X<10\), the residuals are negative, for medium sized \(X\)s most residuals are positive and for large \(X>40\) all residuals are negative again. So, there is a clear relation between \(X\) and \(u\) and they fail to be independent. This particular form of bias is coined \textbf{specification bias}. There is another issue here and that is that the effect on \(Y\) of a change in \(X\) depends on the value of \(X\)---that is, the \emph{marginal} effect of \(X\) is not constant.

To remedy the specification bias, we will use nonlinear regression population regression \textbf{functions} of \(X\), or we estimate a regression function that is nonlinear in \(X\). Here, it is important to see that we do so by \emph{transforming} \(X\), so the population regression `line'. The estimator still remain a linear regression model.

We will analyse below two complementary and often adopted approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using \textbf{polynomials} to transform \(X\). That means that the effect is approximated by a quadratic, cubic, or higher-degree polynomial. This approach as well governs to an extent so-called interaction effects which is a special case, where we multiply two different variables.
\item
  Using \textbf{logarithmic} transformations of \(X\), where \(Y\) and/or \(X\) is transformed by taking its logarithm. Here, the main focus is on the interpretation of the \(\hat{\beta}\)s, as they change from a unit increase interpretation to a percentages interpretation which often can be found useful.
\end{enumerate}

\hypertarget{polynomials}{%
\subsection{Polynomials}\label{polynomials}}

Our first approach to non-linear specification is applying polynomials of the variables that we suspect has a non-linear impact. If that is the independent variable \(X\), the we can construct the following \emph{linear regression} model by using polynomials:
\begin{equation}
Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X^2_i + \ldots + \beta_r X_i^r + u_i
\label{eq:poly}
\end{equation}
Note again that this is just the linear regression model---except that the regressors are powers of \(X\)! So, in effect we transform the data---actually create new variables \(X^r\)---, but the specification in parameters remains linear. Estimation, hypothesis testing, etc. proceeds as in the multiple regression model using OLS. However, the coefficients are now a bit more difficult to interpret. Consider the example of above about the relation between test scores average district income, where \(Income_i\) is defined as the average district income in the \(i^{\mathrm{th}}\) district (thousands of dollars per capita). For a quadratic specification, we specify the linear regression model as below:
\begin{equation}
TestScore_i = \beta_0 + \beta_1 Income_i + \beta_2 (Income_i)^2 + u_i
\end{equation}
For a cubic specification the linear regression model becomes:
\begin{equation}
TestScore_i = \beta_0 + \beta_1 Income_i + \beta_2 (Income_i)^2 +
\beta_3 (Income_i)^3 + u_i
\end{equation}

First, we focus on the estimation of the quadratic function. In \texttt{STATA} this would look like:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{reg}\NormalTok{ testscr c.avginc\#\#c.avginc, }\FunctionTok{r}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear regression                               Number of obs     =        420
                                                F(2, 417)         =     428.52
                                                Prob > F          =     0.0000
                                                R-squared         =     0.5562
                                                Root MSE          =     12.724

-----------------------------------------------------------------------------------
                  |               Robust
          testscr | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
------------------+----------------------------------------------------------------
           avginc |   3.850995   .2680941    14.36   0.000      3.32401    4.377979
                  |
c.avginc#c.avginc |  -.0423085   .0047803    -8.85   0.000     -.051705   -.0329119
                  |
            _cons |   607.3017   2.901754   209.29   0.000     601.5978    613.0056
-----------------------------------------------------------------------------------
\end{verbatim}

Now, it is straightforward to test the null-hypothesis of linearity against the alternative that the regression function is a quadratic. Namely, we only have to consider the \(t\)-statistic of the quadratic term. And that is larger than 1.96, so against a 5\% significance level we reject the null-hypothesis of linearity.

Note by the way the syntax \texttt{c.avginc\#\#c.avginc} which seems a bit strange. However, this particular line of code is very useful for later tabulation, plotting and other manipulations of the output. In this way \texttt{STATA} knows that there should be a quadratic effect of the same variable (\texttt{avginc}). The syntax \texttt{c.} denotes that the variable should be considered as continuous instead of as an integer (try it and behold the horrible output). There are four useful operators that you want to know when working with polynomials and interaction effect:

\begin{itemize}
\tightlist
\item
  \texttt{i.} operator: this specifies that the following variable is an integers and should be considered on all its level. This actually create indicator or dummies variables
\item
  \texttt{c.} operator: this specifies that the following variable is a continuous variables and should be treated as continuous.
\item
  \texttt{\#} binary operator that specifies an interaction between two variables
\item
  \texttt{\#\#} binary operator that specifies both interaction between two variables and the individual variable effect
\end{itemize}

Plotting, non-linear population regression lines are a bit tricky. Namely, you want to combine a pylonomial with a linear dimension. One way of doing this is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{ hat1 }
\KeywordTok{scatter}\NormalTok{ (testscr avginc) || (}\KeywordTok{line}\NormalTok{ hat1 avginc, }\KeywordTok{sort}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

where after the regression we \textbf{predict} the test scores (and name it something like \texttt{hat1}) and then we ask for a line of the prediction for each value of average district income. Note, though, that we have to \texttt{sort} the prediction from small to large to get a smooth line. And this provides the nice curved population regression line in the following \texttt{STATA} output.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/scatterqua} 

}

\caption{A non-linear relation}\label{fig:scatterqua}
\end{figure}

But what is now the marginal effect of average district income. That, now, depends on itself. Namely, \(\frac{\partial \text{testscore}}{\partial \text{income}} = \beta_1 + \beta_2 \text{income}\). Another way of seeing this is to compute the effects for different values of \(X\)
\begin{equation}
\widehat{TestScore_i} = 607.3 + 3.85 Income_i - 0.0423(Income_i)^2
\end{equation}
The predicted change in test scores for a change in income from \$5,000 per capita to \$6,000 per capita then amounts to:
\begin{eqnarray}
\Delta \widehat{TestScore} &=& 607.3 + 3.85 \times 6 -  0.0423 \times 6^2 \\
&& - (607.3 + 3.85\times 5 - 0.0423\times 5^2)\\
&=&3.4
\end{eqnarray}

And if calculate the predicted effects for different values of \(X\), then we get the following table:

\begin{table}

\caption{\label{tab:effectqua}Effect of $X$}
\centering
\begin{tabular}[t]{ll}
\toprule
Change in Income (1000 dollar per capita) & $\Delta \widehat{TestScore}$\\
\midrule
from 5 to 6 & 3.4\\
from 25 to 26 & 1.7\\
from 45 to 46 & 0.0\\
\bottomrule
\end{tabular}
\end{table}

Thus, the effect of a change in income is greater at low than high income levels (perhaps, a declining marginal benefit of an increase in school budgets?). But, be careful here! What is the effect of a change from 65 to 66? That is quite negative and already Figure \ref{fig:scatterqua} shows that a quadratic specification start to decline again the value of about 50; and perhaps that is not the behaviour that you want. So, with polynomials it is essential not to extrapolate outside the range of the data (and still interpret the outcome).

The estimation of a cubic specification is straightforward:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{reg}\NormalTok{ testscr c.avginc\#\#c.avginc\#\#c.avginc, }\FunctionTok{r}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear regression                               Number of obs     =        420
                                                F(3, 416)         =     270.18
                                                Prob > F          =     0.0000
                                                R-squared         =     0.5584
                                                Root MSE          =     12.707

--------------------------------------------------------------------------------------------
                           |               Robust
                   testscr | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
---------------------------+----------------------------------------------------------------
                    avginc |   5.018677   .7073504     7.10   0.000      3.62825    6.409103
                           |
         c.avginc#c.avginc |  -.0958052   .0289537    -3.31   0.001     -.152719   -.0388913
                           |
c.avginc#c.avginc#c.avginc |   .0006855   .0003471     1.98   0.049     3.26e-06    .0013677
                           |
                     _cons |    600.079   5.102062   117.61   0.000     590.0499     610.108
--------------------------------------------------------------------------------------------
\end{verbatim}

Where if we now want to test the null- hypothesis of linearity, then we have to have invoke an \(F\)-test. Namely, the alternative hypothesis is that the population regression is quadratic and/or cubic, that is, it is a polynomial of degree up to 3, so:

\begin{itemize}
\tightlist
\item
  \(H_0\): Coefficients on \(Income^2\) and \(Income^3 = 0\)
\item
  \(H_1\): at least one of these coefficients is nonzero.
\end{itemize}

And the outcome below shows that the null-hypothesis that the population regression is linear is rejected at the 5\% (and 1\%) significance level against the alternative that it is a polynomial of degree up to 3.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{test}\NormalTok{ avginc\#avginc avginc\#avginc\#avginc  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 ( 1)  c.avginc#c.avginc = 0
 ( 2)  c.avginc#c.avginc#c.avginc = 0

       F(  2,   416) =   37.69
            Prob > F =    0.0000
\end{verbatim}

\hypertarget{interaction-variables}{%
\subsection{Interaction variables}\label{interaction-variables}}

Using interaction variables is a special case of polynomial effects. Namely, instead of multiply a variable with itself \(X\times X = X^2\), you now multiple a variable with another variable. And you want to do this to take into account interactions between independent variables. Assume, for example, that a class size reduction is more effective in some circumstances than in others (which is quite conceivable). Perhaps smaller classes help more if there are many English learners (i.e., large migrant communities), who need more individual attention. That is, \(\frac{\partial TestScore}{\partial STR}\) might depend on \(PctEL\). More generally, this subsection looks into the fact that the marginal effect of \(\frac{\partial Y}{\partial X_1}\) might depend on some other variable \(X_2\).

\hypertarget{interactions-between-two-binary-variables}{%
\subsubsection{Interactions between two binary variables}\label{interactions-between-two-binary-variables}}

First, we look into the simplest (and perhaps most insightful) case of two binary (dummy variables). Consider therefore the following linear regression model:
\begin{equation}
Y_i =\beta_0 +\beta_1 D_{1i} + \beta_2 D_{2i} +u_i, 
\end{equation}
where both \(D_{1i}\) and \$ D\_\{2i\}\$ are now considered to be binary. Now, of course, \(\beta_1\) is the effect of changing \(D_1=0\) to \(D_1=1\). So, in this specification, this effect doesn't depend on the value of \(D_2\). To allow the effect of changing \(D_1\) to depend on \(D_2\), we have to include the interaction term \(D_{1i} \times D_{2i}\) as a regressor:
\begin{equation}
Y_i =\beta_0 +\beta_1 D_{1i} + \beta_2 D_{2i} + \beta_3 (D_{1i} \times D_{2i}) + u_i
\end{equation}

To interpret now the coefficient \(\beta_1\) we comparing the two cases for \(D_1=0\) to \(D_1=1\)''
\begin{eqnarray}
E(Y_i|D_{1i}=0, D_{2i}=d_2) &=& \beta_0 + \beta_2 d_2 \\
E(Y_i|D_{1i}=1, D_{2i}=d_2) &=& \beta_0 + \beta_1 + \beta_2 d_2 + \beta_3 d_2 
\end{eqnarray}
If we now subtract them from each other:
\begin{equation}
E(Y_i|D_{1i}=1, D_{2i}=d2) - E(Y_i|D_{1i}=0, D_{2i}=d_2) = \beta_1 + \beta_3 d_2
\end{equation}
then we have the marginal effect of \(D_1\) which now depend \(d_2\). The interpretation of \(\beta_3\) boils down to being incremental to the effect of \(D_1\), when \(D_2 = 1\)

Let us go back to our Californian school district example with the following variables to be used: test scores, student teacher ratio, and English learners. Let:
\begin{eqnarray}
HiSTR &=& 1 \text{ if } STR \geq 20 \text{ and } HiEL = 1 \text{ if }
PctEL \geq 10 \\
HiSTR &=& 0 \text{ if } STR < 20 \text{ and } HiEL = 0 \text{ if }
PctEL < 10 \\
\end{eqnarray}
And if we have the estimation results we get the following outcome.
\begin{equation}
\widehat{TestScore} = 664.1 - 18.2 HiEL - 1.9 HiSTR - 3.5(HiSTR \times
HiEL)
\end{equation}
So, how to interpret the various parameters? Perhaps the simple way is to construct the following two-by-two table:

\begin{table}

\caption{\label{tab:intdummies}Interpretation of interaction effects with dummies}
\centering
\begin{tabular}[t]{lll}
\toprule
 & $HiEL = 0$ & $HiEL = 1$\\
\midrule
$HiSTR = 0$ & $664.1$ & $664.1 - 18.2 = 645.9$\\
$HiSTR = 1$ & $664.1 - 1.9 = 662.2$ & $664.1 - 1.9 - 18.2 - 3.5= 640.5$\\
\bottomrule
\end{tabular}
\end{table}

Now, Table \ref{tab:intdummies} specifies for each combination (and there are exactly four of them) of \(HiSTR\) and \(HiEL\) the average expected test score outcome. Clearly, there are different `marginal' effect of \(HiSTR\). Namely, the effect of \(HiSTR\) when \(HiEL = 0\) is \(-1.9\), whilst the effect of \(HiSTR\) when \(HiEL = 1\) is \(-1.9 - 3.5 = -5.4\). This clearly points out that a class size reduction is estimated to have a bigger effect when the percent of English learners is large. However, when you estimate in \texttt{STATA} then you see that this interaction is not statistically significant, because the \(t\)-statistic equals \(3.5/3.1 = 1.1\)

\hypertarget{interactions-between-continuous-and-binary-variables}{%
\subsubsection{Interactions between continuous and binary variables}\label{interactions-between-continuous-and-binary-variables}}

The second case we consider is between a continuous and a binary variable. First assume the following regression model:
\begin{equation}
Y_i =\beta_0 + \beta_1 X_i + \beta_2 D_i + +u_i, 
\end{equation}
where \(D_i\) is a binary variable and \(X\) is a continuous variable. As specified above, the effect on \(Y\) of \(X\) (holding \(D\) constant) = \(\beta_1\), which does not depend on \(D\). To allow the effect of \(X\) to depend on \(D\), we can include the interaction term \(D_i \times X_i\) as a regressor:
\begin{equation}
Y_i =\beta_0 + \beta_1 X_i + \beta_2 D_i  + \beta_3 (D_i \times X_i) + u_i
\end{equation}

What this binary-continuous interaction does is essential create two different population regression lines. Namely, for observations with \(D_i= 0\) (the \(D = 0\) group or the \(D=0\) regression line) there is:
\begin{equation}
Y_i = \beta_0 + \beta_1 X_i  + u_i,
\end{equation}
Whilst for observations with \(D_i= 1\) (the \(D = 1\) group or the \(D = 1\) regression line) the regression line comes down to:
\begin{eqnarray}
Y_i &=&   \beta_0 + \beta_2 + \beta_1 X_i + \beta_3 X_i + u_i \\
            &=&  (\beta_0 + \beta_2) + (\beta_1 + \beta_3) X_i + u_i
\end{eqnarray}

And these two population regression lines might both differ in the level (the constant) and in the slope of the line. So, there are three possibilities as depicted in Figure \ref{fig:interaction}

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Sheet44} 

}

\caption{Three possible binary-continuous interaction outcomes}\label{fig:interaction}
\end{figure}

In the first panel (a), \(\beta_3 = 0\), so there is only a level effect. In the second panel (b), both \(\beta_2\) and \(\beta_3\) are not 0, so there is both a level and a slope effect. The last panel indicates that \(\beta_2 = 0\), meaning that there is only a slope effect. But how to interpreting the coefficients now? Therefore, we take the marginal effect of
\begin{equation}
Y =\beta_0  + \beta_1 X  +\beta_2 D+ \beta_3 (D \times X)
\end{equation}
which yields:
\begin{equation}
\frac{\partial Y}{\partial X} = \beta_1 + \beta_3 D
\end{equation}
Thus, the effect of \(X\) depends on \(D\) and \(\beta_3\) is the increment to the effect of \(X\), when \(D = 1\) (a slope effect)

To see this in our Californian school district example we now use the variables test scores, student teacher ratio and the as previously defined dummy variable \(HiEL\) as:
\begin{equation}
\widehat{TestScore} = 682.2 - 0.97 STR + 5.6 HiEL - 1.28(STR \times HiEL) 
\end{equation}
Now when \(HiEL = 0\) the population regression line amounts to:
\begin{equation}
\widehat{TestScore} = 682.2 - 0.97 STR 
\end{equation}
And when \(HiEL = 1\) the population regression line is:
\begin{eqnarray}
\widehat{TestScore} &=& 682.2 - 0.97 STR + 5.6 - 1.28 STR \\
&=& 687.8 - 2.25 STR
\end{eqnarray}
Thus we have two regression lines: one for each \(HiSTR\) group. And the conclusion is that a class size reduction is estimated to have a larger effect when the percent of English learners (migrant communities) is large.

Hypothesis testing is a before. To test whether the two regression lines have the same slope, the null-hypothesis boils down to the coefficient of \(STR \times HiEL\) being zero: the \(t\)-statistic of this one become \(-1.28/0.97 = -1.32\) and thus we do not reject this test. To test whether the two regression lines have the same intercept, the null-hypothesis becomes the coefficient of \(HiEL\) being zero, yielding: \(t = -5.6/19.5 = 0.29\), so we do not reject that null-hypothesis either. Interestingly, the null-hypothesis that the two regression lines are the same---population coefficient on \(HiEL = 0\) and population coefficient on yields \(STR \times HiEL = 0\): \(F = 89.94 (p-value < .001)\). So, we reject the joint hypothesis but neither individual hypothesis.

\hypertarget{interactions-between-two-continuous-variables}{%
\subsubsection{Interactions between two continuous variables}\label{interactions-between-two-continuous-variables}}

The last case are interaction between two continuous variables and that is always a difficult case of interpret. Starting again with the model:
\begin{equation}
Y_i =\beta_0 + \beta1 X_{1i} +\beta_2 {X_{2i}} +u_i,
\end{equation}
where both \(X_1\), \(X_2\) are continuous and as specified, the effect of \(X_1\) doesn't depend on \(X_2\) and the effect of \(X_2\) doesn't depend on \(X_1\). Now, to allow the effect of \(X_1\) to depend on \(X_2\), we include the interaction term \(X_{1i} \times X_{2i}\) as a regressor. Where, to interpret the coefficients, we take the first derivative of \(X_1\) in:
\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 (X_{1i}
\times X_{2i}) + u_i
\end{equation}
which yields:
\begin{equation}
\frac{\partial Y}{\partial X} = \beta_1 + \beta_3 X_2
\end{equation}
where \(\beta_3\) should be interpreted as the increment to the effect of \(X_1\) from a unit change in \(X_2\).

\hypertarget{logarithmic-transformation}{%
\subsection{Logarithmic transformation}\label{logarithmic-transformation}}

To incorporate non-linear effect, very often logarithmic transformations are used of \(Y\) and/or \(X\), where typically we use \(\ln(X)\) as the natural logarithm of \(X\). One feature of logarithmic transformations is that they permit modeling relations in percentage terms (like elasticities), rather than linearly. That is because:
\begin{equation}
\ln(x+\Delta x) - \ln(x) = \ln (1 + \frac{\Delta x}{x}) \cong \frac{\Delta x}{x}
\end{equation}
Note that this is an approximation, but from calculus we know that \(\frac{d \ln(x)}{dx}=\frac{1}{x})\). And the above approximation works quite well for small numbers. For example, numerically: \(\ln(1.01) = .00995 \cong .01\) and \(\ln(1.10) = .0953 \cong .10\), where the latter is still rather close. Now remember the following rules for natural logarithms
1. \(\ln(a\times b)= \ln(a)+\ln(b)\)
2. \(\ln(\frac{a}{b}) =\ln(a) - \ln(b)\)
3. \(\ln(a^\alpha) = \alpha \ln(a)\)
4. \(\ln(e^X) = X\).

When you encounter a nonlinear model such as the ones adopted in Chapter \ref{surplus} a strategy that often works is log-linearization. That works as follows
\begin{equation}
Y = A K^\alpha L^{1-\alpha} \rightarrow \ln(Y) = \ln(A) + \alpha \ln(K) + (1-\alpha) \ln(L), 
\end{equation}
where you take the natural logarithm on both sides. There are three different cases of logarithmic regression models as specified in Table \ref{tab:logspecification}.

\begin{table}

\caption{\label{tab:logspecifications}Three logarithmic transformation}
\centering
\begin{tabular}[t]{ll}
\toprule
Case & Population regression model\\
\midrule
linear-log & $Y_i=\beta_0 + \beta_1 \ln(X_i) + u_i$\\
log-linear & $\ln(Y_i)=\beta_0 + \beta_1 (X_i) + u_i$\\
log-log & $\ln(Y_i)=\beta_0 + \beta_1 \ln(X_i) + u_i$\\
\bottomrule
\end{tabular}
\end{table}

Though statistical testing remains the same, the interpretation of the slope coefficient differs in each case. To derive the interpretationwe want to find the marginal effect of \(X\) using the first derivative.

\hypertarget{linear-log-population-regression-model}{%
\subsubsection{Linear-log population regression model}\label{linear-log-population-regression-model}}

The linear-log population regression model is specified as:
\begin{equation}
    Y = \beta_0 + \beta_1 \ln(X)
\end{equation}
Now take the first derivative:
\begin{equation}
    \frac{\partial Y}{\partial X} = \frac{\beta_1}{X} 
\end{equation}
so
\begin{equation}
    \beta_1  = \frac{\partial Y}{\partial X / X} 
\end{equation}
In this case that means that \(\beta_1\) should be interpreted as the absolute change of \(Y\) when \(X\) changes with \(\beta_1/100\) percent. To illustrate this, consider the case where we take natural logarithm od district income, so we define the new regressor as, \(\ln(Income)\)

The model is now linear in \(\ln(Income)\), so the linear-log model can be estimated by OLS, which yields
\begin{equation}
        \widehat{TestScore} = 557.8 + 36.42\times \ln(Income_i)
\end{equation}
so an 1\% increase in \(Income\) is associated with an increase in test scores of 0.36 points on the test. And again, standard errors, confidence intervals, \(R^2\)---all the usual tools of regression apply here. But the difficulty in plottin the new regression line remains. Consider the following \texttt{STATA} syntax, where we first have to define the new regressor by invoking the \texttt{generate} command.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{gen}\NormalTok{ lninc = }\FunctionTok{ln}\NormalTok{(avginc)}
\KeywordTok{reg}\NormalTok{ testscr lninc, }\FunctionTok{r}
\KeywordTok{predict}\NormalTok{ testhat}
\KeywordTok{graph} \KeywordTok{twoway}\NormalTok{ (}\KeywordTok{line}\NormalTok{ testhat avginc, }\KeywordTok{sort}\NormalTok{) (}\KeywordTok{scatter}\NormalTok{ testscr avginc)}
\end{Highlighting}
\end{Shaded}

This now provides the following \texttt{STATA} output.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/scatterlnincome} 

}

\caption{A non-linear relation}\label{fig:scatterlnincome}
\end{figure}

When you compare \ref{fig:scatterlnincome} with \ref{fig:scatterqua} then you notice that in the case of logarithm the population remains increasing (but less and less steep). This can be considered as an advantage when you want to estimate decreasing (or increasing) return.

\hypertarget{log-linear-population-regression-model}{%
\subsubsection{Log-linear population regression model}\label{log-linear-population-regression-model}}

The second case we consider is the log-linear population regression model, as specified by:
\begin{equation}
    \ln(Y) = \beta_0 + \beta_1 X
\end{equation}
To find the interpretation of \(\beta1_1\), we again take the first derivative \(\frac{\partial Y}{\partial X}\), but first transform the model like this:
\begin{equation}
    Y = exp( \beta_0 + \beta_1 X )
\end{equation}
then take the first derivative:
\begin{equation}
    \frac{\partial Y}{\partial X} = \beta_1  exp( \beta_0 + \beta_1 X ) = \beta_1 Y
\end{equation}
and collec terms
\begin{equation}
    \beta_1  = \frac{\partial Y / Y}{\partial X } 
\end{equation}

The interpretation of \(\beta_1\) now is that one unit change in \(X\) causes a \(\beta_1\) percentage in \(Y\)

\hypertarget{log-log-population-regression-model}{%
\subsubsection{Log-log population regression model}\label{log-log-population-regression-model}}

Finally, we have our third case, being the log-log population regression model as specified by:
\begin{equation}
    \ln(Y) = \beta_0 + \beta_1 \ln(X)
\end{equation}

To find the interpretation of \(\beta1_1\), we again take the first derivative \(\frac{\partial Y}{\partial X}\), but first transform the model like this:
\begin{equation}
    Y = exp( \beta_0 + \beta_1 \ln(X) )
\end{equation}
So
\begin{equation}
    \frac{\partial Y}{\partial X} = \beta_1 /X  exp( \beta_0 + \beta_1 \ln(X) ) = \beta_1 Y /X
\end{equation}
and after collecting terms we end up with an \textbf{elasticity}:
\begin{equation}
    \beta_1  = \frac{\partial Y / Y}{\partial X / X } 
\end{equation}

As an example consider the case when we want to regress ln(test scores) on ln(income). To do so, we first define a new dependent variable, ln(TestScore), and a new regressor, ln(Income)
The model is now a linear regression of ln(TestScore) against ln(Income), which can be estimated by OLS as follows
\begin{equation}
\widehat{ln(TestScore)} = 6.336 + 0.0554 \times ln(Income_i),
\end{equation}
where the interpretation is that an 1\% increase in \(Income\) is associated with an increase of .0554\% in \(TestScore\) (\(Income\) up by a factor of 1.01, \(TestScore\) up by a factor of 1.000554)

Suppose that we now want to plot both the log-linear and the log-log specification, then we can use the following syntax:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{gen}\NormalTok{ lninc = }\FunctionTok{ln}\NormalTok{(avginc)}
\KeywordTok{gen}\NormalTok{ lntestscr = }\FunctionTok{ln}\NormalTok{(testscr)}
\KeywordTok{reg}\NormalTok{ lntestscr lninc, }\FunctionTok{r}
\KeywordTok{predict}\NormalTok{ testhat1}
\KeywordTok{reg}\NormalTok{ lntestscr avginc, }\FunctionTok{r}
\KeywordTok{predict}\NormalTok{ testhat2}
\KeywordTok{graph} \KeywordTok{twoway}\NormalTok{ (}\KeywordTok{line}\NormalTok{ testhat1 avginc, }\KeywordTok{sort}\NormalTok{) (}\KeywordTok{line}\NormalTok{ testhat2 avginc, }\KeywordTok{sort}\NormalTok{) (}\KeywordTok{scatter}\NormalTok{ lntestscr avginc), }\BaseNTok{legend}\NormalTok{(}\KeywordTok{order}\NormalTok{(1 }\StringTok{"log{-}log specification"}\NormalTok{ 2 }\StringTok{"log{-}linear specification"}\NormalTok{ 3 }\StringTok{"Observations"}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

which provides the following \texttt{STATA} output.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/scattercompare} 

}

\caption{A non-linear relation}\label{fig:scattercompare}
\end{figure}

Note that the \(y\)-axis is on a logarithmic scale here, thus the log-linear specification is now a linear line.

\hypertarget{summary-logarithmic-transformations}{%
\subsubsection{Summary: logarithmic transformations}\label{summary-logarithmic-transformations}}

We have seen three different cases of logarithmic specification, differing in whether \(Y\) and/or \(X\) is transformed by taking logarithms. Now, the regression is linear in the new variable(s) \(\ln(Y)\) and/or \(\ln(X)\), and the coefficients can be estimated by OLS where hypothesis tests and confidence intervals are now implemented and interpreted `as usual'. Only the interpretation of the coefficients differs from case to case and is directly related to percentage changes (growth) and elasticities. Oftentimes, the choice of specification, however, should be guided by judgment (which interpretation makes the most sense in your application?), tests, and plotting predicted values. Sometimes, though, you have a structural economic model such as Equation \eqref{eq:directutility}, which defines the type of specification you should use. Finally, see that in economics many models exists with decreasing or increasing return to scale and that these are very closely related with logarithmic specifications.

\hypertarget{sec:fixedeffects}{%
\section{Using fixed effects in panel data}\label{sec:fixedeffects}}

Multivariate regression is a powerfull tool for controlling for the effect of variables for which we have data. But often we do not have data on what we suspect might be important---data, such as individual characteristics like ambition, intelligence, drive or stamina. Or regional of country data, where the type of soil, the ruggedness (hilliness), or population density determine to a large extent the behaviour of people living on it. If we do not have this type of data, then it not always the case that everything is lost. Especially, when we have repeated observations, so observations of the same entity throughout time. This is referred to as panel data and requires one additional subscript \(t\) as in \(X_{it}\) indicating the observation \(X\) on individual made at time \(t\). To understand why this sometimes works, we temporarily change to another dataset and that is the `fatality' data collected by Levitt and Porter (2001) and deals with the relation between drunk driving and fatal accidents in the States of the US between 1982 and 1988. For this particular example we look at the impact of the `beer tax', measured as the real tax in dollars on a case of beer, on `fatality', measured as the number of annual traffic deaths per 10,000 people in the population of each stata. For this we first read the data and manipulate the mortality variable

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{use} \StringTok{"./data/fatality.dta"}\NormalTok{, }\KeywordTok{clear}
\KeywordTok{gen}\NormalTok{ fatality = allmort/pop * 10000}
\end{Highlighting}
\end{Shaded}

and then run a simple regression:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{regress}\NormalTok{ fatality beertax, }\KeywordTok{robust}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear regression                               Number of obs     =        336
                                                F(1, 334)         =      47.59
                                                Prob > F          =     0.0000
                                                R-squared         =     0.0934
                                                Root MSE          =     .54374

------------------------------------------------------------------------------
             |               Robust
    fatality | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
     beertax |   .3646054   .0528524     6.90   0.000     .2606399     .468571
       _cons |   1.853308   .0471297    39.32   0.000     1.760599    1.946016
------------------------------------------------------------------------------
\end{verbatim}

But these outcomes are very strange. For every dollar increase in tax, number of fatal accidents per 10,000 people increases with 0.36, which is statistically significantly different from 0. What is going on here. Most likely this effect is biased because of omitted variable bias. States in the US differ widely in terms of population density, environment, institutions, religion, poverty, and so on and so forth. And Those state characteristics might influence both the variables beertax and fatality.

Fortunately, for each state we have yearly data. So, that is 7 observations per stata and we can make use of that by using fixed effects, which is a very common technique in the social sciences---especially in economics. We model the use of fixed effects in this example as follows:
\begin{equation}
\text{fatality}_{it} = \beta_0 + \beta_1\text{beertax}_{it} + \beta_3 S_1 + \ldots + \beta_51 S_{48} + u_{it}, 
\end{equation}
where \(S_i\) denote indicator (dummies) for each state which constitute the fixed effects. In total there are 48 states in this dataset, so we have 48 dummies. Note that these fixed effects only depend on the state variation, not on time variation. So, essentially what these fixed effects capture is all state specific characteristics which are constant over time. And most of the characteristics' examples given above do not vary that much over time, so by using these state fixed effects we can \textbf{control} for them. In \texttt{STATA} you can estimate this in a straightforward way as \texttt{regress\ fatality\ beertax\ i.state,\ robust}, but this lots of statistical output that you are usually not interested in. Almost just as easy would be is to invoke the \texttt{areg} command, where you specifically state that the state variable should be used as dummies but not shown using \texttt{absorb(state)}:
and then run a simple regression:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{areg}\NormalTok{ fatality beertax, absorb(state) }\KeywordTok{robust}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear regression, absorbing indicators             Number of obs     =    336
Absorbed variable: state                            No. of categories =     48
                                                    F(1, 287)         =  10.41
                                                    Prob > F          = 0.0014
                                                    R-squared         = 0.9050
                                                    Adj R-squared     = 0.8891
                                                    Root MSE          = 0.1899

------------------------------------------------------------------------------
             |               Robust
    fatality | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
     beertax |  -.6558737   .2032797    -3.23   0.001    -1.055982   -.2557655
       _cons |   2.377075   .1051516    22.61   0.000     2.170109    2.584041
------------------------------------------------------------------------------
\end{verbatim}

Now, see what happens with the coefficient of the beer tax variable. It changes sign! So from positive it becomes negative. That is how \textbf{disruptive} omitted variable bias can be. Also see that by including all these state fixed effects, the \(\bar{R^2}\) now increase enormously to 91\%, which does make sense because the states explain the variation in fatality rate to a large extent (e.g., compare Kansas with Connecticut).

This is just a snapshot of the use of fixed effects in panel data, but for now this is enough. But for now, know that the use of fixed effects can go a long way in addressing omitted variable bias.

\hypertarget{conclusion-and-discussion-2}{%
\section{Conclusion and discussion}\label{conclusion-and-discussion-2}}

\hypertarget{specification}{%
\chapter{Specification and Assessment Issues}\label{specification}}

This concluding chapter deals with an epistemological question, namely how to convey your key results, and an ontological question, how do you know whether what you convey is true (e.g., unbiased). To do so, we first look at specification issues in Section \ref{sec:specificationmodel}. Which variables should you include? Thereafter, Section \ref{sec:presentation} discusses how to present your results. And, subsequently, Section \ref{sec:sourcesbias} gives a bit into the question into the validity of your results. When do you know that you really obtained estimated an \textbf{unbiased} parameter. The final section concludes.

\hypertarget{sec:specificationmodel}{%
\section{Specification of your model}\label{sec:specificationmodel}}

A long-standing but simple question is how to decide which variables to include in a regression model. Unfortunately, the answer to this question is rather complex. A straightforward but naive approach would be to include them all! So, throw every variable in that is in your database. This, however, leads to ``causal salad'' (McElreath 2020) as displayed in Figure \ref{fig:causalsalad} and can actually lead to a biased estimator. One reason for this is that if you include a variable that is related to the error term then all other parameters are biased as well.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/causalsalad} 

}

\caption{Causal salad}\label{fig:causalsalad}
\end{figure}

So, for the final time we return to our Californian school district data and now try to devise a specification that \emph{mimimizes} the changes upon biased estimator. So, our focus is to get an unbiased estimate of the effect on test scores of changing class size, holding constant student and school characteristics but not necessarily holding constant the budget (we do not want to control for budget as this actually governs class sizes).

To do this we need to think about what variables to include and what regressions to run---and we should do this before we actually sit down at the computer. Think beforehand about your model specification and try to avoid throwing everything in (your causal salad).

In practice, and especially economics, most follow the following general approach to variable selection and model specification:
1. First you specify a base or benchmark model. In this case that is the univariate regression of test scores on class size.
2. Then you specify a range of plausible alternative models, which include additional candidate variables.
3. Then you assess whether a candidate variable changes the coefficient of interest (\(\hat{\beta}_1\))? You keep focusing on the effect of class size!
4. You assess whether a candidate variable is statistically significantly different from zero; so whether it has an impact of not.
- Use judgment, not a mechanical recipe, meaning that variable statistically insignificant different from zero should not automatically be thrown out.
- In all cases, do not just try to maximize \(\bar{R^2}\). You focus on a causal effect, not on prediction

Considering the last point, tt is easy to fall into the trap of maximizing the \(\bar{R^2}\)---but this loses sight of our real objective, an unbiased estimator of the class size effect. Recall that a high \(\bar{R^2}\) means that the regressors explain the variation in \(Y\). It does \textbf{not} mean

\begin{itemize}
\tightlist
\item
  that you have eliminated omitted variable bias;
\item
  that you have an unbiased estimator of a causal effect \((\beta_1)\);
\item
  that the included variables are statistically significant.
\end{itemize}

So, in this case, what variables would you want---ideally---to include to estimate the effect on test scores of \(STR\) using school district data?

There is a whole set of potential relevant variables in the California class size data set, being:

\begin{itemize}
\tightlist
\item
  student-teacher ratio (\(STR\))---the variable we focus on
\item
  percent English learners in the district (\(PctEL\))---as a proxy for large migrant communities
\item
  school expenditures per pupil---largely correlated with student-teacher ratio
\item
  name of the district (so we could look up average rainfall, for example)
\item
  percent eligible for subsidized/free lunch---proxies district income
\item
  percent on public income assistance---proxies district income
\item
  average district income---a measure for district affuency
\end{itemize}

So, which of these variables would you want to include?

\begin{figure}

{\centering \includegraphics[width=800px]{./figures/cadata} 

}

\caption{Test scores versus various independent variables}\label{fig:cadata}
\end{figure}

Looking at Figure \ref{fig:cadata}, all three percentage variables (English learners, subsidized lunch, and income assistance) behave in a similar manner. But interestingly, the strongest relation is between subsidized lunch and test scores and that is at least the variable that we would like to include.

\hypertarget{sec:presentation}{%
\section{Presentation of results}\label{sec:presentation}}

So, we have a number of regressions (also called specifications) and we want to report them. Often, it is awkward and difficult to read regressions written out in equation form, so instead it is conventional to report them in a table. Note that reading regression estimates from computer output is even more difficult. On top of that it is ugly and contains way too much information. Try to avoid statistical computer output as much as possible---at least in your thesis. Now, regression tables should include a couple of elements:

\begin{itemize}
\tightlist
\item
  The estimated regression coefficients.
\item
  The standard errors or the \(t\)-statistics. Having both of them is too much. Do not report \(p\)-value, because often they are not informaties (being \(p = 0.000\)).
\item
  Some measures of fit (usually just the \(\bar{R^2}\) would do).
\item
  The number of observations.
\item
  Some relevant \(F\)-statistics, if any. Usually they are not included.
\item
  Any other pertinent information but typically there is none.
\end{itemize}

You can find most of this information in the final estimation Table \ref{fig:catable} as presented in Stock, Watson, et al. (2003).

\begin{figure}

{\centering \includegraphics[width=800px]{./figures/catable} 

}

\caption{Various specifications of test score models}\label{fig:catable}
\end{figure}

So, here the variable of interest (student-teacher ratio) is the first variable on top. And the table keeps focusing on that one. Moreover, specification (3) and (5) seems to be preferred as they have the highest \(\bar{R^2}\), although that is perhaps of lesser importance. What we can infer from this is that the estimate for student-teacher ratio remains robust around \(-1\) and is significantly different from \(0\). Does this now mean that this effect is \textbf{unbiased}? Most likely not, but that is something that the next section will discuss.

\hypertarget{sec:sourcesbias}{%
\section{Potential sources of bias}\label{sec:sourcesbias}}

No to include we would like to answer the question whether there is a systematic way to assess regression studies? We already have seen that multivariate regression models have some key virtues:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  They provide an estimate of the marginal effect of the variable of interest \(X\) on \(Y\).
\item
  They resolve the problem of omitted variable bias, if an omitted variable can be measured and included.
\item
  They can handle nonlinear relations (effects that vary with the \(X\)'s) and therefore resolve the problem of misspecification bias.
\end{enumerate}

Still, OLS might yield a \textbf{biased} estimator of the true causal effect. In other words, it might not yield valid inferences. That what you want to measure is not what you actually measure. In general there is two ways to assess statistical studies: threats to internal and threats to external validity.

\begin{itemize}
\tightlist
\item
  \textbf{Internal validity}: the statistical inferences about causal effects are valid for the population being studied.
\item
  \textbf{External validity}: the statistical inferences can be generalized from the population and setting studied to other populations and setting.
\end{itemize}

\hypertarget{threats-to-external-validity}{%
\subsection{Threats to external validity}\label{threats-to-external-validity}}

So, above we came to a (tentative) conclsusion about the impact of class size on test scores. But we have done so in the context of Californian school districts in the year 2005. Can we extend this finding and generalize class size results from California school districts to other population, for example to that of Massachusetts or Mexico in 2005? And can we do so for differences in institutional settings as there are different legal requirements concerning special education, different treatment of bilingual education, and differences in teacher characteristics across regions and countries.

We therefore always to be careful to transfer our finding to that of other settings. Note that this as well a special case of omitted variable bias but now outside the scope of our study (our population).

\hypertarget{threats-to-internal-validity}{%
\subsection{Threats to internal validity}\label{threats-to-internal-validity}}

In applied econometrics, the following five threats to the internal validity of regression studies are usually given (in statistics there is a different framework for this, but in most cases they come down to the same thing)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Omitted variable bias
\item
  Wrong functional form
\item
  Errors-in-variables bias or measurement error
\item
  Sample selection bias
\item
  Simultaneous causality bias
\end{enumerate}

All of these imply that \(E(u_i|X_{1i},\ldots,X_{ki}) \neq 0\), in which case the OLS estimates are therefore \textbf{biased}.

\hypertarget{omitted-variable-bias}{%
\subsubsection{Omitted variable bias}\label{omitted-variable-bias}}

Omitted variable bias arises if an omitted variable is both a determinant of \(Y\) and a determinant of at least one included regressor. We first discussed omitted variable bias in regression with a single \(X\), but Omitted variable bias will arise when there are multiple \(X\)'s as well, if the omitted variable satisfies the two conditions above. Fortunately, there are potential solutions to omitted variable bias

\begin{itemize}
\tightlist
\item
  If the variable can be measured, include it as an additional regressor in multiple regression;
\item
  Possibly, use panel data in which each entity (individual) is observed more than once;
\item
  If the variable cannot be measured, use instrumental variables regression (for later courses);
\item
  Run a randomized controlled experiment.
\end{itemize}

\hypertarget{wrong-functional-form}{%
\subsubsection{Wrong functional form}\label{wrong-functional-form}}

This threat to internal validity arises if the functional form is incorrect. For example, if an interaction term is incorrectly omitted, then inferences on causal effects will be biased. There is a potential solution to functional form misspecification and that is to use the appropriate nonlinear specifications in \(X\) (logarithms, interactions, etc.). Sometimes this is not possible and then one has to resort to direct non-linear estimation techniques.

\hypertarget{errors-in-variables-bias-or-measurement-error}{%
\subsubsection{Errors-in-variables bias or measurement error}\label{errors-in-variables-bias-or-measurement-error}}

The third threat is measurement error or sometimes know as Errors-in-variables bias. So far we have assumed that \(X\) is measured without error. In reality, (economic) data is often measured with error have measurement error. Especially surveys are prone to measurement error. For example recollection errors that arise with questions as ``which month did you start your current job?''. Or ambiguous questions problems as ``what was your income last year?'' What is meant with latter: monthly or yearly income, gross or net income? Also respondents sometimes have an incentive not to answer honestly (intentionally false response problems) with questions as ``What is the current value of your financial assets?'' or ``How often do you drink and drive?''. There are potential solutions to errors-in-variables bias, such as

\begin{itemize}
\tightlist
\item
  Obtain better data, but that is bit easy
\item
  Develop a specific model of the measurement error process. This is only possible if a lot is known about the nature of the measurement error---for example a subsample of the data are cross-checked using administrative records and the discrepancies are analyzed and modeled.
\item
  Instrumental variables regression.
\end{itemize}

\hypertarget{sample-selection-bias}{%
\subsubsection{Sample selection bias}\label{sample-selection-bias}}

So far we have assumed simple random sampling of the population. In some cases, simple random sampling is thwarted because the sample, in effect, \textbf{selects itself}. Now, then sample selection bias arises when a selection process both influences the availability of data and if that process is related to the dependent variable. To illustrate this, I will adopt a hypothetical example given by McElreath (2020). Here we want to look at the relation between trustworthy science and newsworthy science. This example is motivated by the fact that newsworthy science (clickbait in the social media) oftentimes turns out not to be true. To given a reason why this might, we first simulate an artificial database of 400 observations of both newsworthy and trustworthy. Both variable are constructed such that they are \(i.i.d.\) and standard normally distributed. So, there is no relation whatsoever and, indeed, Figure \ref{fig:Rplot} shows a rather random cloud plot.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Rplot} 

}

\caption{Random observations of newsworthy and trustworthy}\label{fig:Rplot}
\end{figure}

But what if editors on social media have a decision rule: scientific output should be either thrustworthy or newsworthy, and preferably both. So, as a rule of thumb the select only the top 10\% scientific outcomes , so the ones that score in the top 10\% when both scores are added up (\(\text{trustworthy} + \text{newsworthy}\)). If we now depict the selected ones in grey in Figure \ref{fig:Rplot01}, then clearly suddenly a negative relation emerges between newsworthiness and thrustworthiness. And that negative relation is caused by the selection (external) editors make. So, if there is a selection somewhere in the process, estimates of what you want to estimates can quickly become biased.

\begin{figure}

{\centering \includegraphics[width=600px]{./figures/Rplot01} 

}

\caption{Negative relation amongst the selected points}\label{fig:Rplot01}
\end{figure}

This process occurs more often than you might think. Consider the two following examples:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Aircraft noise externality}. Here the question is to what extent people ``value'' aircraft noise (that is in a negative sense)? To aim for an answer we adopt the following empirical strategy; we collect housing prices close to Schiphol airport (say Zwanenburg) and compare them with identical houses further away (say Schagen). We have data for individual housing prices (including characteristics) since 1985. As an estimator we assess then the average mean difference between the Zwanenburg and Schagen location. Now the question is whether there is sample selection bias. And indeed there is and that is caused by the fact that humans react on their own situation based upon their preferences. In this case, they react by means of moving residence. So, those who have strong negative preference regarding aircraft noise are the first to move out Zwanenburg (if possible). So the population in both locations is not identical, instead they sorted spatially.
\item
  \textbf{Returns to education}. The question here is rather straightforward and involved the monetary returns to an additional year of education. As empirical strategy we collect data of all employed workers in the Netherlands (actually this data exists and is called micro-data), including worker characteristics, years of education, and hourly wages. Our approach is here to regress \(\ln(Earnings)\) on \(YearsEducation\) and a large set of other characteristics. Now, ignore issues of omitted variable bias and measurement error, then the question is: is there sample selection bias? And, indeed, there is again, as you only sample those people who are employed and not the unemployed (they have no current wage). And this leads to a different population than you wanted in the first place.
\end{enumerate}

In there there are some potential solutions to sample selection bias and most of them deal with data issues. For example, you might want to collect the sample in a way that avoids sample selection. For example you might want to focus on those people who moved between Schagen and Zwanenburg or you include the unemployed as well in the returns to education example.

\hypertarget{simultaneous-causality-bias-in-equations}{%
\subsubsection{Simultaneous causality bias in equations}\label{simultaneous-causality-bias-in-equations}}

Finally, our last threat to causality is simultaneous or reverse causality bias. This means that the causal effect might go either way as in the following system

\begin{itemize}
\tightlist
\item
  Causal effect on \(Y\) of \(X\): \(Y_i = \beta_0 + \beta_1 X_i + u_i\)
\item
  Causal effect on \(X\) of \(Y\): \(X_i = \gamma_0 + \gamma_1 Y_i + v_i\)
\end{itemize}

Where a large \(u_i\) means a large \(Y_i\), which implies large \(X_i\) (if \(\gamma_1>0\)) and therefore, by definition, \(corr(X_i,u_i) \neq 0\). Thus, \(\hat{\beta_1}\) is biased and inconsistent. In our Californian school district example it might as well be that a district with particularly bad test scores given the \(STR\) (negative \(u_i\)) receives extra resources, thereby lowering its \(STR\); so \(STR_i\) and \(u_i\) are then correlated

There are some potential solutions to simultaneous causality bias

The first and always the best one is to conduct a randomized controlled experiment. Because,if \(X_i\) is chosen at random by the experimenter, there is no feedback from the outcome variable to \(Y_i\) (assuming perfect compliance). Secondly, you can develop and estimate a complete model of both directions of causality. This is the idea behind many large macro models (e.g.~those of the Federal Reserve Bank in the US). This is difficult in practice. Finally, you can use instrumental variables regression again to estimate the causal effect of interest (effect of \(X\) on \(Y\), ignoring effect of \(Y\) on \(X\)). But that is not for this course.

\hypertarget{sec:conclusionspec}{%
\section{Concluding remarks}\label{sec:conclusionspec}}

\hypertarget{in-conclusion}{%
\chapter{In conclusion}\label{in-conclusion}}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-allcott2019regressive}{}}%
Allcott, Hunt, Benjamin B Lockwood, and Dmitry Taubinsky. 2019. {``Regressive Sin Taxes, with an Application to the Optimal Soda Tax.''} \emph{The Quarterly Journal of Economics} 134 (3): 1557--1626.

\leavevmode\vadjust pre{\hypertarget{ref-amrhein2019scientists}{}}%
Amrhein, Valentin, Sander Greenland, and Blake McShane. 2019. {``Scientists Rise up Against Statistical Significance.''} Nature Publishing Group.

\leavevmode\vadjust pre{\hypertarget{ref-andreyeva2010impact}{}}%
Andreyeva, Tatiana, Michael W Long, and Kelly D Brownell. 2010. {``The Impact of Food Prices on Consumption: A Systematic Review of Research on the Price Elasticity of Demand for Food.''} \emph{American Journal of Public Health} 100 (2): 216--22.

\leavevmode\vadjust pre{\hypertarget{ref-atkinson2009economics}{}}%
Atkinson, Anthony B. 2009. {``Economics as a Moral Science.''} \emph{Economica} 76: 791--804.

\leavevmode\vadjust pre{\hypertarget{ref-benabou2011identity}{}}%
Bnabou, Roland, and Jean Tirole. 2011. {``Identity, Morals, and Taboos: Beliefs as Assets.''} \emph{The Quarterly Journal of Economics} 126 (2): 805--55.

\leavevmode\vadjust pre{\hypertarget{ref-bernheim2018behavioral}{}}%
Bernheim, B Douglas, and Dmitry Taubinsky. 2018. {``Behavioral Public Economics.''} \emph{Handbook of Behavioral Economics: Applications and Foundations 1} 1: 381--516.

\leavevmode\vadjust pre{\hypertarget{ref-brons2002price}{}}%
Brons, Martijn, Eric Pels, Peter Nijkamp, and Piet Rietveld. 2002. {``Price Elasticities of Demand for Passenger Air Travel: A Meta-Analysis.''} \emph{Journal of Air Transport Management} 8 (3): 165--75.

\leavevmode\vadjust pre{\hypertarget{ref-busse2015psychological}{}}%
Busse, Meghan R, Devin G Pope, Jaren C Pope, and Jorge Silva-Risso. 2015. {``The Psychological Effect of Weather on Car Purchases.''} \emph{The Quarterly Journal of Economics} 130 (1): 371--414.

\leavevmode\vadjust pre{\hypertarget{ref-cadario2020healthy}{}}%
Cadario, Romain, and Pierre Chandon. 2020. {``Which Healthy Eating Nudges Work Best? A Meta-Analysis of Field Experiments.''} \emph{Marketing Science} 39 (3): 465--86.

\leavevmode\vadjust pre{\hypertarget{ref-chetty2015behavioral}{}}%
Chetty, Raj. 2015. {``Behavioral Economics and Public Policy: A Pragmatic Perspective.''} \emph{American Economic Review} 105 (5): 1--33.

\leavevmode\vadjust pre{\hypertarget{ref-chorus2015models}{}}%
Chorus, Caspar G. 2015. {``Models of Moral Decision Making: Literature Review and Research Agenda for Discrete Choice Analysis.''} \emph{Journal of Choice Modelling} 16: 69--85.

\leavevmode\vadjust pre{\hypertarget{ref-chorus2018taboo}{}}%
Chorus, Caspar G, Baiba Pudne, Niek Mouter, and Danny Campbell. 2018. {``Taboo Trade-Off Aversion: A Discrete Choice Model and Empirical Analysis.''} \emph{Journal of Choice Modelling} 27: 37--49.

\leavevmode\vadjust pre{\hypertarget{ref-combes2021production}{}}%
Combes, Pierre-Philippe, Gilles Duranton, and Laurent Gobillon. 2021. {``The Production Function for Housing: Evidence from France.''} \emph{Journal of Political Economy} 129 (10): 2766--816.

\leavevmode\vadjust pre{\hypertarget{ref-epple2010new}{}}%
Epple, Dennis, Brett Gordon, and Holger Sieg. 2010. {``A New Approach to Estimating the Production Function for Housing.''} \emph{American Economic Review} 100 (3): 905--24.

\leavevmode\vadjust pre{\hypertarget{ref-friedman1953methodology}{}}%
Friedman, Milton. 1953. {``The Methodology of Positive Economics.''}

\leavevmode\vadjust pre{\hypertarget{ref-galton1886regression}{}}%
Galton, Francis. 1886. {``Regression Towards Mediocrity in Hereditary Stature.''} \emph{The Journal of the Anthropological Institute of Great Britain and Ireland} 15: 246--63.

\leavevmode\vadjust pre{\hypertarget{ref-godfray2018meat}{}}%
Godfray, H Charles J, Paul Aveyard, Tara Garnett, Jim W Hall, Timothy J Key, Jamie Lorimer, Ray T Pierrehumbert, Peter Scarborough, Marco Springmann, and Susan A Jebb. 2018. {``Meat Consumption, Health, and the Environment.''} \emph{Science} 361 (6399): eaam5324.

\leavevmode\vadjust pre{\hypertarget{ref-green2005metropolitan}{}}%
Green, Richard K, Stephen Malpezzi, and Stephen K Mayo. 2005. {``Metropolitan-Specific Estimates of the Price Elasticity of Supply of Housing, and Their Sources.''} \emph{American Economic Review} 95 (2): 334--39.

\leavevmode\vadjust pre{\hypertarget{ref-high1985economics}{}}%
High, Jack. 1985. {``Is Economics Independent of Ethics.''} \emph{Reason Papers} 10 (1): 3--16.

\leavevmode\vadjust pre{\hypertarget{ref-hirschman1970exit}{}}%
Hirschman, Albert O. 1970. \emph{Exit, Voice, and Loyalty: Responses to Decline in Firms, Organizations, and States}. Vol. 25. Harvard university press.

\leavevmode\vadjust pre{\hypertarget{ref-infante2016preference}{}}%
Infante, Gerardo, Guilhem Lecouteux, and Robert Sugden. 2016. {``Preference Purification and the Inner Rational Agent: A Critique of the Conventional Wisdom of Behavioural Welfare Economics.''} \emph{Journal of Economic Methodology} 23 (1): 1--25.

\leavevmode\vadjust pre{\hypertarget{ref-jachimowicz2019and}{}}%
Jachimowicz, Jon M, Shannon Duncan, Elke U Weber, and Eric J Johnson. 2019. {``When and Why Defaults Influence Decisions: A Meta-Analysis of Default Effects.''} \emph{Behavioural Public Policy} 3 (2): 159--86.

\leavevmode\vadjust pre{\hypertarget{ref-kahneman2011thinking}{}}%
Kahneman, Daniel. 2011. \emph{Thinking, Fast and Slow}. Macmillan.

\leavevmode\vadjust pre{\hypertarget{ref-klamer2017doing}{}}%
Klamer, Arjo. 2017. \emph{Doing the Right Thing: A Value Based Economy}. Ubiquity Press.

\leavevmode\vadjust pre{\hypertarget{ref-laibson2015principles}{}}%
Laibson, David, and John A List. 2015. {``Principles of (Behavioral) Economics.''} \emph{American Economic Review} 105 (5): 385--90.

\leavevmode\vadjust pre{\hypertarget{ref-layard2008marginal}{}}%
Layard, Richard, Guy Mayraz, and Stephen Nickell. 2008. {``The Marginal Utility of Income.''} \emph{Journal of Public Economics} 92 (8-9): 1846--57.

\leavevmode\vadjust pre{\hypertarget{ref-lazear2000economic}{}}%
Lazear, Edward P. 2000. {``Economic Imperialism.''} \emph{The Quarterly Journal of Economics} 115 (1): 99--146.

\leavevmode\vadjust pre{\hypertarget{ref-leonard2008richard}{}}%
Leonard, Thomas C. 2008. {``Richard h. Thaler, Cass r. Sunstein, Nudge: Improving Decisions about Health, Wealth, and Happiness.''} Springer.

\leavevmode\vadjust pre{\hypertarget{ref-levitt2001dangerous}{}}%
Levitt, Steven D, and Jack Porter. 2001. {``How Dangerous Are Drinking Drivers?''} \emph{Journal of Political Economy} 109 (6): 1198--1237.

\leavevmode\vadjust pre{\hypertarget{ref-mcelreath2020statistical}{}}%
McElreath, Richard. 2020. \emph{Statistical Rethinking: A Bayesian Course with Examples in r and Stan}. Chapman; Hall/CRC.

\leavevmode\vadjust pre{\hypertarget{ref-medema2007hesitant}{}}%
Medema, Steven G. 2007. {``The Hesitant Hand: Mill, Sidgwick, and the Evolution of the Theory of Market Failure.''} \emph{History of Political Economy} 39 (3): 331--58.

\leavevmode\vadjust pre{\hypertarget{ref-mouter2021contrasting}{}}%
Mouter, Niek, Paul Koster, and Thijs Dekker. 2021. {``Contrasting the Recommendations of Participatory Value Evaluation and Cost-Benefit Analysis in the Context of Urban Mobility Investments.''} \emph{Transportation Research Part A: Policy and Practice} 144: 54--73.

\leavevmode\vadjust pre{\hypertarget{ref-pearl2009causality}{}}%
Pearl, Judea. 2009. \emph{Causality}. Cambridge university press.

\leavevmode\vadjust pre{\hypertarget{ref-pillai2015drivers}{}}%
Pillai, Unni. 2015. {``Drivers of Cost Reduction in Solar Photovoltaics.''} \emph{Energy Economics} 50: 286--93.

\leavevmode\vadjust pre{\hypertarget{ref-popper2005logic}{}}%
Popper, Karl. 2005. \emph{The Logic of Scientific Discovery}. Routledge.

\leavevmode\vadjust pre{\hypertarget{ref-posner2017moral}{}}%
Posner, Eric A, and Cass R Sunstein. 2017. {``Moral Commitments in Cost-Benefit Analysis.''} \emph{Va. L. Rev.} 103: 1809.

\leavevmode\vadjust pre{\hypertarget{ref-reurink2018financial}{}}%
Reurink, Arjan. 2018. {``Financial Fraud: A Literature Review.''} \emph{Journal of Economic Surveys} 32 (5): 1292--1325.

\leavevmode\vadjust pre{\hypertarget{ref-roth2018marketplaces}{}}%
Roth, Alvin E. 2018. {``Marketplaces, Markets, and Market Design.''} \emph{American Economic Review} 108 (7): 1609--58.

\leavevmode\vadjust pre{\hypertarget{ref-sandel2013market}{}}%
Sandel, Michael J. 2013. {``Market Reasoning as Moral Reasoning: Why Economists Should Re-Engage with Political Philosophy.''} \emph{Journal of Economic Perspectives} 27 (4): 121--40.

\leavevmode\vadjust pre{\hypertarget{ref-sen2000discipline}{}}%
Sen, Amartya. 2000. {``The Discipline of Cost-Benefit Analysis.''} \emph{The Journal of Legal Studies} 29 (S2): 931--52.

\leavevmode\vadjust pre{\hypertarget{ref-senn2011francis}{}}%
Senn, Stephen. 2011. {``Francis Galton and Regression to the Mean.''} \emph{Significance} 8 (3): 124--26.

\leavevmode\vadjust pre{\hypertarget{ref-simon1957models}{}}%
Simon, Herbert A. 1957. {``Models of Man; Social and Rational.''}

\leavevmode\vadjust pre{\hypertarget{ref-singh2020moral}{}}%
Singh, Prabhpal et al. 2020. {``Moral Realism and Expert Disagreement.''} \emph{Trames} 24 (3): 441--57.

\leavevmode\vadjust pre{\hypertarget{ref-spash1997ethics}{}}%
Spash, Clive L. 1997. {``Ethics and Environmental Attitudes with Implications for Economic Valuation.''} \emph{Journal of Environmental Management} 50 (4): 403--16.

\leavevmode\vadjust pre{\hypertarget{ref-stanovich2000individual}{}}%
Stanovich, Keith E, and Richard F West. 2000. {``Individual Differences in Reasoning: Implications for the Rationality Debate?''} \emph{Behavioral and Brain Sciences} 23 (5): 645--65.

\leavevmode\vadjust pre{\hypertarget{ref-stock2003introduction}{}}%
Stock, James H, Mark W Watson, et al. 2003. \emph{Introduction to Econometrics}. Vol. 104. Addison Wesley Boston.

\leavevmode\vadjust pre{\hypertarget{ref-tversky1992advances}{}}%
Tversky, Amos, and Daniel Kahneman. 1992. {``Advances in Prospect Theory: Cumulative Representation of Uncertainty.''} \emph{Journal of Risk and Uncertainty} 5 (4): 297--323.

\leavevmode\vadjust pre{\hypertarget{ref-van2010true}{}}%
Van Drunen, Michiel, Pieter Van Beukering, and Harry Aiking. 2010. {``The True Price of Meat.''} In \emph{Meat the Truth, Essays on Livestock Production, Sustainability and Climate Change}, 87--104. Nicolaas G. Pierson Foundation.

\leavevmode\vadjust pre{\hypertarget{ref-van2007beyond}{}}%
Van Staveren, Irene. 2007. {``Beyond Utilitarianism and Deontology: Ethics in Economics.''} \emph{Review of Political Economy} 19 (1): 21--35.

\leavevmode\vadjust pre{\hypertarget{ref-varian2003intermediate}{}}%
Varian, Hal R. 2003. \emph{Intermediate Microeconomics: A Modern Approach}. Elsevier Brasil.

\leavevmode\vadjust pre{\hypertarget{ref-verhoef1996second}{}}%
Verhoef, Erik, Peter Nijkamp, and Piet Rietveld. 1996. {``Second-Best Congestion Pricing: The Case of an Untolled Alternative.''} \emph{Journal of Urban Economics} 40 (3): 279--302.

\leavevmode\vadjust pre{\hypertarget{ref-visschers2016sorting}{}}%
Visschers, Vivianne HM, Nadine Wickli, and Michael Siegrist. 2016. {``Sorting Out Food Waste Behaviour: A Survey on the Motivators and Barriers of Self-Reported Amounts of Food Waste in Households.''} \emph{Journal of Environmental Psychology} 45: 66--78.

\leavevmode\vadjust pre{\hypertarget{ref-wempe2018reframing}{}}%
Wempe, Ben, and Jeff Frooman. 2018. {``Reframing the Moral Limits of Markets Debate: Social Domains, Values, Allocation Methods.''} \emph{Journal of Business Ethics} 153 (1): 1--15.

\leavevmode\vadjust pre{\hypertarget{ref-white2009defense}{}}%
White, Mark D. 2009. {``In Defense of Deontology and Kant: A Reply to van Staveren.''} \emph{Review of Political Economy} 21 (2): 299--307.

\end{CSLReferences}

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{appderivation}{%
\chapter{Derivation of the optimal cost function}\label{appderivation}}

The Lagrangian function is written as:

\begin{equation}
H = r K + \omega L + \lambda(Q - T K^{\alpha_K} L^{\alpha_L})
\end{equation}

The three first-order conditions are given by:

\begin{align}
  \frac{\partial H}{\partial \lambda} =& Q - T K^{\alpha_K} L^{\alpha_L} = 0\\
  \frac{\partial H}{\partial K} =& r - \lambda \frac{\alpha_K}{K}T K^{\alpha_K} L^{\alpha_L} = 0\\
  \frac{\partial H}{\partial L} =& \omega - \lambda \frac{\alpha_L}{L}T K^{\alpha_K} L^{\alpha_L} =0
\end{align}

Solving the second first-order condition for the Lagrangian multiplier gives:

\begin{equation}
\lambda = \frac{r}{\frac{\alpha_K}{K} T K^{\alpha_K} L^{\alpha_L}} = r \alpha_K^{-1} T^{-1} K^{1-\alpha_K} L^{-\alpha_L}
\end{equation}

Substituting this intermediate solution in the third first-order condition gives:

\begin{equation}
\omega - \frac{r}{\frac{\alpha_K}{K}T K^{\alpha_K} L^{\alpha_L}}\frac{\alpha_L}{L}T K^{\alpha_K} L^{\alpha_L} \longleftrightarrow \omega = r \frac{\frac{\alpha_L}{L}}{\frac{\alpha_K}{K}}
\end{equation}

Solving this equation for labour \(L\) gives:

\begin{equation}
L = \frac{r}{\omega} \frac{\alpha_L}{\frac{\alpha_K}{K}} = K \frac{r}{\omega} \frac{\alpha_L}{\alpha_K} 
\end{equation}

Substituting this intermediate solution in the first first-order condition gives:

\begin{equation}
Q - T K^{\alpha_K} \left(K\frac{r}{\omega}\frac{\alpha_L}{\alpha_K}\right)^{\alpha_L} = Q - T K^{\alpha_K + \alpha_L} \left(\frac{r}{\omega}\frac{\alpha_L}{\alpha_K}\right)^{\alpha_L} =0
\end{equation}

Solving this equation for capital gives:

\begin{eqnarray}
K^{\alpha_K + \alpha_L} \left(\frac{r}{\omega}\frac{\alpha_L}{\alpha_K}\right)^{\alpha_L} &=& \frac{Q}{T} \\
K^{\alpha_K + \alpha_L}  &=& \frac{Q}{T} \left(\frac{r}{\omega}\frac{\alpha_L}{\alpha_K}\right)^{-\alpha_L} 
\end{eqnarray}

Which leads to the following expression for optimal \(K^*\):

\begin{eqnarray}
K^* &=& \left(\frac{Q}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}} \left(\frac{r}{\omega}\frac{\alpha_L}{\alpha_K}\right)^{\frac{-\alpha_L}{\alpha_K + \alpha_L}} \\
&=&\left(\frac{Q}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}} \left(\frac{r}{\omega}\right)^{\frac{-\alpha_L}{\alpha_K + \alpha_L}} \left(\frac{\alpha_L}{\alpha_K}\right)^{\frac{-\alpha_L}{\alpha_K + \alpha_L}} \\
&=&\left(\frac{Q}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}} \left(\frac{\omega}{r}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}} \left(\frac{\alpha_L}{\alpha_K}\right)^{\frac{-\alpha_L}{\alpha_K + \alpha_L}} \\
&=&\left(\frac{Q}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}} \left(\frac{\omega}{r}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}} \left(\frac{\alpha_K}{\alpha_L}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}} 
\end{eqnarray}

Substituting this in the intermediate solution for labour gives:

\begin{equation}
L^* = K^* \frac{r}{\omega} \frac{\alpha_L}{\alpha_K} = \left(\frac{Q}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}} \left(\frac{r}{\omega}\frac{\alpha_L}{\alpha_K}\right)^{\frac{-\alpha_L}{\alpha_K + \alpha_L}}\frac{r}{\omega} \frac{\alpha_L}{\alpha_K} = \left(\frac{Q}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}} \left(\frac{r}{\omega}\frac{\alpha_L}{\alpha_K}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}}
\end{equation}

The last step is to simplify the exponent:

\begin{eqnarray}
L^* &=& \left(\frac{Q}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}} \left(\frac{r}{\omega}\frac{\alpha_L}{\alpha_K}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}} \\
&=& \left(\frac{Q}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}} \left(\frac{r}{\omega}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}}\left( \frac{\alpha_L}{\alpha_K}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}}
\end{eqnarray}

Using the inputs, we can derive the optimal cost function which expresses the costs of the firm as a function of target quantities and production parameters:

\begin{eqnarray}
C(K^*, L^*) &=& r K^* + \omega L^* \\
&=&r \left(\frac{Q}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}} \left(\frac{\omega}{r}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}} \left(\frac{\alpha_K}{\alpha_L}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}} + \omega  \left(\frac{Q}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}} \left(\frac{r}{\omega}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}}\left( \frac{\alpha_L}{\alpha_K}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}}
\end{eqnarray}

Now note that:

\begin{eqnarray}
r\left(\frac{\omega}{r}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}} &=& r \omega^{\frac{\alpha_L}{\alpha_K + \alpha_L}} r^{- \frac{\alpha_L}{\alpha_K + \alpha_L}} = \omega^{\frac{\alpha_L}{\alpha_K + \alpha_L}} r^{\frac{\alpha_K + \alpha_L}{\alpha_K + \alpha_L}-\frac{\alpha_L}{\alpha_K + \alpha_L}} = \omega^{\frac{\alpha_L}{\alpha_K + \alpha_L}} r^{ \frac{\alpha_K}{\alpha_K + \alpha_L}} \\
\omega\left(\frac{r}{\omega}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}} &=& \omega
\omega^{-\frac{\alpha_K}{\alpha_K + \alpha_L}} r^{\frac{\alpha_K}{\alpha_K + \alpha_L}} = 
\omega^{\frac{\alpha_K + \alpha_L}{\alpha_K + \alpha_L}-\frac{\alpha_K}{\alpha_K + \alpha_L}} r^{\frac{\alpha_K}{\alpha_K + \alpha_L}} = \omega^{\frac{\alpha_L}{\alpha_K + \alpha_L}} r^{\frac{\alpha_K}{\alpha_K + \alpha_L}}
\end{eqnarray}

\begin{eqnarray}
C(K^*, L^*) &=& r K^* + \omega L^* \\
&=& \left(\frac{Q}{T}\right)^{\frac{1}{\alpha_K + \alpha_L}}
\omega^{\frac{\alpha_L}{\alpha_K + \alpha_L}} r^{\frac{\alpha_K}{\alpha_K + \alpha_L}}
\left(\left(\frac{\alpha_K}{\alpha_L}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}}  +  \left( \frac{\alpha_L}{\alpha_K}\right)^{\frac{\alpha_L}{\alpha_K + \alpha_L}}\right) 
\end{eqnarray}

\hypertarget{apperror}{%
\chapter{Taxation in the presence of behavioural error}\label{apperror}}

The policy goal is to optimize economic surplus subject to the constraint that marginal decision benefits are equal to the consumer price and the supply price is equal to the marginal decision costs. Define the Lagrangian function as:

\begin{equation}
H = \int_0^Q p_e (Q')dQ' - \int^Q_0 s_e(Q')dQ' + \lambda (p_d(Q) - p^* - \tau)
\end{equation}

The derivatives of the Lagrangian function are given by:

\begin{align}
  \frac{\partial H}{\partial Q} =& p_e(Q) - s_e(Q) + \lambda \frac{\partial p_d(Q)}{\partial Q} = 0 \\
  \frac{\partial H}{\partial \lambda} =& p_d (Q) - p^* - \tau \\
  \frac{\partial H}{\partial \tau} =& -\lambda  =0
\end{align}

Now note that in the regulated equilibrium: \(p_d (Q^{*R} )=b_c p_e (Q^{*R} )\) and \(p^*=s_d (Q^{*R}) =b_p s_e (Q^{*R})\). Combining the conditions and substituting gives:

\begin{equation}
\frac{1}{b_c} (p^* + \tau) - \frac{1}{b_p}p^* = 0
\end{equation}

Solving for the tax results in:

\begin{equation}
\frac{1}{b_c} \tau = \frac{1}{b_p} p^* - \frac{1}{b_c} p^*.
\end{equation}

This results in:

\begin{equation}
\tau = p^* \left(\frac{b_c}{b_p} - 1 \right) = p^* \left(\frac{b_c - b_p}{b_p}\right).
\end{equation}

Now add the external costs \(\frac{\partial e(Q)}{\partial Q}\) to the economic surplus function. Define the Lagrangian function as:

\begin{equation}
H = \int_0^Q p_e (Q')dQ' - \int^Q_0 s_e(Q')dQ' - e(Q) + \lambda (p_d(Q) - p^* - \tau)
\end{equation}

The derivatives of the Lagrangian function are now given by:

\begin{align}
  \frac{\partial H}{\partial Q} =& p_e(Q) - s_e(Q) -\frac{\partial e(Q)}{\partial Q} + \lambda \frac{\partial p_d(Q)}{\partial Q} = 0 \\
  \frac{\partial H}{\partial \lambda} =& p_d (Q) - p^* - \tau \\
  \frac{\partial H}{\partial \tau} =& -\lambda  =0
\end{align}

Now note that \(p_d (Q)=b_c p_e (Q)\) and \(p^*=s_d (Q)=b_p s_e (Q)\). Substituting gives:

\begin{equation}
\frac{1}{b_c} (p^* + \tau) - \frac{1}{b_p}p^* -\frac{\partial e(Q)}{\partial Q} = 0
\end{equation}

Solving for the tax gives
\begin{equation}
\tau = p^{R*} \left(\frac{b_c - b_p}{b_p}\right) + b_c \frac{\partial e(Q)}{\partial Q}.
\end{equation}

\hypertarget{background-calculations-for-chapter-refmoral}{%
\chapter{Background calculations for Chapter \ref{moral}}\label{background-calculations-for-chapter-refmoral}}

The derivations in Appendices \ref{apptax} and \ref{apppricing} follow the methodological approach of Verhoef, Nijkamp, and Rietveld (1996) for deriving implicit tax expressions. For the regulated and unregulated case, it is assumed that an equilibrium exists.

\hypertarget{apptax}{%
\section{Deriving the tax that optimizes adjusted social surplus}\label{apptax}}

For the case without externalities, the adjusted social surplus \(AES\) is given by Eq. \eqref{eq:adjsocsur}. The Lagrangian function is given by:

\begin{equation}
\mathcal{L} = AES  + \lambda(p(Q) - p - \tau)
\end{equation}
and the first-order conditions are given by:
\begin{align}
  \frac{\partial \mathcal{L}}{\partial Q} =& \bar{\alpha} p(Q) - \bar{\delta} s(Q) + \lambda \frac{\partial p(Q)}{\partial Q} = 0 \label{eq:apptax1}\\
  \frac{\partial \mathcal{L}}{\partial \lambda} =& p(Q) - p - \tau = 0 \label{eq:apptax2}\\
  \frac{\partial \mathcal{L}}{\partial \tau} =& - \lambda = 0 \label{eq:apptax3}
\end{align}

Substituting Eqs. \eqref{eq:apptax3} and \eqref{eq:apptax2} in Eq. \eqref{eq:apptax2} and using \(s(Q)=p^{*R}\) gives an implicit equilibrium expression for the tax:

\begin{equation}
 \tau = p^{*R} \frac{\bar{\delta} - \bar{\alpha}}{\bar{\alpha}}.
\end{equation}

\hypertarget{apppricing}{%
\section{Pricing of a consumption externality}\label{apppricing}}

The adjusted social surplus AES with externalities is given by Eq. \eqref{eq:adjsocsur2}. The Lagrangian is given by:

\begin{equation}
\mathcal{L} = AES  + \lambda(p(Q) - p - \tau)
\end{equation}
and the first-order conditions are given by:
\begin{align}
  \frac{\partial \mathcal{L}}{\partial Q} =& \bar{\alpha} p(Q) - \bar{\delta} s(Q) - \bar{\eta}\frac{\partial e(Q)}{\partial Q} = \lambda \frac{\partial p(Q)}{\partial Q} = 0 \label{eq:appex1}\\
  \frac{\partial \mathcal{L}}{\partial \lambda} =& p(Q) - p - \tau = 0 \label{eq:appex2}\\
  \frac{\partial \mathcal{L}}{\partial \tau} =& - \lambda = 0 \label{eq:appex3}
\end{align}

Substituting Eqs. \eqref{eq:appex3} and \eqref{eq:appex2} in Eq. \eqref{eq:appex2} and rearranging gives the equilibrium expression for the first-best tax:

\begin{equation}
 \tau = p^{*R} \frac{\bar{\delta} - \bar{\alpha}}{\bar{\alpha}} + \frac{\bar{\eta}}{\bar{\alpha}}\frac{\partial e(Q)}{\partial Q}.
\end{equation}

\hypertarget{appreviewstat}{%
\chapter{Reviewing probability and statistics}\label{appreviewstat}}

This appendix very briefly reviews the statistical knowledge you need for applied econometrics. We assume you had an introductory course in statistics already and will go over this material very quickly.

\hypertarget{reviewing-probability}{%
\section{Reviewing probability}\label{reviewing-probability}}

\hypertarget{probability}{%
\subsection{Probability}\label{probability}}

As a definition of probability we use the concept of \emph{empirical probability} which is the proportion of time that something (a specific outcome or event \(X\)) occurs in the long-run of total events. Usually it is give by:

\begin{equation}
 \text{probability} = p = \frac{\text{Number of times specific event $X$ happens}}{\text{Total amount of events that can happen}}
\end{equation}

Now probabilities are \emph{defined} by a set of definations (axioms). These are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Probabilities, \(p\), are always between 0 and 1. So, \(0 \leq p \leq 1\)
\item
  If something does not happen, then \(p = 0\)
\item
  If something always happens, then \(p = 1\)
\item
  Probabilities for the total amount of events always add up to \(1\). So, if the probability that something happens is \(p\), then the probabilities that it will not happen is \(1 -p\) (see that \(p + 1 - p = 1\))
\end{enumerate}

\hypertarget{population-random-variables}{%
\subsection{Population \& random variables}\label{population-random-variables}}

In general we see a population as the the group or collection of all possible entities of interest (school districts, inhabitants of the Netherlands, homeowners) and we will think of populations as infinitely large (\(\infty\)). From this population we then \emph{sample} specific observations. This sample contains then a random variable \(Y\), which denotes a characteristics of the entity (district average test score, prices of houses, prices of meat). An important feature is that the sample characteristics are unknown, that is before measurement (\(y\)), after measurement the sample is know and is called data.

So, a random variable (also called a stochastic variable) is a mathematical formalization of something that depends on \emph{random} outcomes. Unfortunately, randomness is not clearly defined and depends on specific scientific philosophical schools. The scientific philosophical school we implicitly assume in this course---and, in fact, in most statistical courses---is that of frequentist statistics. Here we assume that all things we measure are \emph{intrinsically} random. In fact, this is an \emph{ontological} argument---in other words, what are our beliefs in the state of the world. Because all things we measure are random, every time we measure something our measurements are (slightly) different. However, the more we measure, the more precise we \emph{know} something. But there is still randomness.

In general, there are two types of random variables. First, there are \emph{discrete} random variables, where outcomes can be counted, such as \(0, 1, 2, 3, \ldots\) and \emph{continuous} random variables, where outcomes can be any real number.\footnote{There is slightly more to this as fractions such as \(\frac{1}{2}\) can in fact be counted as well, and continuous outcomes can be as well complex numbers. But for now we typically see integer numbers as discrete, and real numbers as continuous.}

\hypertarget{distribution-functions}{%
\subsection{Distribution functions}\label{distribution-functions}}

Random variables are governed by \emph{distribution functions} which are mathematical function that
provides the probabilities of occurrence of all different possible outcomes of the random variable \emph{experiment}: e.g.~for a discrete distribution, \(f(x) = \Pr(Y = y)\) \(\forall y\). Or, in other words, the distribution function maps discrete outcomes to probabilities. For continuous distribution function, this is not possible as there an infinite number of possible outcomes, so that means that for each \(y\) must yield \(\Pr(Y = y) = 0\). Therefore, with continuous distribution, often the \emph{cumulative distribution function} is used, which is defined as \(F(x) = \Pr(Y \leq y)\). This is why we always use the surface of areas under the \emph{normal} distribution function.

Distribution functions have characteristics of which the most important are:
- The mean, also known as the expected value (or expectation) of \(Y\). It is usually denoted as \(E(Y) = \mu_Y\) and can as well be interpreted as the long-run average value of \(Y\) over repeated realizations of \(Y\): \(\frac{1}{n}\sum_{i = 1}^{n}y_{i}\)

\begin{itemize}
\tightlist
\item
  The variance, which is denoted as \(E(Y - \mu_Y)^2\). Usually it is associated with \(\sigma^2_Y\) and provides a measure of the squared spread of the distribution. If we take the square root then we have the standard deviation (\(=\sqrt{\text{variance}} = \sigma_Y\)). For a symmetrical normal distribution, it is useful to know that the mean plus or minus 1 time the standard deviation governs about \(2/3\) of all probability while the mean plus or minus 2 times the standard deviation governs about 95\% of all probability associated with that random variable.
\end{itemize}

Now in statistics we are usually related in relations between random variables, and luckily most entities in real life are related. To capture the relation we need two concept, joint distributions and covariance. If we assume that that random variables \(X\) and \(Z\) have a joint distribution then the covariance between \(X\) and \(Z\) is:
\begin{equation}
cov(X,Z) = E[(X- \mu_X)(Z- \mu_Z)] = \sigma_{XZ}
\end{equation}

Note that this covariance is a measure of the \emph{linear} association between \(X\) and \(Z\) and that its units are units of \(X\) times units of \(Z\). \(cov(X,Z) > 0\) means a positive relation between \(X\) and \(Z\), and finally if \(X\) and \(Z\) are independently distributed, then \(cov(X,Z) = 0\). Note that the covariance of a random variable with itself is just its variance:
\begin{equation}
cov(X,X) = E[(X-\mu_X)(X - \mu_X)] = E[(X - \mu_X)^2] = \sigma^2_X
\end{equation}

However, the covariance is still measured in the units of \(X\) and \(Z\). To correct for that, we often use the correlation coefficient, defined by:
\begin{equation}
corr(X,Z) = \frac{cov(X,Z)}{\sqrt{var(X)var(Z)}} = \frac{\sigma_{XZ}}{\sigma{_X}\sigma{_Z}} = r_{XZ}
\end{equation}
where \(-1 \leq corr(X,Z) \leq 1\), a \(corr(X,Z) = 1\) means perfect positive linear association, a \(corr(X,Z) = -1\) means perfect negative linear association, and a \(corr(X,Z) = 0\) denotes no linear association.

\begin{figure}
\includegraphics[width=14.01in]{./figures/Sheet27} \caption{The correlation coefficient and the relation between observed $x$ and $y$}\label{fig:corrcoef}
\end{figure}

It is very important to notice that a correlation coefficient measures \textbf{linear} association. So, \(corr(X,Z) = 0\) does not mean that there is no relation, there is only no linear correlation. This is illustrated by figure \ref{fig:corrcoef}. In panel (a) there is clearly a positive relation, and panel (b) shows a negatve relation, but what about panel (d)? Here, the correlation coefficient is 0, just as in panel (c), but obviously there is a clear \emph{non-linear} relation.

\hypertarget{conditional-distributions-and-conditional-means}{%
\subsection{Conditional distributions and conditional means}\label{conditional-distributions-and-conditional-means}}

An important notion is applied statistics (and in applied econometrics) is that of the condition distributions, that is the distribution of \(Y\), given value(s) of some other random variable, \(X\). For example, in our California school example, we might want to know something about the distribution of test scores, \textbf{given} that \(STR < 20\). Therefore, we use the concept of conditional mean, which is defined as the mean of a conditional distribution = \(E(Y|X = x)\). Note here the \(|\) symbol---it means given that a random variable \(X\) is measured with \(x\). As an example: \(E(Test scores|STR < 20)\) which denotes the mean of test scores among districts with small class sizes. We also denote this with the \emph{conditional} mean.

Now, if we want to know the difference in means, then we can denote that with
\begin{equation}
\Delta = E(Test scores|STR < 20)-  E(Test scores|STR \geq 20),
\end{equation}
which is a very important concept in applied economics as it resembles two groups of which one received \emph{treatment} and the other one not. Other examples of the use of conditional means: difference in wages among gender (glass ceiling) and mortality rate differences between those who are treated and those who are. Now if \(E(X|Z)\) is constant, then \(corr(X,Z) = 0\). We then say that \(X\) and \(Z\) are independent.

\hypertarget{secssampling}{%
\section{Sampling in frequentist statistics}\label{secssampling}}

So, we mentioned above that we sample from the population which is assumed to be infinitely large. Now, how does this sampling then carry over to statistics. For that we need a statistical framework based on \emph{random sampling}. First, choose an individual, \(i\), (or district, firm, etc.) at random from the population. Now, prior to sample selection, the value of what we want to know \(Y_i\) is random because the individual selected is \textbf{random}. Once the individual is selected and the value of \(Y\) is observed, then \(Y\) is just a number---not random anymore but data. And then we say it has the value \(y\). Hence the notation \(\Pr(Y = y\)).

If we sample multiple entities, the we construct a data set that looks like \((y_1, y_2,\dots, y_n)\), where \(y_i\) = value of \(y\) for the \(i^{\mathrm{th}}\) individual (district, entity) sampled. Again the lower case here denotes a realisation---the dataset. Now, we want to know what the distribution of the random variables \(Y_1\),\ldots, \(Y_n\) is under simple random sampling. Note that because entities (say individuals) \#1 and \#2 are selected at random, the value of Y\(_1\) has no information content for \(Y_2\). Thus: \(Y_1\) and \(Y_2\) are independently distributed. And if \(Y_1\) and \(Y_2\) come from the same distribution, that is, \(Y_1\), \(Y_2\) are identically distributed, then we say that, under simple random sampling, Y\(_1\) and Y\(_2\) are independently and identically distributed (i.i.d.). More generally, under simple random sampling, Y\(_i\), \(i = 1\),\ldots, \(n\), are \emph{i.i.d}---this term always come back in all sorts of statistics.

This simple framework already allows rigorous statistical inferences about, e.g., \emph{the mean} \(\bar{Y}\) of population distributions using a sample of data from that population. The next subsection does this because the mean is not only an important, but mainly because the results immediately can be transferred to the regression context.

\hypertarget{the-sampling-distribution-of-bary}{%
\subsection{\texorpdfstring{The sampling distribution of \(\bar{Y}\)}{The sampling distribution of \textbackslash bar\{Y\}}}\label{the-sampling-distribution-of-bary}}

Now because \(\bar{Y}\) is formed by a sample of \(\{Y_i\}'s\) it is as well a random variable, and its properties are determined by the \emph{sampling distribution} of \(\bar{Y}\). Again, we assume that the elements in the sample are drawn at random, that thus the values of \((Y_1,\ldots, Y_n)\) are random, and that thus functions of \((Y_1,\ldots, Y_n)\), such as \(\bar{Y}\), are random: had a different sample been drawn, they would have taken on a different value. Finally, the distribution of \(\bar{Y}\) over different possible samples of size \(n\) is called the sampling distribution of \(\bar{Y}\), which underpins all of \emph{frequentists} statistics.

\hypertarget{example-simple-binomial-random-variables}{%
\subsection{Example: simple binomial random variables}\label{example-simple-binomial-random-variables}}

So how this work. Let's take the easiest statistical example: coin flipping, where the coin is this case is notoriously biased. Suppose the random variable \(Y\) takes on 0 (head) or 1 (tails) with the following probability distribution, \(\Pr[Y = 0] = 0.22\), \(\Pr(Y =1) = 0.78\). Then the mean and variance are given by:
\begin{eqnarray} 
\mu_{Y} &=& p \times 1 + (1-  p) \times 0 = p = 0.78\\
\sigma^2_Y&=& E[Y - \mu_{Y}]^2 = p(1 - p) \\
&=& 0.78 \times 0.22 = 0.17
\end{eqnarray}
But this is only one throw (\(n= 1\)). We would like to have multiple observations to derive at our sampling distribution of \(\bar{Y}\), which we assume to depend on the number of throws, \(n\).

Consider therefore first the case of \(n = 2\). The sampling distribution of \(\bar{Y}\) is,
\begin{eqnarray} 
\Pr(\bar{Y}  = 0) &= 0.22^2 &= 0.05 \\
\Pr(\bar{Y}  = 1/2) &=  2 \times 0.22 \times 0.78 &= 0.34\\
\Pr(\bar{Y}  = 1) &= 0.78^2 &= 0.61. 
\end{eqnarray}

but this start to become boring as \(n\) increases. Therefore, we turn to \texttt{STATA}. Let's first check for \(n = 2\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set} \KeywordTok{obs}\NormalTok{ 10000}
\KeywordTok{generate}\NormalTok{ Y = rbinomial(2,0.78)/2 }
\KeywordTok{hist}\NormalTok{(Y), fraction}
\end{Highlighting}
\end{Shaded}

The first line of this code sets the number of experiments (called \texttt{obs}). So, I throw a coin twice, for 10000 times in a row. The second line generates the outcomes, which in this case are no heads (0), head once (1), or two heads (2). To arrive at probabilities I divide by 2 again. Finally, the third line gives a history of fraction (not counts). And this provides the following \texttt{STATA} output.

\begin{figure}
\includegraphics[width=18.33in]{./figures/coin1} \caption{Sampling distribution when you throw a coin two times}\label{fig:coin1}
\end{figure}

But what if I do this a 100 times, so \(n = 100\)?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{clear}
\KeywordTok{set} \KeywordTok{obs}\NormalTok{ 10000}
\KeywordTok{generate}\NormalTok{ Y = rbinomial(100,0.78)/100 }
\KeywordTok{hist}\NormalTok{(Y), fraction}
\end{Highlighting}
\end{Shaded}

Note that fhe first line now clears \texttt{STATA}'s memory as I actually create a new dataset. The histogram can be seen now in Figure \ref{fig:coin100}.

\begin{figure}
\includegraphics[width=18.33in]{./figures/coin100} \caption{Sampling distribution when you throw a coin 100 times}\label{fig:coin100}
\end{figure}

But isn't this strange. Apart from some omitted bars (which is a fluke of \texttt{STATA}), we can now observe a couple of things. First, the average of the distribution of Figure \ref{fig:coin100} is very close to 0.78, which is the actual probability that our biased coin provides tails. But, more importantly the distribution starts to look like a symmetric normal distribution. And we started with a binomial distribution!

This is the result of two amazing statistical theorems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{The law of large numbers}: the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed. That is, if there a no biases in the experiment itself. It also means that with more experiments the precision become better, or the variance decreases. In general this implies that:

  \begin{itemize}
  \tightlist
  \item
    \(\bar{Y}\) is an \emph{unbiased} estimator of \(\mu_Y\) (that is, \(E(\bar{Y}) = \mu_Y\))
  \item
    var(\(\bar{Y}\)) is \emph{inversely proportional} to \(n\)
  \item
    Thus the standard error associated with \(\bar{Y}\) is \(\sqrt{\frac{\sigma_Y^2}{n}}\) (that means that with larger samples there is less uncertainty but see the square-root law)
  \end{itemize}
\item
  \textbf{The Central Limit Theorem}: when independent random variables are summed up, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed. So \(\bar{Y}\) is approximately distributed \(N(\mu_Y,\frac{\sigma^2_Y}{n})\)

  \begin{itemize}
  \tightlist
  \item
    When working with standardized variables then \(\bar{Y} = \frac{\bar{Y}-\mu_Y}{\sigma_Y/\sqrt{n}}\) is approximately distributed as \(N(0,1)\) \newline
  \item
    The larger is \(n\), the better is the approximation. And this already holds for \(n \geq 50\). So with a reasonable amount of observations, the mean of \emph{i.i.d.} variables is normally distributed
  \end{itemize}
\end{enumerate}

\end{document}
