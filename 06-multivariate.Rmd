# Modeling in the Social Sciences {#modeling}

In Chapter \@ref(univariateregression) we discussed the origins of, working of and assumptions behind univariate regression. That is, a regression model with only one independent variable $X$ on the right hand side.^[With right hand side we mean on the right side of the equal sign $=$. It is often abbreviated with RHS.] However, and especially in the social science, you almost always see regressions many independent variables. Depending on the field, these variables can be called control variables, confounding factors or moderator variables. But why are these variables included? Is it only to improve model performance or are there other reasons? Section \@ref(sec:morevar) deals with this question where after Section \@ref(sec:multivariate) shows how you can include additional variables in a *multivariate regression model* and especially how you should interpret them. Section \@ref(sec:nonlinear) extends the multivariate regression model and shows how you can actually use this model to estimate a broad range of non-linear economic models. Section \@ref(sec:fixedeffects) discusses the use of multiple dummy variables (see again Subsection \@ref(sec:dummy)) in a way that economists refer to as *fixed effects*. The last section concludes and provides a futher discussion of the benefits and limitations of multivariate regression models. 

## Why more independent variables? {#sec:morevar}

So, why do we include more variables? One possible answer is because it makes a better predictive model. That is, a model that is able to explain the variation in the dependent variable $Y$ better.^[This is not entirely true. Increasing the R$^2$ explains **in-sample** variation better, not necessarily **out-of-sample**. The latter is really what matters for prediction and this is the focus of many machine learning techniques. Note that this argument is directly related with the regression towards the mean argument made in Subsection \@ref(sec:genesis).] So, the R$^2$ increases. But, as argued in Chapter \@ref(univariateregression) we are not so much interested in prediction, but more in establishing a **causal** relation between $X$ and $Y$. So, if you change $X$ (and only $X$) does $Y$ changes and then with how much?

Although economists often claim that they are the only (social-)science which is focused on causality and provides a statistical framework for that, there are other approaches to causality as well. One that is often used in other sciences is the approach of the mathematican Judea Pearl [@pearl2009causality]. This approach focuses on the use of Directed Acyclical Graphs (DAGs), which is graphical visualisation of causality chains (or, what impacts what). We borrow this approach for the most simple setting as explained in Figure \@ref(fig:unknown). Here, we go back to our Californian school district dataset again, where we still are interested in the effect of class size on school performance. So, we suppose that that there is an effect from student teacher ratio on test scores as displayed with an directed arrow in Figure \@ref(fig:unknown). We also know that the R$^2$ of that regression model was rather low (5%), so by default there must be other but yet unknown factors, let us name them for now $U$, that influence test scores as well (so a directed arrow going from $U$ to test scores). 

```{r fig.align="center", echo=FALSE, fig.cap = "Unrelated omitted variables", label='unknown', out.width = "600px"}
knitr::include_graphics("./figures/unknown.png")
```

Now we are fine with this is as long as $U$ does **not impact** the student teacher ratio. Then, there is still an isolated effect of student teacher ratio on class size and that is exactly what we want to measure. However, if there is directed arrow going from $U$ into $STR$ as depicted by Figure \@ref(fig:unobshet), then the effect of student teacher ratio is not isolated anymore. Essentially, the effect of student teacher ratio on class size is composed out of two parts:

1) The **causal** effect on student teacher ratio on class size captured by the chain $\text{STR} \longrightarrow \text{testscore}$. The one we are after.
2) The impact of the unknown variables on test scores. As we have not modeled them in our regression model, the effect is captured by the chain $U \longrightarrow \text{STR} \longrightarrow \text{testscore}$

Economists refer to this phenomenon as **omitted variable bias**, whilst in the statistical world, this is as often called confounding variables or the  **the confounding fork** [@mcelreath2020statistical] and it, unfortunately, occurs very often.

```{r fig.align="center", echo=FALSE, fig.cap = "Related omitted variables", label='unobshet', out.width = "600px"}
knitr::include_graphics("./figures/Unobshet.png")
```

So, when **U** is a *common* cause for both student teacher ratio and test scores there is omitted variable bias. If we go back to our population regression model as follows:
\begin{equation}
Y_i = \beta_0 + \beta_1 X_i + u_i,
\end{equation}
then we know that the error $u$ arises because of factors that influence $Y$ but are not included in the regression function; so, there are *always* omitted variables. But do not always lead to bias. For omitted variable bias to occur, the omitted factor, let's call it $Z$^[$Z$ can be both known or unknown, so that is why we change from $U$ to $Z$], must be:

1. A **determinant** of $Y$ (i.e. $Z$ is part of $u$)
2. A **determinant** of the regressor $X$ (*at least*, there should hold that $corr(Z,X) \neq 0$)^[In econometric textbooks, as, e.g, in @stock2003introduction, this condition is weakened to only being correlation ($Z$ and $X$ are correlated). However, if the directed arrow goes from $STR$ into $U$ in Figure \@ref(fig:unobshet) then that would lead to something else than omitted variables, namely to a difference between a direct ($\text{STR} \longrightarrow \text{testscore}$) and an indirect effect ($\text{STR} \longrightarrow U \longrightarrow \text{testscore}$).]

Thus, both conditions must hold for the omission of $Z$ to result in omitted variable bias.

Now, in our Californian district school dataset we have many more variables. One of them is variable that measures the english language ability (whether the student has English as a second language). Note that in California there are many migrants, especially from Latin-America. Now, you can readily argue that not having English as first language plausibly affects standardized test scores: so, $Z$ is a **determinant** of $Y$. Moreover, immigrant communities tend to be less affluent and thus have smaller school budgets---and, therefore, higher $STR$: $Z$ is most likely as well a **determinant** of $X$.

So, most likely, our original estimation from Chapter \@ref(univariateregression), $\hat{\beta}_1$, is biased (so not the true causal effect). But can we say something about the direction that bias? Yes, but the argument tend to become very quickly rather complex. In this case, note that districts with more migrant communities tend to have (*i*) higher class sizes and (*ii*) lower test scores. So, to the original estimation they added a *negative* effect. Thus, following this reasoning, the "true" effect must be less negative. Now, especially with negative signs this becomes rather complex, so if common sense fails you, there is the following formula:

\begin{equation}
\hat{\beta}_1 \overset{p}{\to} \beta_1 + \frac{\sigma_u}{\sigma_X}\rho_{Xu},
\end{equation}
where you should focus on the sign of the correlation between $X$ and the regression residual $u$ (all standard errors, $\sigma$, are always positive by default). Now, the first least squares assumption states that $\rho_{Xu} = 0$---no correlation between the regressor and the regression residual. But now there is because of omitted variable bias. And because the negative relation between immigrants communities and school performance $\rho_{Xu}$ should be negative. And because the original estimation from Chapter \@ref(univariateregression) was already negative to begin with the "true" $\beta_1$ should be less negative. In conclusion, districts with more English learning students (*i*) do worse on standardized tests and (*ii*) have bigger classes (smaller budgets), so ignoring the English learning factor results in overstating the class size effect.

You might wonder whether this is actually going on in the Californian district school data. To see this, Figure \@ref(fig:omitca) offers a cross tabulation of test scores by class size and percentage English learners. 

```{r fig.align="center", echo=FALSE, fig.cap = "Cross tabulation of test scores by class size and percentage English learners", label='omitca', out.width = "800px"}
knitr::include_graphics("./figures/Sheet7.png")
```

Now, the table depicted in Figure \@ref(fig:omitca) is complex in its various dimensions. We have our two categories of class size (small and large), together with the difference in test scores, but we now stratify this by four categories of percentage English learners. There are several important observations to make here:

1) districts with *fewer* English Learners (so less migrants) have on average *higher* test scores (what we assumed above);
2) districts with *fewer* English Learners (so less migrants) have *smaller* classes (what we assumed above);
3) the effect of class size with comparable percentages English learners is still (mostly negative), but not as much as we compares for all districts together (the Difference-column). This confirms our reasoning that our original estimate was too negative. 

No, as already mentioned above omitted variable bias occurs very often. So, how to correct for this such that the bias disappaers. In general, there are strategies:

1. we can run a randomized controlled experiment in which treatment ($STR$) is randomly assigned: then percentage English learners ($PctEL$) is still a determinant of test score, but by construction $PctEL$ should be uncorrelated with $STR$. Unfortunately, is very difficult to randomize class size and often this strategy is just not attainable as being too costly or unethical (this accounts for all sciences);
2. we can adopt the cross tabulation approach of above, with finer gradations of $STR$ and $PctEL$. Then by construction,  within each group all classes have the same $PctEL$ so we control for $PctEL$. A disadvantages is that one needs many observations, especially when one wants to stratify upon other variables as well; 
3. finally, and perhaps the easiest approach, we can use a population regression model in which the omitted variable ($PctEL$) is no longer omitted. We just include $PctEL$ as an additional regressor in a multiple regression model. This is what the next section deals with. Obviously, a disadvantage of this approach is that you need observations for the omitted variable. 

## Multivariate regression analysis {#sec:multivariate}


Consider the case of two regressors:
\begin{equation}
Y_i =\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i, i=1,\ldots,n
\end{equation}

- $Y$ is the dependent variable
- $X_1$, $X_2$ are the two independent variables (regressors)
- $(Y_i, X_{1i}, X_{2i})$ denote the i$^{\mathrm{th}}$ observation on $Y$, $X_1$, and $X_2$.
- $\beta_0$ = unknown population intercept
- $\beta_1$ = effect on $Y$ of a change in $X_1$, **holding** $X_2$ constant}
- $\beta_2$ = effect on $Y$ of a change in $X_2$, **holding** $X_1$ constant}
- $u_i$ = the regression error (omitted factors)


\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i}+u_i, i=1,\ldots,n
\end{equation}

Consider changing $X_1$ by $\Delta X_1$ while holding $X_2$:

Population regression line before the change:
\begin{equation}
Y = \beta_0 + \beta_1 X_{1} + \beta_2 X_{2}
\end{equation}

Population regression line, after the change:
\begin{equation}
Y + \Delta Y = \beta_0 + \beta_1 (X_{1} + \Delta X_1) + \beta_2 X_{2}
\end{equation}

- So before: $Y = \beta_0 + \beta_1 X_1  + \beta_2 X_2$
- After: $Y + \Delta Y = \beta_0 + \beta_1 (X_{1} + \Delta X_1) + \beta_2 X_{2}$
- Difference: $\Delta Y = \beta_1 \Delta X_1$
- $\beta_1 = \frac{\Delta Y}{\Delta X_1}$ Holding $X_2$ constant
- $\beta_2 = \frac{\Delta Y}{\Delta X_2}$ Holding $X_1$ constant
- $\beta_0$ predicted value of $Y$ when $X_1 = X_2 = 0$

Example: the California test score data

Regression of TestScore against STR:
\begin{equation}
\widehat{TestScore} = 698.9- 2.28 STR
\end{equation}
Now include percent English Learners in the district ($PctEL$):
\begin{equation}
\widehat{TestScore} = 686.0- 1.10 STR - 0.65  PctEL
\end{equation}

What happens to the coefficient on $STR$ Why? (Note: $corr(STR, PctEL) = 0.19$)

Multiple regression in STATA

```{stata, collectcode=TRUE}
reg testscr str el_pct, robust
```

### Measures of Fit for Multiple Regression

Actual = predicted + residual: $Y_i = \hat{Y}_i + \hat{u_i}$
- $SER$ =std. deviation of $\hat{u}_i$ (with d.f. correction)
- $RMSE$ =std. deviation of $\hat{u}_i$ (without d.f. correction)
- $R^2$ = fraction of variance of $Y$ explained by $X$
- $\bar{R}^2$ = ``adjusted $R^2$'' = $R^2$ with a degrees-of-freedom correction that adjusts for estimation uncertainty; $\bar{R}^2 <R^2$

Measures of fit

Test score example:

\begin{eqnarray}
TestScore &= &698.9- 2.28  STR \\
&&R^2 = .05, SER = 18.6
\end{eqnarray}

\begin{eqnarray}
TestScore &=& 686.0 - 1.10  STR - 0.65 PctEL \\
&&R^2=.426, \bar{R}^2=0.424, SER = 14.5
\end{eqnarray}

 What---precisely--does this tell you about the fit of regression (2) compared with regression (1)?
Why are the $R^2$ and the $\bar{R}^2$  so close in (2)?

### The Least Squares Assumptions for Multiple Regression

\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i}+\ldots + \beta_k X_{ki}+u_i, i=1,\ldots,n
\end{equation}

1.  The conditional distribution of $u$ given the $X$'s has mean zero, that is, $E(u|X_1 = x_1,\ldots, X_k = x_k) = 0$.
2.  (X$_{1i}$,\ldots,X$_{ki}$,Y$_i$), i =1,\ldots,n, are i.i.d.
3.  Large outliers are rare for $X_1,\ldots, X_k$, and $Y$
4.  There is no perfect multicollinearity.


Assumption 1: the conditional mean of $u$ given the included $X$'s is zero.

\begin{equation}
E(u|X_1 = x_1,\ldots, X_k = x_k) = 0
\end{equation}

- This has the same interpretation as in regression with a single regressor.
- If an omitted variable (1) belongs in the equation (so is in $u$) and (2) is correlated with an included $X$, then this condition fails
- Failure of this condition leads to omitted variable bias
- The solution---if possible---is to include the omitted variable in the regression.

Least squares assumption 4: no perfect multicollinearity

Multicollinearity, Perfect and Imperfect
Examples of perfect multicollinearity

1. First example: you include $STR$ twice.
2. Second example:
    - regress $TestScore$ on a constant, $D$, and $B$, where:$D_i =1$ if $STR \leq 20$, $=0$ otherwise ; $B_i =1$ if $STR>20$, $= 0$ otherwise, so $B_i = 1 - D_i$ and there is perfect multicollinearity
    - Would there be perfect multicollinearity if the intercept (constant) were somehow dropped (that is, omitted or suppressed) in this regression?
   - This example is a special case of dummy variable} trap. Suppose you have a set of multiple binary (dummy) variables, which are mutually exclusive and exhaustive---that is, there are multiple categories and every observation falls in one and only one category (Freshmen, Sophomores, Juniors, Seniors, Other). If you include all these dummy variables and a constant, you will have perfect multicollinearity---the dummy variable trap.
    - Why is there perfect multicollinearity here?

Solutions to the dummy variable trap:
1. Omit one of the groups (e.g. Senior), or
2. Omit the intercept

What are the implications of (1) or (2) for the interpretation of the coefficients?
Perfect multicollinearity usually reflects a mistake in the definitions of the regressors, or an oddity in the data


- If you have perfect multicollinearity, your statistical software will let you know---either by crashing or giving an error message or by ``dropping'' one of the variables arbitrarily
- The solution to perfect multicollinearity is to modify your list of regressors so that you no longer have perfect multicollinearity.

Imperfect multicollinearity
Imperfect and perfect multicollinearity are quite different despite the similarity of the names.
Imperfect multicollinearity occurs when two or more regressors are very highly correlated.
Why this term? If two regressors are very highly correlated, then their scatterplot will pretty much look like a straight line---they are collinear---but unless the correlation is exactly $\pm$ 1, that collinearity is imperfect.

Imperfect multicollinearity implies that one or more of the regression coefficients will be imprecisely estimated.

- the coefficient on $X_1$ is the effect of $X_1$ holding $X_2$ constant
but if $X_1$ and $X_2$ are highly correlated, there is very little variation in $X_1$ once $X_2$ is held constant
- so the data are pretty much uninformative about what happens when $X_1$ changes but $X_2$ doesn't, so the variance of the OLS estimator of the coefficient on $X_1$ will be large.

Imperfect multicollinearity (correctly) results in large standard errors for one or more of the OLS coefficients.

### Testing with multivariate regression models

Hypothesis Tests and Confidence Intervals for a Single Coefficient in Multiple Regression

$\frac{\hat{\beta}_1- E(\hat{\beta}_1)}{\sqrt{var(\hat{\beta}_1)}}$ is approximately distributed $N(0,1)$ (CLT)

Thus hypotheses on $\beta_1$ can be tested using the usual $t$-statistic, and confidence intervals are constructed as $\{\hat{\beta}_1 \pm 1.96 SE (\hat{\beta}_1)\}$

So too for $\beta_2,\ldots, \beta_k$.

$\hat{\beta}_1$ and $\hat{\beta}_2$ are generally not independently distributed---so neither are their $t$-statistics (more on this later).

Example: The California class size data


\begin{equation}
TestScore =\underbrace{698.9}_{10.4} - \underbrace{2.28}_{0.52}  STR
\end{equation}
\begin{equation}
TestScore = \underbrace{686.0}_{8.7} - \underbrace{1.10}_{0.43} STR - \underbrace{0.650}_{0.031} PctEL
\end{equation}

The coefficient on $STR$ in (2) is the effect on $TestScores$ of a unit change in $STR$, holding constant the percentage of English Learners in the district
The 95\% confidence interval for coefficient on $STR$ in (2) is $\{-1.10 \pm 1.96 \times 0.43\} = (-1.95,-0.26)$
The $t$-statistic testing $\beta_{STR} = 0$ is $t = -1.10/0.43 = -2.54$, so we reject the hypothesis at the 5\% significance level
\begin{verbatim}

```{stata, collectcode=TRUE}
reg testscr str el_pct, r
```

Tests of Joint Hypotheses


Let $Expn =$ expenditures per pupil and consider the population
regression model:
\begin{equation}
TestScore_i = \beta0 + \beta_1 STR_i + \beta_2 Expn_i + \beta_3PctEL_i + u_i
\end{equation}
The null hypothesis that "school resources don't matter" and the alternative that they do, corresponds to: 

- $H_0:\beta_1 =0$ and $\beta_2 =0$ vs
- $H_1:$ either $\beta_1 \neq 0$ or $\beta_2 \neq 0$ or both


- $H_0:\beta_1 =0$ and $\beta_2 =0$ vs
- $H_1:$ either $\beta_1 \neq 0$ or $\beta_2 \neq 0$ or both
A joint hypothesis specifies a value for two or more coefficients, that is, it imposes a restriction on two or more coefficients.
- In general, a joint hypothesis will involve $q$ restrictions. In the example above, $q = 2$, and the two restrictions are $\beta_1 = 0$ and $\beta_2 = 0$.
-  A "common sense" idea is to reject if either of the individual $t$-statistics exceeds 1.96 in absolute value.

But this "one at a time" test isn't valid: the resulting test rejects too often under the null hypothesis (more than 5%)!

Therefore, we need the $F$-statistic

The $F$-statistic tests all parts of a joint hypothesis at once.
Formula for the special case of the joint hypothesis $\beta_1 = \beta_{1,0}$ and $\beta_2 = \beta_{2,0}$ in a regression with two regressors:

\begin{equation}
F = \frac{1}{2} \left(\frac{t_1^2 + t_2^2 - 2\hat{\rho}_{t_1,t_2}t_1 t_2}{1-\hat{\rho}^2_{t_1 t_2}}  \right)
\end{equation}

where $\hat{\rho}_{t_1,t_2}$ estimates the correlation between $t_1$ and $t_2$. Reject when $F$ is large (how large?)

The F-statistic testing $\beta_1$ and $\beta_2$:

\begin{equation}
F = \frac{1}{2} \left(\frac{t_1^2 + t_2^2 - 2\hat{\rho}_{t_1,t_2}t_1 t_2}{1-\hat{\rho}^2_{t_1 t_2}}  \right)
\end{equation}

The F-statistic is large when $t_1$ and/or $t_2$ is large
The F-statistic corrects (in just the right way) for the correlation between $t_1$ and $t_2$.

The formula for more than two $\beta$'s is nasty unless you use matrix algebra.

This gives the $F$-statistic a nice large-sample approximate distribution, which is 

Computing the p-value using the $F$-statistic:

$p$-value = tail probability of the $\chi^2_q /q$ distribution beyond the $F$-statistic actually computed.

Use the `test` command **right** after the regression

Example: Test the joint hypothesis that the population coefficients on $STR$ and expenditures per pupil ($expn_{stu}$) are both zero, against the alternative that at least one of the population coefficients is nonzero.

```{stata,}
test str el_pct
```

## Non-linear specifications {#sec:nonlinear}

The $TestScore$--$STR$ relation looks linear 

```{stata, echo=1, results="hide"}
graph twoway (lfit testscr str) (scatter testscr str)
quietly graph export figures/scatterlfit.png, replace
```

And this provides the following `STATA` output.

```{r, echo=FALSE, fig.cap = "A linear relation", label='scatterlfitcaschool'}
knitr::include_graphics("./figures/scatterlfit.png")
```

But the $TestScore$--$Income$ relation looks 

```{stata, echo=1, results="hide"}
graph twoway (lfit testscr avginc) (scatter testscr avginc)
quietly graph export figures/scatterincome.png, replace
```

And this provides the following `STATA` output.

```{r, echo=FALSE, fig.cap = "A non-linear relation", label='scatterincome'}
knitr::include_graphics("./figures/scatterincome.png")
```

Nonlinear regression population regression functions

If a relation between $Y$ and $X$ is nonlinear:
- The effect on $Y$ of a change in $X$ depends on the value of $X$---that is, the *marginal* effect of $X$ is not constant
- A linear regression is misspecified
- The estimator of the effect on $Y$ of $X$ is biased---it needn't even be right on average.
- The solution to this is to estimate a regression function that is nonlinear in $X$

Nonlinear functions of a single independent variable

We look at two complementary approaches:
- Polynomials in $X$
    - The effect is approximated by a quadratic, cubic, or higher-degree polynomial
- Logarithmic transformations
    - $Y$ and/or $X$ is transformed by taking its logarithm
    - this gives a percentages interpretation which is often useful

### Polynomials

Approximate the regression function by a polynomial:
\begin{equation}
Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X^2_i + \ldots + \beta_r X_i^r + u_i
\end{equation}

This is just the linear regression model---except that the regressors are powers of $X$! Estimation, hypothesis testing, etc. proceeds as in the multiple regression model using OLS

The coefficients are difficult to interpret, but the regression function itself is interpretable

Example: the $TestScore$--$Income$ relation

$Income_i$ = average district income in the $i^{\mathrm{th}}$ district (thousands of dollars per capita)
Quadratic specification:
\begin{equation}
TestScore_i = \beta_0 + \beta_1 Income_i + \beta_2 (Income_i)^2 + u_i
\end{equation}
Cubic specification:
\begin{equation}
TestScore_i = \beta_0 + \beta_1 Income_i + \beta_2 (Income_i)^2 +
\beta_3 (Income_i)^3 + u_i
\end{equation}

Estimation of the quadratic


```{stata, collectcode=TRUE}
reg testscr c.avginc##c.avginc, r
```

Test the null hypothesis of linearity against the alternative that the regression function is a quadratic 

Digression: nonlinearities and plots in stata
	
There are four factor-variable operators:

- `i.` operator to specify indicators (dummies)
- `c.` operator to treat as continuous
- `#` binary operator to specify interactions
- `##` binary operator to specify factorial interactions


```{stata, echo=-3, results="hide"}
predict hat1 
scatter (testscr avginc) || (line hat1 avginc, sort)
quietly graph export figures/scatterqua.png, replace
```

And this provides the following `STATA` output.

```{r, echo=FALSE, fig.cap = "A non-linear relation", label='scatterqua'}
knitr::include_graphics("./figures/scatterqua.png")
```

Compute "effects" for different values of $X$
\begin{equation}
\widehat{TestScore_i} = 607.3 + 3.85 Income_i - 0.0423(Income_i)^2
\end{equation}
Predicted change in TestScore for a change in income from \$5,000 per capita to \$6,000 per capita:
\begin{eqnarray}
\Delta \widehat{TestScore} &=& 607.3 + 3.85 \times 6 -  0.0423 \times 6^2 \\
&& - (607.3 + 3.85\times 5 - 0.0423\times 5^2)\\
&=&3.4
\end{eqnarray}

Predicted "effects" for different values of $X$:}

```{r effectqua, echo = FALSE}
v1 <- c(
"from 5 to 6", "from 25 to 26", "from 45 to 46"
  )
v2 <- c(3.4, 1.7, 0.0)
df <- data.frame( v1, v2 )
kable(df, 
      align="l", 
      booktabs=TRUE, escape = F, 
      caption = 'Effect of $X$',
      col.names = c("Change in Income (1000 dollar per capita)","$\\Delta \\widehat{TestScore}$")
) %>%
  kable_styling(bootstrap_options = "striped")
```

The "effect" of a change in income is greater at low than high income levels (perhaps, a declining marginal benefit of an increase in school budgets?)

Caution! What is the effect of a change from 65 to 66?  Don't extrapolate outside the range of the data!

Estimation of a cubic

```{stata, collectcode=TRUE}
reg testscr c.avginc##c.avginc##c.avginc, r
```
	

Testing the null hypothesis of linearity

Alternative hypothesis: the population regression is quadratic and/or cubic, that is, it is a polynomial of degree up to 3:

- $H_0$: Coefficients on $Income^2$ and $Income^3 = 0$
- $H_1$: at least one of these coefficients is nonzero.

The hypothesis that the population regression is linear is rejected at the 1% significance level against the alternative that it is a polynomial of degree up to 3.

```{stata, collectcode=TRUE}
test avginc#avginc avginc#avginc#avginc  
```

### Interaction variables

Interactions between independent variables

Perhaps a class size reduction is more effective in some
		circumstances than in others 
	
 Perhaps smaller classes help more if there are many English
learners, who need individual attention 

 That is, $\frac{\partial TestScore}{\partial STR}$ might \alert{depend} on $PctEL$

More generally, $\frac{\partial Y}{\partial X_1}$ might \alert{depend} on $X_2$

#### Interactions between two binary variables

\begin{equation}
Y_i =\beta_0 +\beta_1 D_{1i} + \beta_2 D_{2i} +u_i
\end{equation}

$D_{1i}$ and $ D_{2i}$ are binary

$\beta_1$ is the effect of changing $D_1=0$ to $D_1=1$. In this specification, this effect doesn't depend on the value of $D_2$.

To allow the effect of changing $D_1$ to depend on $D_2$, include the interaction term $D_{1i} \times D_{2i}$ as a regressor:\\

\begin{equation}
Y_i =\beta_0 +\beta_1 D_{1i} + \beta_2 D_{2i} + \beta_3 (D_{1i} \times
D_{2i}) + u_i
\end{equation}

Interpreting the coefficients

\begin{equation}
Y_i =\beta_0 +\beta_1 D_{1i} + \beta_2 D_{2i} + \beta_3 (D_{1i} \times
D_{2i}) + u_i
\end{equation}
General rule: compare the various cases:

\begin{eqnarray}
E(Y_i|D_{1i}=0, D_{2i}=d_2) &=& \beta_0 + \beta_2 d_2 \\
E(Y_i|D_{1i}=1, D_{2i}=d_2) &=& \beta_0 + \beta_1 + \beta_2 d_2 + \beta_3 d_2 
\end{eqnarray}

subtract from each other:
\begin{equation}
E(Y_i|D_{1i}=1, D_{2i}=d2) - E(Y_i|D_{1i}=0, D_{2i}=d_2) = \beta_1 +
\beta_3 d_2
\end{equation}


The effect of $D_1$ depends on $d_2$

$\beta_3$ = increment to the effect of $D_1$, when $D_2 = 1$\\

Example: $TestScore$, $STR$, English learners


Let:
\begin{eqnarray}
HiSTR &=& 1 \text{ if } STR \geq 20 \text{ and } HiEL = 1 \text{ if }
PctEL \geq 10 \\
HiSTR &=& 0 \text{ if } STR < 20 \text{ and } HiEL = 0 \text{ if }
PctEL < 10 \\
\end{eqnarray}
\begin{equation}
\widehat{TestScore} = 664.1 - 18.2 HiEL - 1.9 HiSTR - 3.5(HiSTR \times
HiEL)
\end{equation}


"Effect" of $HiSTR$ when $HiEL = 0$ is $-1.9$

"Effect" of $HiSTR$ when $HiEL = 1$ is $-1.9 - 3.5 = -5.4$

Class size reduction is estimated to have a bigger effect when the percent of English learners is large

This interaction isn't statistically significant: $t = 3.5/3.1$


#### Interactions between continuous and binary variables

\begin{equation}
Y_i =\beta_0 +\beta_1 D_i + \beta_2 X_i +u_i
\end{equation}


$D_i$ is binary, $X$ is continuous

As specified above, the effect on $Y$ of $X$ (holding constant $D$) = $\beta_2$, which does not depend on $D$

To allow the effect of $X$ to depend on $D$, include the interaction term $D_i \times X_i$ as a regressor:

\begin{equation}
Y_i =\beta_0 +\beta_1 D_i + \beta_2 X_i + \beta_3 (D_i \times X_i) + u_i
\end{equation}

Binary-continuous interactions: the two regression lines
\begin{equation}
Y_i =\beta_0 +\beta_1 D_i + \beta_2 X_i + \beta_3 (D_i \times X_i) + u_i
\end{equation}
Observations with $D_i= 0$ (the $D = 0$ group or the $D=0$ regression line):
\begin{equation}
Y_i = \beta_0 + \beta_2 X_i  + u_i 
\end{equation}
Observations with $D_i= 1$ (the $D = 1$ group or the $D = 1$ regression line):
\begin{eqnarray}
Y_i &=&   \beta_0 + \beta_1 + \beta_2 X_i + \beta_3 X_i + u_i \\
			&=&  (\beta_0 + \beta_1) + (\beta_2 + \beta_3) X_i + u_i
\end{eqnarray}


Binary-continuous interactions, ctd.

```{r, echo=FALSE, fig.cap = "A non-linear relation", label='interaction'}
knitr::include_graphics("./figures/Sheet44.jpg")
```

Interpreting the coefficients

\begin{equation}
Y_i =\beta_0 +\beta_1 D_i + \beta_2 X_i + \beta_3 (D_i \times X_i) + u_i
\end{equation}

General rule: take the marginal effect of
\begin{equation}
Y =\beta_0 +\beta_1 D + \beta_2 X + \beta_3 (D \times X)
\end{equation}
\begin{equation}
\frac{\partial Y}{\partial X} = \beta_2 + \beta_3 D
\end{equation}
The effect of $X$ depends on $D$ 

$\beta_3=$ increment to the effect of $X$, when $D = 1$

Example: $TestScore$, $STR$, $HiEL$ 

\begin{equation}
\widehat{TestScore} = 682.2 - 0.97 STR + 5.6 HiEL - 1.28(STR \times HiEL) 
\end{equation}


When $HiEL = 0$:\\

\begin{equation}
\widehat{TestScore} = 682.2 - 0.97 STR 
\end{equation}

When $HiEL = 1$:

\begin{eqnarray}
\widehat{TestScore} &=& 682.2 - 0.97 STR + 5.6 - 1.28 STR \\
&=& 687.8 - 2.25 STR
\end{eqnarray}

Two regression lines: one for each $HiSTR$ group.

Class size reduction is estimated to have a larger effect when the percent of English learners is large.

Example, ctd: Testing hypotheses


\begin{equation}
\widehat{TestScore} = 682.2 - 0.97 STR + 5.6 HiEL - 1.28(STR \times HiEL)
\end{equation}


 The two regression lines have the same slope---the coefficient on $STR \times HiEL$ is zero: $t = -1.28/0.97 = -1.32$


 The two regression lines have the same intercept---the coefficient on $HiEL$ is zero: $t = -5.6/19.5 = 0.29$


 The two regression lines are the same---population coefficient on $HiEL = 0$ and population coefficient on $STR \times HiEL = 0$: $F = 89.94 (p-value < .001)$ 
 We reject the joint hypothesis but neither individual hypothesis (how can this be?)
 
#### Interactions between two continuous variables}

\begin{itemize}
Y_i =\beta_0 + \beta1 X_{1i} +\beta_2 {X_{2i}} +u_i
\end{itemize}

$X_1$, $X_2$ are continuous

As specified, the effect of $X_1$ doesn't depend on $X_2$

As specified, the effect of $X_2$ doesn't depend on $X_1$


To allow the effect of $X_1$ to depend on $X_2$, include the "interaction term" $X_{1i} \times X_{2i}$ as a regressor:

Interpreting the coefficients:

\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 (X_{1i}
\times X_{2i}) + u_i
\end{equation}

General rule: compare the various cases

Now change $X_1$:

subtract from each other

The effect of $X_1$ depends on $X_2$ (what we wanted)

$\beta_3$ = increment to the effect of $X_1$ from a unit change in $X_2$


### Logarithmic transformation

Logarithmic functions of $Y$ and/or $X$

 $\ln(X) =$ the natural logarithm of $X$
Logarithmics permit modeling relations in percentage terms (like elasticities), rather than linearly.

Here's why:

\begin{equation}
\ln(x+\Delta x) - \ln(x) = \ln (1 + \frac{\Delta x}{x}) \cong \frac{\Delta x}{x}
\end{equation}
(calculus: $\frac{d \ln(x)}{dx}=\frac{1}{x})$ 
Numerically: $\ln(1.01) = .00995 \cong .01$;
    - $\ln(1.10) = .0953 \cong .10$ (sort of)

Rules for natural logarithms
1. $\ln(a\times b)= \ln(a)+\ln(b)$
2. $\ln(\frac{a}{b}) =\ln(a) - \ln(b)$
3. $\ln(a^\alpha) = \alpha \ln(a)$

If nonlinear models: try log-linearizing!

\begin{equation}
Y = A K^\alpha L^{1-\alpha} \rightarrow \ln(Y) = \ln(A) + \alpha \ln(K) + (1-\alpha) \ln(L)
\end{equation}
Note: similar operations to LHS and RHS!

```{r logspecifications, echo = FALSE}
v1 <- c(
"linear-log", "log_linear", "log-log"
  )
v2 <- c("$Y_i=\\beta_0 + \\beta_1 \\ln(X_i) + u_i$", "$\\ln(Y_i)=\\beta_0 + \\beta_1 (X_i) + u_i$ ", "$\\ln(Y_i)=\\beta_0 + \\beta_1 \\ln(X_i) + u_i$")
df <- data.frame( v1, v2 )
kable(df, 
      align="l", 
      booktabs=TRUE, escape = F, 
      caption = 'Three logarithmic transformation',
      col.names = c("Case","Population regression model")
) %>%
  kable_styling(bootstrap_options = "striped")
```

The interpretation of the slope coefficient differs in each case.
The interpretation is finding the marginal effect of X using the first derivative

#### Linear-log population regression model

The linear-log population regression model is specified as:	

\begin{equation}
	Y = \beta_0 + \beta_1 \ln(X)
\end{equation}
	
Now take first derivative:
	
\begin{equation}
	\frac{\partial Y}{\partial X} = \frac{\beta_1}{X} 
\end{equation}
so 
\begin{equation}
	\beta_1  = \frac{\partial Y}{\partial X / X} 
\end{equation}

Example: TestScore vs. ln(Income)

First defining the new regressor, $\ln(Income)$

The model is now linear in $\ln(Income)$, so the linear-log model can be estimated by OLS:
\begin{equation}
		\widehat{TestScore} = 557.8 + 36.42\times \ln(Income_i)
\end{equation}
so a 1\% increase in $Income$ is associated with an increase in TestScore of 0.36 points on the test.
Standard errors, confidence intervals, $R^2$---all the usual tools of regression apply here.

```{stata, echo=-5, results="hide"}
gen lninc = ln(avginc)
reg testscr lninc, r
predict testhat
graph twoway (line testhat avginc, sort) (scatter testscr avginc)
quietly graph export figures/scatterlnincome.png, replace
```

And this provides the following `STATA` output.

```{r, echo=FALSE, fig.cap = "A non-linear relation", label='scatterlnincome'}
knitr::include_graphics("./figures/scatterlnincome.png")
```
#### Log-linear population regression model

\begin{equation}
	\ln(Y) = \beta_0 + \beta_1 X
\end{equation}
	
Now take first derivative $\frac{\partial Y}{\partial X}$:
\begin{equation}
	Y = exp( \beta_0 + \beta_1 X )
\end{equation}
So
\begin{equation}
	\frac{\partial Y}{\partial X} = \beta_1  exp( \beta_0 + \beta_1 X ) = \beta_1 Y
\end{equation}
Thus
\begin{equation}
	\beta_1  = \frac{\partial Y / Y}{\partial X } 
\end{equation}

#### Log-log population regression model

\begin{equation}
	\ln(Y) = \beta_0 + \beta_1 \ln(X)
\end{equation}

Now take first derivative $\frac{\partial Y}{\partial X}$:
\begin{equation}
	Y = exp( \beta_0 + \beta_1 \ln(X) )
\end{equation}
So
\begin{equation}
	\frac{\partial Y}{\partial X} = \beta_1 /X  exp( \beta_0 + \beta_1 \ln(X) ) = \beta_1 Y /X
\end{equation}
Thus an **elasticity**
\begin{equation}
	\beta_1  = \frac{\partial Y / Y}{\partial X / X } 
\end{equation}

Example: ln( TestScore) vs. ln(Income)

First define a new dependent variable, ln(TestScore), and the new regressor, ln(Income)
The model is now a linear regression of ln(TestScore) against ln(Income), which can be estimated by OLS:
\begin{equation}
\widehat{ln(TestScore)} = 6.336 + 0.0554 \times ln(Income_i)
\end{equation}
		
- An 1\% increase in $Income$ is associated with an increase of .0554\% in $TestScore$ ($Income$ up by a factor of 1.01, $TestScore$ up by a factor of 1.000554)

```{stata, echo=-8, results="hide"}
gen lninc = ln(avginc)
gen lntestscr = ln(testscr)
reg lntestscr lninc, r
predict testhat1
reg lntestscr avginc, r
predict testhat2
graph twoway (line testhat1 avginc, sort) (line testhat2 avginc, sort) (scatter lntestscr avginc), legend(order(1 "log-log specification" 2 "log-linear specification" 3 "Observations")) 
quietly graph export figures/scattercompare.png, replace
```

And this provides the following `STATA` output.

```{r, echo=FALSE, fig.cap = "A non-linear relation", label='scattercompare'}
knitr::include_graphics("./figures/scattercompare.png")
```

Summary: Logarithmic transformations

Three cases, differing in whether $Y$ and/or $X$ is transformed by taking logarithms.
- The regression is linear in the new variable(s) $\ln(Y)$ and/or $\ln(X)$, and the coefficients can be estimated by OLS.
- Hypothesis tests and confidence intervals are now implemented and interpreted "as usual".
- The interpretation of $\beta_1$  differs from case to case.
- Choice of specification should be guided by judgment (which interpretation makes the most sense in your application?), tests, and plotting predicted values

## Using fixed effects in panel data {#sec:fixedeffects}

## Conclusion and discussion


